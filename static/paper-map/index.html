<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Arabic NLP Papers - UMAP Visualization</title>
<meta name="description" content="Interactive UMAP visualization of Arabic NLP papers from 2023-2025">

<!-- Vega/Altair Dependencies -->
<script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>

<style>
:root {
    --bg-primary: #1a1a1a;
    --bg-secondary: #252525;
    --bg-tertiary: #2a2a2a;
    --text-primary: #ffffff;
    --text-secondary: #b8b8b8;
    --text-tertiary: #8a8a8a;
    --accent: #ff6b35;
    --accent-hover: #ff8555;
    --border: #3a3a3a;
}

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

html, body {
    height: 100%;
}

body {
    font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    background-color: var(--bg-primary);
    color: var(--text-primary);
    line-height: 1.4;
    font-size: 15px;
}

.container {
    max-width: 1400px;
    margin: 0 auto;
    padding: 0.75rem 1.25rem 1.25rem;
}

.header {
    display: flex;
    justify-content: space-between;
    align-items: flex-end;
    gap: 1.5rem;
    padding-bottom: 0.6rem;
    border-bottom: 1px solid var(--border);
}

.header-main {
    flex: 1 1 260px;
}

h1 {
    font-size: 2rem;
    font-weight: 300;
    letter-spacing: -0.02em;
    margin-bottom: 0.25rem;
}

h1 .accent {
    color: var(--accent);
    font-weight: 400;
}

.subtitle {
    color: var(--text-secondary);
    font-size: 0.95rem;
}

.stats {
    display: flex;
    flex-wrap: wrap;
    gap: 1.2rem;
    justify-content: flex-end;
}

.stat {
    min-width: 80px;
    text-align: right;
}

.stat-value {
    font-size: 1.6rem;
    font-weight: 200;
    color: var(--accent);
    line-height: 1.1;
}

.stat-label {
    font-size: 0.75rem;
    color: var(--text-tertiary);
    text-transform: uppercase;
    letter-spacing: 0.06em;
    margin-top: 0.2rem;
}

.visualization {
    background: var(--bg-secondary);
    border-radius: 8px;
    padding: 0.9rem;
    border: 1px solid var(--border);
    margin-top: 0.9rem;
    margin-bottom: 1.3rem;
}

#vis {
    width: 100%;
}

.vega-embed {
    width: 100% !important;
}

.vega-embed .vega-actions {
    display: none;
}

/* Filter controls from Altair/Vega bindings */
.vega-bind {
    padding: 0.5rem 0;
    flex-direction: row;
    display: flex;
    gap: 1rem;
    flex-wrap: wrap;
    border-bottom: 1px solid var(--border);
    margin-bottom: 0.7rem;
}

.vega-bind label {
    color: var(--text-secondary);
    font-size: 0.9rem;
    display: flex;
    align-items: center;
    gap: 0.4rem;
}

.vega-bind select {
    background: var(--bg-tertiary);
    color: var(--text-primary);
    border: 1px solid var(--border);
    padding: 0.25rem 0.6rem;
    border-radius: 4px;
    font-size: 0.88rem;
    cursor: pointer;
    transition: all 0.2s ease;
}

.vega-bind select:hover {
    background: var(--bg-primary);
    border-color: var(--accent);
}

.vega-bind select:focus {
    outline: none;
    border-color: var(--accent);
    box-shadow: 0 0 0 2px rgba(255, 107, 53, 0.2);
}

footer {
    text-align: center;
    padding-top: 0.9rem;
    border-top: 1px solid var(--border);
    color: var(--text-tertiary);
    font-size: 0.85rem;
}

footer a {
    color: var(--accent);
    text-decoration: none;
    transition: color 0.2s ease;
}

footer a:hover {
    color: var(--accent-hover);
    text-decoration: underline;
}

@media (max-width: 900px) {
    .header {
        flex-direction: column;
        align-items: flex-start;
    }
    .stats {
        justify-content: flex-start;
    }
    .stat {
        text-align: left;
    }
    h1 {
        font-size: 1.7rem;
    }
}
</style>
</head>
<body>
<div class="container">
    <header class="header">
        <div class="header-main">
            <h1>Arabic NLP Papers <span class="accent">Explorer</span></h1>
            <p class="subtitle">Interactive UMAP visualization of research papers (2023â€“2025)</p>
            <p class="subtitle">Check the write-up at <a href="https://cairo-nlp.github.io/paper-digest/arabic-nlp-viz/" target="_blank" style="color: var(--accent); text-decoration: none;">Arabic NLP Papers Explorer Blog Post</a></p>
        </div>
        <div class="stats">
            <div class="stat">
                <div class="stat-value">328</div>
                <div class="stat-label">Total Papers</div>
            </div>
            <div class="stat">
                <div class="stat-value">98</div>
                <div class="stat-label">Main Papers</div>
            </div>
            <div class="stat">
                <div class="stat-value">230</div>
                <div class="stat-label">Shared Tasks</div>
            </div>
            <div class="stat">
                <div class="stat-value">3</div>
                <div class="stat-label">Years</div>
            </div>
        </div>
    </header>

    <div class="visualization">
        <div id="vis"></div>
    </div>

    <footer>
        <p>
            <a href="https://cairo-nlp.github.io/" target="_blank">cAIro NLP Group</a>
        </p>
    </footer>
</div>

<script type="text/javascript">
const spec = {
  "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json",
  "config": {
    "background": "#FDF7F0",
    "view": {
      "continuousHeight": 300,
      "continuousWidth": 300
    }
  },
  "data": {
    "name": "data-f987755f87cb264e77d2275347bdb7fa"
  },
  "datasets": {
    "data-f987755f87cb264e77d2275347bdb7fa": [
      {
        "abstract": "In this paper, we present our approach for the \u201cNuanced Arabic Dialect Identification (NADI) Shared Task 2023\u201d. We highlight our methodology for subtask 1 which deals with country-level dialect identification. Recognizing dialects plays an instrumental role in enhancing the performance of various downstream NLP tasks such as speech recognition and translation. The task uses the Twitter dataset (TWT-2023) that encompasses 18 dialects for the multi-class classification problem. Numerous transformer-based models, pre-trained on Arabic language, are employed for identifying country-level dialects. We fine-tune these state-of-the-art models on the provided dataset. Ensembling method is leveraged to yield improved performance of the system. We achieved an F1-score of 76.65 (11th rank on leaderboard) on the test dataset.",
        "authors": [
          "Vedant Deshpande",
          "Yash Patwardhan",
          "Kshitij Deshpande",
          "Sudeep Mangalvedhekar",
          "Ravindra Murumkar"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "678",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "682",
        "paper_id": 74,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.74.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.74.jpg",
        "title": "Mavericks at NADI 2023 Shared Task: Unravelling Regional Nuances through Dialect Identification using Transformer-based Approach",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.74",
        "x": -4.841315269470215,
        "y": 0.0004340308078099042,
        "year": "2023"
      },
      {
        "abstract": "As Electronic Health Records (EHR) become ubiquitous in healthcare systems worldwide, including in Arabic-speaking countries, the dual imperative of safeguarding patient privacy and leveraging data for research and quality improvement grows. This paper presents a first-of-its-kind automated de-identification pipeline for medical text specifically tailored for the Arabic language. This includes accurate medical Named Entity Recognition (NER) for identifying personal information; data obfuscation models to replace sensitive entities with fake entities; and an implementation that natively scales to large datasets on commodity clusters. This research makes two contributions. First, we adapt two existing NER architectures\u2014 BERT For Token Classification (BFTC) and BiLSTM-CNN-Char \u2013 to accommodate the unique syntactic and morphological characteristics of the Arabic language. Comparative analysis suggests that BFTC models outperform Bi-LSTM models, achieving higher F1 scores for both identifying and redacting personally identifiable information (PII) from Arabic medical texts. Second, we augment the deep learning models with a contextual parser engine to handle commonly missed entities. Experiments show that the combined pipeline demonstrates superior performance with micro F1 scores ranging from 0.94 to 0.98 on the test dataset, which is a translated version of the i2b2 2014 de-identification challenge, across 17 sensitive entities. This level of accuracy is in line with that achieved with manual de-identification by domain experts, suggesting that a fully automated and scalable process is now viable.",
        "authors": [
          "Veysel Kocaman",
          "Youssef Mellah",
          "Hasham Haq",
          "David Talby"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "33",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "40",
        "paper_id": 4,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.4.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.4.jpg",
        "title": "Automated De-Identification of Arabic Medical Records",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.4",
        "x": -4.184329986572266,
        "y": 1.0900959968566895,
        "year": "2023"
      },
      {
        "abstract": "This paper presents the first Arabic crossword puzzle generator driven by advanced AI technology. Leveraging cutting-edge large language models including GPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system generates distinctive and challenging clues. Based on a dataset comprising over 50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot learning strategies, and rigorous quality-checking protocols to enforce the generation of high-quality clue-answer pairs. Importantly, educational crosswords contribute to enhancing memory, expanding vocabulary, and promoting problem-solving skills, thereby augmenting the learning experience through a fun and engaging approach, reshaping the landscape of traditional learning methods. The overall system can be exploited as a powerful educational tool that amalgamates AI and innovative learning techniques, heralding a transformative era for Arabic crossword puzzles and the intersection of technology and education.",
        "authors": [
          "Kamyar Zeinalipour",
          "Mohamed Saad",
          "Marco Maggini",
          "Marco Gori"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "288",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "301",
        "paper_id": 23,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.23.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.23.jpg",
        "title": "ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.23",
        "x": -3.526142120361328,
        "y": 0.7918063402175903,
        "year": "2023"
      },
      {
        "abstract": "Automatic Text Simplification (TS) involves simplifying language complexity while preserving the original meaning. The main objective of TS is to enhance the readability of complex texts, making them more accessible to a broader range of readers. This work focuses on developing a lexical text simplification system specifically for Arabic. We utilized FastText and Arabert pre-trained embedding models to create various simplification models. Our lexical approach involves a series of steps: identifying complex words, generating potential replacements, and selecting one replacement for the complex word within a sentence. We presented two main identification models: binary and multi-complexity models. We assessed the efficacy of these models by employing BERTScore to measure the similarity between the sentences generated by these models and the intended simple sentences. This comparative analysis evaluated the effectiveness of these models in accurately identifying and selecting complex words.",
        "authors": [
          "Yousef SalahEldin",
          "Caroline Sabty"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "418",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "422",
        "paper_id": 35,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.35.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.35.jpg",
        "title": "Simplify: Automatic Arabic Sentence Simplification using Word Embeddings",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.35",
        "x": -4.818359375,
        "y": 1.7315070629119873,
        "year": "2023"
      },
      {
        "abstract": "We describe the findings of the fourth Nuanced Arabic Dialect Identification Shared Task (NADI 2023). The objective of NADI is to help advance state-of-the-art Arabic NLP by creating opportunities for teams of researchers to collaboratively compete under standardized conditions. It does so with a focus on Arabic dialects, offering novel datasets and defining subtasks that allow for meaningful comparisons between different approaches. NADI 2023 targeted both dialect identification (Subtask1) and dialect-to-MSA machine translation (Subtask 2 and Subtask 3). A total of 58 unique teams registered for the shared task, of whom 18 teams have participated (with 76 valid submissions during test phase). Among these, 16 teams participated in Subtask 1, 5 participated in Subtask 2, and 3 participated in Subtask 3. The winning teams achieved 87.27 F1 on Subtask 1, 14.76 Bleu in Subtask 2, and 21.10 Bleu in Subtask 3, respectively. Results show that all three subtasks remain challenging, thereby motivating future work in this area. We describe the methods employed by the participating teams and briefly offer an outlook for NADI.",
        "authors": [
          "Muhammad Abdul-Mageed",
          "Abdelrahim Elmadany",
          "Chiyu Zhang",
          "El-Moatez-Billah Nagoudi",
          "Houda Bouamor",
          "Nizar Habash"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "600",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "613",
        "paper_id": 62,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.62.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.62.jpg",
        "title": "NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.62",
        "x": -4.4640021324157715,
        "y": 0.4175247848033905,
        "year": "2023"
      },
      {
        "abstract": "Wikipedia articles are a widely used source of training data for Natural Language Processing (NLP) research, particularly as corpora for low-resource languages like Arabic. However, it is essential to understand the extent to which these corpora reflect the representative contributions of native speakers, especially when many entries in a given language are directly translated from other languages or automatically generated through automated mechanisms. In this paper, we study the performance implications of using inorganic corpora that are not representative of native speakers and are generated through automated techniques such as bot generation or automated template-based translation. The case of the Arabic Wikipedia editions gives a unique case study of this since the Moroccan Arabic Wikipedia edition (ARY) is small but representative, the Egyptian Arabic Wikipedia edition (ARZ) is large but unrepresentative, and the Modern Standard Arabic Wikipedia edition (AR) is both large and more representative. We intrinsically evaluate the performance of two main NLP upstream tasks, namely word representation and language modeling, using word analogy evaluations and fill-mask evaluations using our two newly created datasets: Arab States Analogy Dataset (ASAD) and Masked Arab States Dataset (MASD). We demonstrate that for good NLP performance, we need both large and organic corpora; neither alone is sufficient. We show that producing large corpora through automated means can be a counter-productive, producing models that both perform worse and lack cultural richness and meaningful representation of the Arabic language and its native speakers.",
        "authors": [
          "Saied Alshahrani",
          "Norah Alshahrani",
          "Soumyabrata Dey",
          "Jeanna Matthews"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "218",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "231",
        "paper_id": 19,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.19.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.19.jpg",
        "title": "Performance Implications of Using Unrepresentative Corpora in Arabic Natural Language Processing",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.19",
        "x": -4.725620269775391,
        "y": 0.169932559132576,
        "year": "2023"
      },
      {
        "abstract": "The spread of disinformation and propagandistic content poses a threat to societal harmony, undermining informed decision-making and trust in reliable sources. Online platforms often serve as breeding grounds for such content, and malicious actors exploit the vulnerabilities of audiences to shape public opinion. Although there have been research efforts aimed at the automatic identification of disinformation and propaganda in social media content, there remain challenges in terms of performance. The ArAIEval shared task aims to further research on these particular issues within the context of the Arabic language. In this paper, we discuss our participation in these shared tasks. We competed in subtasks 1A and 2A, where our submitted system secured positions 9th and 10th, respectively. Our experiments consist of fine-tuning transformer models and using zero- and few-shot learning with GPT-4.",
        "authors": [
          "Yunze Xiao",
          "Firoj Alam"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "576",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "582",
        "paper_id": 58,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.58.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.58.jpg",
        "title": "Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.58",
        "x": -3.4801952838897705,
        "y": 1.8307487964630127,
        "year": "2023"
      },
      {
        "abstract": "The Qur\u2019an holds immense theological and historical significance, and developing a technology-driven solution for answering questions from this sacred text is of paramount importance. This paper presents our approach to task B of Qur\u2019an QA 2023, part of EMNLP 2023, addressing this challenge by proposing a robust method for extracting answers from Qur\u2019anic passages. Leveraging the Qur\u2019anic Reading Comprehension Dataset (QRCD) v1.2, we employ innovative techniques and advanced models to improve the precision and contextuality of answers derived from Qur\u2019anic passages. Our methodology encompasses the utilization of start and end logits, Long Short-Term Memory (LSTM) networks, and fusion mechanisms, contributing to the ongoing dialogue at the intersection of technology and spirituality.",
        "authors": [
          "Hariram Veeramani",
          "Surendrabikram Thapa",
          "Usman Naseem"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "708",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "713",
        "paper_id": 78,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.78.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.78.jpg",
        "title": "LowResContextQA at Qur\u2019an QA 2023 Shared Task: Temporal and Sequential Representation Augmented Question Answering Span Detection in Arabic",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.78",
        "x": -2.1801655292510986,
        "y": 2.8239197731018066,
        "year": "2023"
      },
      {
        "abstract": "Text summarization has been intensively studied in many languages, and some languages have reached advanced stages. Yet, Arabic Text Summarization (ATS) is still in its developing stages. Existing ATS datasets are either small or lack diversity. We build, LANS, a large-scale and diverse dataset for Arabic Text Summarization task. LANS offers 8.4 million articles and their summaries extracted from newspapers websites\u2019 metadata between 1999 and 2019. The high-quality and diverse summaries are written by journalists from 22 major Arab newspapers and include an eclectic mix of at least more than 7 topics from each source. We conduct an intrinsic evaluation on LANS by both automatic and human evaluations. Human evaluation of 1,000 random samples reports 95.4% accuracy for our collected summaries, and automatic evaluation quantifies the diversity and abstractness of the summaries.",
        "authors": [
          "Abdulaziz Alhamadani",
          "Xuchao Zhang",
          "Jianfeng He",
          "Aadyant Khatri",
          "Chang-Tien Lu"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "89",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "100",
        "paper_id": 8,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.8.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.8.jpg",
        "title": "LANS: Large-scale Arabic News Summarization Corpus",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.8",
        "x": -2.8987274169921875,
        "y": 0.8717883825302124,
        "year": "2023"
      },
      {
        "abstract": "This paper outlines the KSAA-RD shared task, which aims to develop a Reverse Dictionary (RD) system for the Arabic language. RDs allow users to find words based on their meanings or definition. This shared task, KSAA-RD, includes two subtasks: Arabic RD and cross-lingual reverse dictionaries (CLRD). Given a definition (referred to as a \u201cgloss\u201d) in either Arabic or English, the teams compete to find the most similar word embeddings of their corresponding word. The winning team achieved 24.20 and 12.70 for RD and CLRD, respectively in terms of rank metric. In this paper, we describe the methods employed by the participating teams and offer an outlook for KSAA-RD.",
        "authors": [
          "Rawan Al-Matham",
          "Waad Alshammari",
          "Abdulrahman Alosaimy",
          "Sarah Alhumoud",
          "Asma Wazrah",
          "Afrah Altamimi",
          "Halah Alharbi",
          "Abdullah Alaifi"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "450",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "460",
        "paper_id": 39,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.39.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.39.jpg",
        "title": "KSAA-RD Shared Task: Arabic Reverse Dictionary",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.39",
        "x": -5.609728813171387,
        "y": 0.6432681679725647,
        "year": "2023"
      },
      {
        "abstract": "In this paper, we present our approach to tackle Qur\u2019an QA 2023 shared tasks A and B. To address the challenge of low-resourced training data, we rely on transfer learning together with a voting ensemble to improve prediction stability across multiple runs. Additionally, we employ different architectures and learning mechanisms for a range of Arabic pre-trained transformer-based models for both tasks. To identify unanswerable questions, we propose using a thresholding mechanism. Our top-performing systems greatly surpass the baseline performance on the hidden split, achieving a MAP score of 25.05% for task A and a partial Average Precision (pAP) of 57.11% for task B.",
        "authors": [
          "Mohammed Elkomy",
          "Amany Sarhan"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "728",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "742",
        "paper_id": 81,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.81.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.81.jpg",
        "title": "TCE at Qur\u2019an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur\u2019anic QA",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.81",
        "x": -2.856245756149292,
        "y": 0.7432745099067688,
        "year": "2023"
      },
      {
        "abstract": "We present CamelParser2.0, an open-source Python-based Arabic dependency parser targeting two popular Arabic dependency formalisms, the Columbia Arabic Treebank (CATiB), and Universal Dependencies (UD). The CamelParser2.0 pipeline handles the processing of raw text and produces tokenization, part-of-speech and rich morphological features. As part of developing CamelParser2.0, we explore many system design hyper-parameters, such as parsing model architecture and pretrained language model selection, achieving new state-of-the-art performance across diverse Arabic genres under gold and predicted tokenization settings.",
        "authors": [
          "Ahmed Elshabrawy",
          "Muhammed AbuOdeh",
          "Go Inoue",
          "Nizar Habash"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "170",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "180",
        "paper_id": 15,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.15.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.15.jpg",
        "title": "CamelParser2.0: A State-of-the-Art Dependency Parser for Arabic",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.15",
        "x": -4.3686652183532715,
        "y": 1.1227277517318726,
        "year": "2023"
      },
      {
        "abstract": "This paper presents a novel approach to the Arabic Reverse Dictionary Shared Task at WANLP 2023 by leveraging the BERT Multilingual model and introducing modifications augmentation and using a multi attention head. The proposed method aims to enhance the performance of the model in understanding and generating word embeddings for Arabic definitions, both in monolingual and cross-lingual contexts. It achieved good results compared to benchmark and other models in the shared task 1 and 2.",
        "authors": [
          "Abdelrahim Qaddoumi"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "472",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "476",
        "paper_id": 42,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.42.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.42.jpg",
        "title": "Abed at KSAA-RD Shared Task: Enhancing Arabic Word Embedding with Modified BERT Multilingual",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.42",
        "x": -3.4251420497894287,
        "y": 1.0270074605941772,
        "year": "2023"
      },
      {
        "abstract": "To enhance persuasion detection, we investigate the use of multilingual systems on Arabic data by conducting a total of 22 experiments using baselines, multilingual, and monolingual language transformers. Our aim is to provide a comprehensive evaluation of the various systems employed throughout this task, with the ultimate goal of comparing their performance and identifying the most effective approach. Our empirical analysis shows that *ReDASPersuasion* system performs best when combined with multilingual \u201cXLM-RoBERTa\u201d and monolingual pre-trained transformers on Arabic dialects like \u201cCAMeLBERT-DA SA\u201d depending on the NLP classification task.",
        "authors": [
          "Fatima Zahra Qachfar",
          "Rakesh Verma"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "549",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "557",
        "paper_id": 54,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.54.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.54.jpg",
        "title": "ReDASPersuasion at ArAIEval Shared Task: Multilingual and Monolingual Models For Arabic Persuasion Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.54",
        "x": -3.214635133743286,
        "y": 0.9507960677146912,
        "year": "2023"
      },
      {
        "abstract": "In this paper, we introduce our participating system to the ArAIEval Shared Task, addressing both the detection of persuasion techniques and disinformation tasks. Our proposed system employs a pre-trained transformer-based language model for Arabic, alongside a classifier. We have assessed the performance of three Arabic Pre-trained Language Models (PLMs) for sentence encoding. Additionally, to enhance our model\u2019s performance, we have explored various training objectives, including Cross-Entropy loss, regularized Mixup loss, asymmetric multi-label loss, and Focal Tversky loss. On the official test set, our system has achieved micro-F1 scores of 0.7515, 0.5666, 0.904, and 0.8333 for Sub-Task 1A, Sub-Task 1B, Sub-Task 2A, and Sub-Task 2B, respectively. Furthermore, our system has secured the 4th, 1st, 3rd, and 2nd positions, respectively, among all participating systems in sub-tasks 1A, 1B, 2A, and 2B of the ArAIEval shared task.",
        "authors": [
          "Salima Lamsiyah",
          "Abdelkader El Mahdaouy",
          "Hamza Alami",
          "Ismail Berrada",
          "Christoph Schommer"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "558",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "564",
        "paper_id": 55,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.55.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.55.jpg",
        "title": "UL & UM6P at ArAIEval Shared Task: Transformer-based model for Persuasion Techniques and Disinformation detection in Arabic",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.55",
        "x": -4.873266220092773,
        "y": -0.36694398522377014,
        "year": "2023"
      },
      {
        "abstract": "A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the \u201cTip-of-the-Tongue\u201d (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each model within the ensemble. In contrast, the most effective solution for the second subtask involves translating the English test definitions into Arabic and applying them to the finetuned models originally trained for the first subtask. This straightforward method achieves the highest score across both subtasks.",
        "authors": [
          "Ahmed Elbakry",
          "Mohamed Gabr",
          "Muhammad N. ElNokrashy",
          "Badr Alkhamissi"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "477",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "482",
        "paper_id": 43,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.43.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.43.jpg",
        "title": "Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word\u2013Definition Alignment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.43",
        "x": -5.025086402893066,
        "y": 1.8797638416290283,
        "year": "2023"
      },
      {
        "abstract": "Extracting and disambiguating geolocation information from social media data enables effective disaster management, as it helps response authorities; for example, locating incidents for planning rescue activities and affected people for evacuation. Nevertheless, the dearth of resources and tools hinders the development and evaluation of Location Mention Disambiguation (LMD) models in the disaster management domain. Consequently, the LMD task is greatly understudied, especially for the low resource languages such as Arabic. To fill this gap, we introduce IDRISI-D, the largest to date English and the first Arabic public LMD datasets. Additionally, we introduce a modified hierarchical evaluation framework that offers a lenient and nuanced evaluation of LMD systems. We further benchmark IDRISI-D datasets using representative baselines and show the competitiveness of BERT-based models.",
        "authors": [
          "Reem Suwaileh",
          "Tamer Elsayed",
          "Muhammad Imran"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "158",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "169",
        "paper_id": 14,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.14.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.14.jpg",
        "title": "IDRISI-D: Arabic and English Datasets and Benchmarks for Location Mention Disambiguation over Disaster Microblogs",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.14",
        "x": -3.258962869644165,
        "y": 0.5672783255577087,
        "year": "2023"
      },
      {
        "abstract": "The Qur\u2019an QA 2023 shared task has two sub tasks: Passage Retrieval (PR) task and Machine Reading Comprehension (MRC) task. Our participation in the PR task was to further train several Arabic pre-trained models using a Sentence-Transformers architecture and to ensemble the best performing models. The results of the test set did not reflect the results of the development set. CL-AraBERT achieved the best results, with a 0.124 MAP. We also participate in the MRC task by further fine-tuning the base and large variants of AraBERT using Classical Arabic and Modern Standard Arabic datasets. Base AraBERT achieved the best result with the development set with a partial average precision (pAP) of 0.49, while it achieved 0.5 with the test set. In addition, we applied the ensemble approach of best performing models and post-processing steps to the final results. Our experiments with the development set showed that our proposed model achieved a 0.537 pAP. On the test set, our system obtained a pAP score of 0.49.",
        "authors": [
          "Sarah Alnefaie",
          "Abdullah Alsaleh",
          "Eric Atwell",
          "Mohammad Alsalka",
          "Abdulrahman Altahhan"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "720",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "727",
        "paper_id": 80,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.80.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.80.jpg",
        "title": "LKAU23 at Qur\u2019an QA 2023: Using Transformer Models for Retrieving Passages and Finding Answers to Questions from the Qur\u2019an",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.80",
        "x": -4.632069110870361,
        "y": -0.0733947679400444,
        "year": "2023"
      },
      {
        "abstract": "Arabic is a complex language with many varieties and dialects spoken by ~ 450 millions all around the world. Due to the linguistic diversity and vari-ations, it is challenging to build a robust and gen-eralized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identi-fication (DID) as well as automatic speech recog-nition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the re-maining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selec-tion, and the option to raise flags for incorrect out-puts. Overall, we believe VoxArabica will be use-ful for a wide range of audiences concerned with Arabic research. Our system is currently running at https://cdce-206-12-100-168.ngrok.io/.",
        "authors": [
          "Abdul Waheed",
          "Bashar Talafha",
          "Peter Sullivan",
          "Abdelrahim Elmadany",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "441",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "449",
        "paper_id": 38,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.38.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.38.jpg",
        "title": "VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.38",
        "x": -4.247939586639404,
        "y": 1.3228590488433838,
        "year": "2023"
      },
      {
        "abstract": "Large language models (LLMs) finetuned to follow human instruction have recently exhibited significant capabilities in various English NLP tasks. However, their performance in grammatical error correction (GEC), especially on languages other than English, remains significantly unexplored. In this work, we evaluate the abilities of instruction finetuned LLMs in Arabic GEC, a complex task due to Arabic\u2019s rich morphology. Our findings suggest that various prompting methods, coupled with (in-context) few-shot learning, demonstrate considerable effectiveness, with GPT-4 achieving up to 65.49 F1 score under expert prompting (approximately 5 points higher than our established baseline). Despite these positive results, we find that instruction finetuned models, regardless of their size, are still outperformed by fully finetuned ones, even if they are significantly smaller in size. This disparity highlights substantial room for improvements for LLMs. Inspired by methods used in low-resource machine translation, we also develop a method exploiting synthetic data that significantly outperforms previous models on two standard Arabic benchmarks. Our best model achieves a new SOTA on Arabic GEC, with 73.29 and 73.26 F1 on the 2014 and 2015 QALB datasets, respectively, compared to peer-reviewed published baselines.",
        "authors": [
          "Sang Kwon",
          "Gagan Bhatia",
          "El-Moatez-Billah Nagoudi",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "101",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "119",
        "paper_id": 9,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.9.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.9.jpg",
        "title": "Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.9",
        "x": -5.089722633361816,
        "y": 0.19636239111423492,
        "year": "2023"
      },
      {
        "abstract": "This work addresses the challenges of question answering for vintage texts like the Quran. It introduces two tasks: passage retrieval and reading comprehension. For passage retrieval, it employs unsupervised fine-tuning sentence encoders and supervised multi-task learning. In reading comprehension, it fine-tunes an Electra-based model, demonstrating significant improvements over baseline models. Our best AraElectra model achieves 46.1% partial Average Precision (pAP) on the unseen test set, outperforming the baseline by 23%.",
        "authors": [
          "Ghazaleh Mahmoudi",
          "Yeganeh Morshedzadeh",
          "Sauleh Eetemadi"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "714",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "719",
        "paper_id": 79,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.79.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.79.jpg",
        "title": "GYM at Qur\u2019an QA 2023 Shared Task: Multi-Task Transfer Learning for Quranic Passage Retrieval and Question Answering with Large Language Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.79",
        "x": -2.7985496520996094,
        "y": 1.817003607749939,
        "year": "2023"
      },
      {
        "abstract": "In this work, we present our systems developed for \u201cArAIEval\u201d shared task of ArabicNLP 2023 (CITATION). We used an mBERT transformer for Subtask 1A, which targets persuasion in Arabic tweets, and we used the MARBERT transformer for Subtask 2A to identify disinformation in Arabic tweets. Our persuasion detection system achieved micro-F1 of0.745by surpassing the baseline by 13.2%, and registered a macro-F1 of 0.717 based on leaderboard scores. Similarly, our disinformation system recorded a micro-F1 of0.816, besting the na\u00efve majority by 6.7%, with a macro-F1 of 0.637. Furthermore, we present our preliminary results on a variety of pre-trained models. In terms of overall ranking, our systems placed7thout of 16 and12thout of 17 teams for Subtasks 1A and 2A, respectively.",
        "authors": [
          "Dilshod Azizov",
          "Jiyong Li",
          "Shangsong Liang"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "583",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "588",
        "paper_id": 59,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.59.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.59.jpg",
        "title": "Frank at ArAIEval Shared Task: Arabic Persuasion and Disinformation: The Power of Pretrained Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.59",
        "x": -4.311586380004883,
        "y": 0.5920886993408203,
        "year": "2023"
      },
      {
        "abstract": "In digital communication, emoji are essential in decoding nuances such as irony, sarcasm, and humour. However, their incorporation in Arabic natural language processing (NLP) has been cautious because of the perceived complexities of the Arabic language. This paper introduces ArSarcasMoji, a dataset of 24,630 emoji-augmented texts, with 17. 5% that shows irony. Through our analysis, we highlight specific emoji patterns paired with sentiment roles that denote irony in Arabic texts. The research counters prevailing notions, emphasising the importance of emoji\u2019s role in understanding Arabic textual irony, and addresses their potential for accurate irony detection in Arabic digital content.",
        "authors": [
          "Shatha Ali A. Hakami",
          "Robert J. Hendley",
          "Phillip Smith"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "208",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "217",
        "paper_id": 18,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.18.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.18.jpg",
        "title": "ArSarcasMoji Dataset: The Emoji Sentiment Roles in Arabic Ironic Contexts",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.18",
        "x": -5.104147434234619,
        "y": 0.14448724687099457,
        "year": "2023"
      },
      {
        "abstract": "With approximately 400 million speakers worldwide, Arabic ranks as the fifth most-spoken language globally, necessitating advancements in natural language processing. This paper addresses this need by presenting a system description of the approaches employed for the subtasks outlined in the Nuanced Arabic Dialect Identification (NADI) task at EMNLP 2023. For the first subtask, involving closed country-level dialect identification classification, we employ an ensemble of two Arabic language models. Similarly, for the second subtask, focused on closed dialect to Modern Standard Arabic (MSA) machine translation, our approach combines sequence-to-sequence models, all trained on an Arabic-specific dataset. Our team ranks 10th and 3rd on subtask 1 and subtask 2 respectively.",
        "authors": [
          "Hariram Veeramani",
          "Surendrabikram Thapa",
          "Usman Naseem"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "614",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "619",
        "paper_id": 63,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.63.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.63.jpg",
        "title": "DialectNLU at NADI 2023 Shared Task: Transformer Based Multitask Approach Jointly Integrating Dialect and Machine Translation Tasks in Arabic",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.63",
        "x": -4.701131820678711,
        "y": 0.06477586179971695,
        "year": "2023"
      },
      {
        "abstract": "Low-resource Machine Translation (MT) is characterized by the scarce availability of training data and/or standardized evaluation benchmarks. In the context of Dialectal Arabic, recent works introduced several evaluation benchmarks covering both Modern Standard Arabic (MSA) and dialects, mapping, however, mostly to a single Indo-European language - English. In this work, we introduce a multi-lingual corpus consisting of 120,600 multi-parallel sentences in English, French, German, Greek, Spanish, and MSA selected from the OpenSubtitles corpus, which were manually translated into the North Levantine Arabic. By conducting a series of training and fine-tuning experiments, we explore how this novel resource can contribute to the research on Arabic MT.",
        "authors": [
          "Mateusz Krubi\u0144ski",
          "Hashem Sellat",
          "Shadi Saleh",
          "Adam Posp\u00ed\u0161il",
          "Petr Zem\u00e1nek",
          "Pavel Pecina"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "411",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "417",
        "paper_id": 34,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.34.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.34.jpg",
        "title": "Multi-Parallel Corpus of North Levantine Arabic",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.34",
        "x": -5.107827663421631,
        "y": -0.10066473484039307,
        "year": "2023"
      },
      {
        "abstract": "We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models are released for research use.",
        "authors": [
          "Hawau Olamide Toyin",
          "Amirbek Djanibekov",
          "Ajinkya Kulkarni",
          "Hanan Aldarmaki"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "41",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "51",
        "paper_id": 5,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.5.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.5.jpg",
        "title": "ArTST: Arabic Text and Speech Transformer",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.5",
        "x": -3.1764111518859863,
        "y": 2.412169933319092,
        "year": "2023"
      },
      {
        "abstract": "This paper presents the ArBanking77, a large Arabic dataset for intent detection in the banking domain. Our dataset was arabized and localized from the original English Banking77 dataset, which consists of 13,083 queries to ArBanking77 dataset with 31,404 queries in both Modern Standard Arabic (MSA) and Palestinian dialect, with each query classified into one of the 77 classes (intents). Furthermore, we present a neural model, based on AraBERT, fine-tuned on ArBanking77, which achieved an F1-score of 0.9209 and 0.8995 on MSA and Palestinian dialect, respectively. We performed extensive experimentation in which we simulated low-resource settings, where the model is trained on a subset of the data and augmented with noisy queries to simulate colloquial terms, mistakes and misspellings found in real NLP systems, especially live chat queries. The data and the models are publicly available at https://sina.birzeit.edu/arbanking77.",
        "authors": [
          "Mustafa Jarrar",
          "Ahmet Birim",
          "Mohammed Khalilia",
          "Mustafa Erden",
          "Sana Ghanem"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "276",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "287",
        "paper_id": 22,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.22.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.22.jpg",
        "title": "ArBanking77: Intent Detection Neural Model and a New Dataset in Modern and Dialectical Arabic",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.22",
        "x": -4.426362037658691,
        "y": 0.09403976052999496,
        "year": "2023"
      },
      {
        "abstract": "In this paper, we present our findings within the context of the NADI-2023 Shared Task (Subtask 2). Our task involves developing a translation model from the Palestinian, Jordanian, Emirati, and Egyptian dialects to Modern Standard Arabic (MSA) using the MADAR parallel corpus, even though it lacks a parallel subset for the Emirati dialect. To address this challenge, we conducted a comparative analysis, evaluating the fine-tuning results of various transformer models using the MADAR corpus as a learning resource. Additionally, we assessed the effectiveness of existing translation tools in achieving our translation objectives. The best model achieved a BLEU score of 11.14% on the dev set and 10.02 on the test set.",
        "authors": [
          "Wiem Derouich",
          "Sam\u00e9h Kchaou",
          "Rahma Boujelbane"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "683",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "689",
        "paper_id": 75,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.75.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.75.jpg",
        "title": "ANLP-RG at NADI 2023 shared task: Machine Translation of Arabic Dialects: A Comparative Study of Transformer Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.75",
        "x": -4.553701877593994,
        "y": -0.17648153007030487,
        "year": "2023"
      },
      {
        "abstract": "Named Entity Recognition (NER) is the task of identifying word-units that correspond to mentions as location, organization, person, or currency. In this shared task we tackle flat-entity classification for Arabic, where for each word-unit a single entity should be identified. To resolve the classification problem we propose StagedNER a novel technique to fine-tuning NER downstream tasks that divides the learning process of a transformer-model into two phases, where a model is tasked to learn sequence tags and then entity tags rather than learn both together simultaneously for an input sequence. We create an ensemble of two base models using this method that yield a score of on the development set and an F1 performance of 90.03% on the validation set and 91.95% on the test set.",
        "authors": [
          "Nehal Elkaref",
          "Mohab El-karef"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "803",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "808",
        "paper_id": 91,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.91.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.91.jpg",
        "title": "El-Kawaref at WojoodNER shared task: StagedNER for Arabic Named Entity Recognition",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.91",
        "x": -4.583127021789551,
        "y": 0.3430851697921753,
        "year": "2023"
      },
      {
        "abstract": "This paper presents the approach of the NLPeople team to the Nuanced Arabic Dialect Identification (NADI) 2023 shared task. Subtask 1 involves identifying the dialect of a source text at the country level. Our approach to Subtask 1 makes use of language-specific language models, a clustering and retrieval method to provide additional context to a target sentence, a fine-tuning strategy which makes use of the provided data from the 2020 and 2021 shared tasks, and finally, ensembling over the predictions of multiple models. Our submission achieves a macro-averaged F1 score of 87.27, ranking 1st among the other participants in the task.",
        "authors": [
          "Mohab El-karef",
          "Movina Moses",
          "Shinnosuke Tanaka",
          "James Barry",
          "Geeth Mel"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "642",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "646",
        "paper_id": 68,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.68.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.68.jpg",
        "title": "NLPeople at NADI 2023 Shared Task: Arabic Dialect Identification with Augmented Context and Multi-Stage Tuning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.68",
        "x": -2.5638575553894043,
        "y": 0.8712708950042725,
        "year": "2023"
      },
      {
        "abstract": "In this paper, we present our submitted system for the WojoodNER Shared Task, addressing both flat and nested Arabic Named Entity Recognition (NER). Our system is based on a BERT-based multi-task learning model that leverages the existing Arabic Pretrained Language Models (PLMs) to encode the input sentences. To enhance the performance of our model, we have employed a multi-task loss variance penalty and combined several training objectives, including the Cross-Entropy loss, the Dice loss, the Tversky loss, and the Focal loss. Besides, we have studied the performance of three existing Arabic PLMs for sentence encoding. On the official test set, our system has obtained a micro-F1 score of 0.9113 and 0.9303 for Flat (Sub-Task 1) and Nested (Sub-Task 2) NER, respectively. It has been ranked in the 6th and the 2nd positions among all participating systems in Sub-Task 1 and Sub-Task 2, respectively.",
        "authors": [
          "Abdelkader El Mahdaouy",
          "Salima Lamsiyah",
          "Hamza Alami",
          "Christoph Schommer",
          "Ismail Berrada"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "777",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "782",
        "paper_id": 87,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.87.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.87.jpg",
        "title": "UM6P & UL at WojoodNER shared task: Improving Multi-Task Learning for Flat and Nested Arabic Named Entity Recognition",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.87",
        "x": -2.9355661869049072,
        "y": 0.8341048955917358,
        "year": "2023"
      },
      {
        "abstract": "Product information in e-commerce is usually localized using machine translation (MT) systems. Arabic language has rich morphology and dialectal variations, so Arabic MT in e-commerce training requires a larger volume of data from diverse data sources; Given the dynamic nature of e-commerce, such data needs to be acquired periodically to update the MT. Consequently, validating the quality of training data periodically within an industrial setting presents a notable challenge. Meanwhile, the performance of MT systems is significantly impacted by the quality and appropriateness of the training data. Hence, this study first examines the Arabic MT in e-commerce and investigates the data quality challenges for English-Arabic MT in e-commerce then proposes heuristics-based and topic-based data selection approaches to improve MT for product information. Both online and offline experiment results have shown our proposed approaches are effective, leading to improved shopping experiences for customers.",
        "authors": [
          "Bryan Zhang",
          "Salah Danial",
          "Stephan Walter"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "150",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "157",
        "paper_id": 13,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.13.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.13.jpg",
        "title": "Enhancing Arabic Machine Translation for E-commerce Product Information: Data Quality Challenges and Innovative Selection Approaches",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.13",
        "x": -5.493224620819092,
        "y": 3.2118940353393555,
        "year": "2023"
      },
      {
        "abstract": "We present an overview of the ArAIEval shared task, organized as part of the first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two tasks over Arabic text: (1) persuasion technique detection, focusing on identifying persuasion techniques in tweets and news articles, and (2) disinformation detection in binary and multiclass setups over tweets. A total of 20 teams participated in the final evaluation phase, with 14 and 16 teams participating in Task 1 and Task 2, respectively. Across both tasks, we observe that fine-tuning transformer models such as AraBERT is the core of majority of participating systems. We provide a description of the task setup, including description of datasets construction and the evaluation setup. We also provide a brief overview of the participating systems. All datasets and evaluation scripts from the shared task are released to the research community. We hope this will enable further research on such important tasks within the Arabic NLP community.",
        "authors": [
          "Maram Hasanain",
          "Firoj Alam",
          "Hamdy Mubarak",
          "Samir Abdaljalil",
          "Wajdi Zaghouani",
          "Preslav Nakov",
          "Giovanni Da San Martino",
          "Abed Freihat"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "483",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "493",
        "paper_id": 44,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.44.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.44.jpg",
        "title": "ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.44",
        "x": -6.317139625549316,
        "y": 0.43942415714263916,
        "year": "2023"
      },
      {
        "abstract": "The rise of propaganda and disinformation in the digital age has necessitated the development of effective detection methods to combat the spread of deceptive information. In this paper we present our approach proposed for ArAIEval shared task : propaganda and disinformation detection in Arabic text. Our system utilised different pre-trained BERT based models, that makes use of prompt-learning based on knowledgeable expansion and prefix-tuning. The proposed approach secured third place in subtask-1A with 0.7555 F1-micro score, second place in subtask-1B with 0.5658 F1-micro score. However, for subtask-2A & 2B, the proposed system achieved fourth place with an F1-micro score of 0.9040, 0.8219 respectively. Our findings suggest that prompt-tuning-based & prefix-tuning based models performed better than conventional fine-tuning. Furthermore, using loss aware class imbalance, improved performance.",
        "authors": [
          "Reem Abdel-Salam"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "536",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "542",
        "paper_id": 52,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.52.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.52.jpg",
        "title": "rematchka at ArAIEval Shared Task: Prefix-Tuning & Prompt-tuning for Improved Detection of Propaganda and Disinformation in Arabic Social Media Content",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.52",
        "x": -4.642744064331055,
        "y": 0.1605653017759323,
        "year": "2023"
      },
      {
        "abstract": "In this paper, we describe our participation in the NADI2023 shared task for the classification of Arabic dialects in tweets. For training, evaluation, and testing purposes, a primary dataset comprising tweets from 18 Arab countries is provided, along with three older datasets. The main objective is to develop a model capable of classifying tweets from these 18 countries. We outline our approach, which leverages various machine learning models. Our experiments demonstrate that large language models, particularly Arabertv2-Large, Arabertv2-Base, and CAMeLBERT-Mix DID MADAR, consistently outperform traditional methods such as SVM, XGBOOST, Multinomial Naive Bayes, AdaBoost, and Random Forests.",
        "authors": [
          "Yash Hatekar",
          "Muhammad Abdo"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "665",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "669",
        "paper_id": 72,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.72.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.72.jpg",
        "title": "IUNADI at NADI 2023 shared task: Country-level Arabic Dialect Classification in Tweets for the Shared Task NADI 2023",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.72",
        "x": -4.557359218597412,
        "y": 4.387223720550537,
        "year": "2023"
      },
      {
        "abstract": "This paper presents N\u00e2bra (\u0646\u064e\u0628\u0652\u0631\u064e\u0629), a corpora of Syrian Arabic dialects with morphological annotations. A team of Syrian natives collected more than6Ksentences containing about60Kwords from several sources including social media posts, scripts of movies and series, lyrics of songs and local proverbs to build N\u00e2bra. N\u00e2bra covers several local Syrian dialects including those of Aleppo, Damascus, Deir-ezzur, Hama, Homs, Huran, Latakia, Mardin, Raqqah, and Suwayda. A team of nine annotators annotated the60Ktokens with full morphological annotations across sentence contexts. We trained the annotators to follow methodological annotation guidelines to ensure unique morpheme annotations, and normalized the annotations. F1 and\ud835\udf05agreement scores ranged between 74% and 98% across features, showing the excellent quality of N\u00e2bra annotations. Our corpora are open-source and publicly available as part of the Currasat portal https://sina.birzeit.edu/currasat.",
        "authors": [
          "Amal Nayouf",
          "Tymaa Hammouda",
          "Mustafa Jarrar",
          "Fadi Zaraket",
          "Mohamad-Bassam Kurdy"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "12",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "23",
        "paper_id": 2,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.2.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.2.jpg",
        "title": "N\u00e2bra: Syrian Arabic Dialects with Morphological Annotations",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.2",
        "x": -7.0432257652282715,
        "y": 1.0141174793243408,
        "year": "2023"
      },
      {
        "abstract": "Traditional NER systems are typically trained to recognize coarse-grained categories of entities, and less attention is given to classifying entities into a hierarchy of fine-grained lower-level sub-types. This article aims to advance Arabic NER with fine-grained entities. We chose to extend Wojood (an open-source Nested Arabic Named Entity Corpus) with sub-types. In particular, four main entity types in Wojood (geopolitical entity (GPE), location (LOC), organization (ORG), and facility (FAC) are extended with 31 sub-types of entities. To do this, we first revised Wojood\u2019s annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC\u2019s ACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC, ORG, and FAC (~ 44K) in Wojood are manually annotated with the LDC\u2019s ACE subtypes. This extended version of Wojood is called WojoodFine. To evaluate our annotations, we measured the inter-annotator agreement (IAA) using both Cohen\u2019s Kappa and F1 score, resulting in 0.9861 and 0.9889, respectively. To compute the baselines of WojoodFine, we fine-tune three pre-trained Arabic BERT encoders in three settings: flat NER, nested NER and nested NER with sub-types and achieved F1 score of 0.920, 0.866, and 0.885, respectively. Our corpus and models are open source and available at https://sina.birzeit.edu/wojood/.",
        "authors": [
          "Haneen Abdallatif Liqreina",
          "Mustafa Jarrar",
          "Mohammed Khalilia",
          "Ahmed Oumar El-Shangiti",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "310",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "323",
        "paper_id": 25,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.25.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.25.jpg",
        "title": "Arabic Fine-Grained Entity Recognition",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.25",
        "x": -5.7467827796936035,
        "y": 0.508769154548645,
        "year": "2023"
      },
      {
        "abstract": "The remarkable capabilities of Natural Language Models to grasp language subtleties has paved the way for their widespread adoption in diverse fields. However, adapting them for specific tasks requires the time-consuming process of fine-tuning, which consumes significant computational power and energy. Therefore, optimizing the fine-tuning time is advantageous. In this study, we propose an alternate approach that limits parameter manipulation to select layers. Our exploration led to identifying layers that offer the best trade-off between time optimization and performance preservation. We further validated this approach on multiple downstream tasks, and the results demonstrated its potential to reduce fine-tuning time by up to 50% while maintaining performance within a negligible deviation of less than 5%. This research showcases a promising technique for significantly improving fine-tuning efficiency without compromising task- or domain-specific learning capabilities.",
        "authors": [
          "Abir Betka",
          "Zeyd Ferhat",
          "Riyadh Barka",
          "Selma Boutiba",
          "Zineddine Kahhoul",
          "Tiar Lakhdar",
          "Ahmed Abdelali",
          "Habiba Dahmani"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "405",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "410",
        "paper_id": 33,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.33.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.33.jpg",
        "title": "On Enhancing Fine-Tuning for Pre-trained Language Models",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.33",
        "x": -6.786996364593506,
        "y": -0.2835753262042999,
        "year": "2023"
      },
      {
        "abstract": "In this paper we present our approach towards Arabic Dialect identification which was part of the Fourth Nuanced Arabic Dialect Identification Shared Task (NADI 2023). We tested several techniques to identify Arabic dialects. We obtained the best result by fine-tuning the pre-trained MARBERTv2 model with a modified training dataset. The training set was expanded by sorting tweets based on dialects, concatenating every two adjacent tweets, and adding them to the original dataset as new tweets. We achieved 82.87 on F1 score and we were at the seventh position among 16 participants.",
        "authors": [
          "Abduslam F A Nwesri",
          "Nabila A S Shinbir",
          "Hassan Ebrahem"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "620",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "624",
        "paper_id": 64,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.64.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.64.jpg",
        "title": "UoT at NADI 2023 shared task: Automatic Arabic Dialect Identification is Made Possible",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.64",
        "x": -4.5081939697265625,
        "y": 0.6325613260269165,
        "year": "2023"
      },
      {
        "abstract": "In an era of widespread digital communication, the challenge of identifying and countering disinformation has become increasingly critical. However, compared to the solutions available in the English language, the resources and strategies for tackling this multifaceted problem in Arabic are relatively scarce. To address this issue, this paper presents our solutions to tasks in ArAIEval 2023. Task 1 focuses on detecting persuasion techniques, while Task 2 centers on disinformation detection within Arabic text. Leveraging a multi-head model architecture, fine-tuning techniques, sequential learning, and innovative activation functions, our contributions significantly enhance persuasion techniques and disinformation detection accuracy. Beyond improving performance, our work fills a critical research gap in content analysis for Arabic, empowering individuals, communities, and digital platforms to combat deceptive content effectively and preserve the credibility of information sources within the Arabic-speaking world.",
        "authors": [
          "Hariram Veeramani",
          "Surendrabikram Thapa",
          "Usman Naseem"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "519",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "524",
        "paper_id": 49,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.49.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.49.jpg",
        "title": "KnowTellConvince at ArAIEval Shared Task: Disinformation and Persuasion Detection in Arabic using Similar and Contrastive Representation Alignment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.49",
        "x": -6.949194431304932,
        "y": 0.7280840873718262,
        "year": "2023"
      },
      {
        "abstract": "Our system, submitted to the Nuanced Arabic Dialect Identification (NADI-23), tackles the first sub-task: Closed Country-level dialect identification. In this work, we propose a model that is based on an ensemble of layer-wise fine-tuned BERT-based models. The proposed model ranked fourth out of sixteen submissions, with an F1-macro score of 85.43.",
        "authors": [
          "Nada Almarwani",
          "Samah Aloufi"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "625",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "630",
        "paper_id": 65,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.65.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.65.jpg",
        "title": "SANA at NADI 2023 shared task: Ensemble of Layer-Wise BERT-based models for Dialectal Arabic Identification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.65",
        "x": -6.988103866577148,
        "y": 0.023284699767827988,
        "year": "2023"
      },
      {
        "abstract": "Most recent models for Arabic topic classification leveraged fine-tuning existing pre-trained transformer models and targeted a limited number of categories. More recently, advances in automated ML and generative models introduced novel potentials for the task. While these approaches work for English, it is a question of whether they perform well for low-resourced languages; Arabic in particular. This paper presents (i) ArBoNeClass; a novel Arabic dataset with an extended 14-topic class set covering modern books from social sciences and humanities along with newspaper articles, and (ii) a set of topic classifiers built from it. We finetuned an open LLM model to build ArGTClass. We compared its performance against the best models built with Vertex AI (Google), AutoML(H2O), and AutoTrain(HuggingFace). ArGTClass outperformed the VertexAi and AutoML models and was reasonably similar to the AutoTrain model.",
        "authors": [
          "Doha Albared",
          "Hadi Hamoud",
          "Fadi Zaraket"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "399",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "404",
        "paper_id": 32,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.32.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.32.jpg",
        "title": "Arabic Topic Classification in the Generative and AutoML Era",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.32",
        "x": -3.9137916564941406,
        "y": 2.925525188446045,
        "year": "2023"
      },
      {
        "abstract": "Arabic is one of the most globally spoken languages with more than 313 million speakers worldwide. Arabic handwriting is known for its cursive nature and the variety of writing styles used. Despite the increase in effort to digitize artistic and historical elements, no public dataset was released to deal with Arabic text recognition for realistic manuscripts and calligraphic text. We present the Handwriting Identification of Manuscripts and Calligraphy in Arabic (HICMA) dataset as the first publicly available dataset with real-world and diverse samples of Arabic handwritten text in manuscripts and calligraphy. With more than 5,000 images across five different styles, the HICMA dataset includes image-text pairs and style labels for all images. We further present a comparison of the current state-of-the-art optical character recognition models in Arabic and benchmark their performance on the HICMA dataset, which serves as a baseline for future works. Both the HICMA dataset and its benchmarking tool are made available to the public under the CC BY-NC 4.0 license in the hope that the presented work opens the door to further enhancements of complex Arabic text recognition.",
        "authors": [
          "Anis Ismail",
          "Zena Kamel",
          "Reem Mahmoud"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "24",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "32",
        "paper_id": 3,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.3.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.3.jpg",
        "title": "HICMA: The Handwriting Identification for Calligraphy and Manuscripts in Arabic Dataset",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.3",
        "x": -5.449769020080566,
        "y": 0.520147979259491,
        "year": "2023"
      },
      {
        "abstract": "Research studies on Machine Translation (MT) between Modern Standard Arabic (MSA) and English are abundant. However, studies on MT between Omani Arabic (OA) dialects and English are very scarce. This research study focuses on the lack of availability of an Omani dialect parallel dataset, as well as MT of OA to English. The study uses social media data from X (formerly Twitter) to build an authentic parallel text of the Omani dialects. The research presents baseline results on this dataset using Google Translate, Microsoft Translation, and Marian NMT. A taxonomy of the most common linguistic errors is used to analyze the translations made by the NMT systems to provide insights on future improvements. Finally, transfer learning is used to adapt Marian NMT to the Omani dialect, which significantly improved by 9.88 points in the BLEU score.",
        "authors": [
          "Khoula Al-Kharusi",
          "Abdurahman AAlAbdulsalam"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "302",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "309",
        "paper_id": 24,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.24.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.24.jpg",
        "title": "Machine Translation of Omani Arabic Dialect from Social Media",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.24",
        "x": -6.755408763885498,
        "y": 0.18863724172115326,
        "year": "2023"
      },
      {
        "abstract": "The Helsinki-NLP team participated in the NADI 2023 shared tasks on Arabic dialect translation with seven submissions. We used statistical (SMT) and neural machine translation (NMT) methods and explored character- and subword-based data preprocessing. Our submissions placed second in both tracks. In the open track, our winning submission is a character-level SMT system with additional Modern Standard Arabic language models. In the closed track, our best BLEU scores were obtained with the leave-as-is baseline, a simple copy of the input, and narrowly followed by SMT systems. In both tracks, fine-tuning existing multilingual models such as AraT5 or ByT5 did not yield superior performance compared to SMT.",
        "authors": [
          "Yves Scherrer",
          "Aleksandra Mileti\u0107",
          "Olli Kuparinen"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "670",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "677",
        "paper_id": 73,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.73.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.73.jpg",
        "title": "The Helsinki-NLP Submissions at NADI 2023 Shared Task: Walking the Baseline",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.73",
        "x": -4.421606540679932,
        "y": 2.3671908378601074,
        "year": "2023"
      },
      {
        "abstract": "This paper outlines a methodology aimed at combating disinformation in Arabic social media, a strategy that secured a first-place finish in tasks 2A and 2B at the ArAIEval shared task during the ArabicNLP 2023 conference. Our team, DetectiveRedasers, developed a hyperparameter-optimized pipeline centered around singular BERT-based models for the Arabic language, enhanced by a soft-voting ensemble strategy. Subsequent evaluation on the test dataset reveals that ensembles, although generally resilient, do not always outperform individual models. The primary contributions of this paper are its multifaceted strategy, which led to winning solutions for both binary (2A) and multiclass (2B) disinformation classification tasks.",
        "authors": [
          "Bryan Tuck",
          "Fatima Zahra Qachfar",
          "Dainis Boumber",
          "Rakesh Verma"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "494",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "501",
        "paper_id": 45,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.45.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.45.jpg",
        "title": "DetectiveRedasers at ArAIEval Shared Task: Leveraging Transformer Ensembles for Arabic Deception Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.45",
        "x": -6.302113056182861,
        "y": 1.9635961055755615,
        "year": "2023"
      },
      {
        "abstract": "In this paper, we study the transferability of Named Entity Recognition (NER) models between Arabic dialects. This question is important because the available manually-annotated resources are not distributed equally across dialects: Modern Standard Arabic (MSA) is much richer than other dialects for which little to no datasets exist. How well does a NER model, trained on MSA, perform on other dialects? To answer this question, we construct four datasets. The first is an MSA dataset extracted from the ACE 2005 corpus. The others are datasets for Egyptian, Morocan and Syrian which we manually annotate following the ACE guidelines. We train a span-based NER model on top of a pretrained language model (PLM) encoder on the MSA data and study its performance on the other datasets in zero-shot settings. We study the performance of multiple PLM encoders from the literature and show that they achieve acceptable performance with no annotation effort. Our annotations and models are publicly available (https://github.com/niamaelkhbir/Arabic-Cross-Dialectal-NER).",
        "authors": [
          "Niama El Elkhbir",
          "Urchade Zaratiana",
          "Nadi Tomeh",
          "Thierry Charnois"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "140",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "149",
        "paper_id": 12,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.12.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.12.jpg",
        "title": "Cross-Dialectal Named Entity Recognition in Arabic",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.12",
        "x": -5.8140997886657715,
        "y": 3.5026142597198486,
        "year": "2023"
      },
      {
        "abstract": "In this paper, we conduct an in-depth analysis of several key factors influencing the performance of Arabic Dialect Identification NADI\u20192023, with a specific focus on the first subtask involving country-level dialect identification. Our investigation encompasses the effects of surface preprocessing, morphological preprocessing, FastText vector model, and the weighted concatenation of TF-IDF features. For classification purposes, we employ the Linear Support Vector Classification (LSVC) model. During the evaluation phase, our system demonstrates noteworthy results, achieving an F1score of 62.51%. This achievement closely aligns with the average F1scores attained by other systems submitted for the first subtask, which stands at 72.91%.",
        "authors": [
          "Mohamed Lichouri",
          "Khaled Lounnas",
          "Aicha Zitouni",
          "Houda Latrache",
          "Rachida Djeradi"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "647",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "651",
        "paper_id": 69,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.69.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.69.jpg",
        "title": "USTHB at NADI 2023 shared task: Exploring Preprocessing and Feature Engineering Strategies for Arabic Dialect Identification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.69",
        "x": -4.505796432495117,
        "y": 4.086145401000977,
        "year": "2023"
      },
      {
        "abstract": "Named entity recognition (NER) is one of many challenging tasks in Arabic Natural Language Processing. It is also the base of many critical downstream tasks to help understand the source of major trends and public opinion. In this paper, we will describe our submission in the NER Shared Task of ArabicNLP 2023. We used a simple machine reading comprehension-based technique in the Flat NER Subtask ranking eighth on the leaderboard, while we fine-tuned a language model for the Nested NER Subtask ranking third on the leaderboard.",
        "authors": [
          "Shereen Elkordi",
          "Noha Adly",
          "Marwan Torki"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "771",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "776",
        "paper_id": 86,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.86.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.86.jpg",
        "title": "AlexU-AIC at WojoodNER shared task: Sequence Labeling vs MRC and SWA for Arabic Named Entity Recognition",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.86",
        "x": -4.546307563781738,
        "y": 4.016441345214844,
        "year": "2023"
      },
      {
        "abstract": "Delegating short answer grading to automated systems enhances efficiency, giving teachers more time for vital human-centered aspects of education. Studies in automatic short answer grading (ASAG) approach the problem from instance-based or reference-based perspectives. Recent studies have favored instance-based methods, but they demand substantial data for training, which is often scarce in classroom settings. This study compares both approaches using an Arabic ASAG dataset. We employ in-context meta-learning for instance-based and semantic score-based similarity for reference-based grading. Results show both methods outperform a baseline and occasionally even surpass human raters when grading unseen answers. Notably, the semantic score-based similarity approach excels in zero-shot settings, outperforming in-context meta-learning. Our work contributes insights to Arabic ASAG and introduces a prompt category classification model, leveraging GPT3.5 to augment Arabic data for improved performance.",
        "authors": [
          "Menna Fateen",
          "Tsunenori Mine"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "350",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "358",
        "paper_id": 28,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.28.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.28.jpg",
        "title": "In-Context Meta-Learning vs. Semantic Score-Based Similarity: A Comparative Study in Arabic Short Answer Grading",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.28",
        "x": -3.7898526191711426,
        "y": 3.2646360397338867,
        "year": "2023"
      },
      {
        "abstract": "Disinformation involves the dissemination of incomplete, inaccurate, or misleading information; it has the objective, goal, or purpose of deliberately or intentionally lying to others aboutthe truth. The spread of disinformative information on social media has serious implications, and it causes concern among internet users in different aspects. Automatic classification models are required to detect disinformative posts on social media, especially on Twitter. In this article, DistilBERT multilingual model was fine-tuned to classify tweets either as dis-informative or not dis-informative in Subtask 2A of the ArAIEval shared task. The system outperformed the baseline and achieved F1 micro 87% and F1 macro 80%. Our system ranked 11 compared with all participants.",
        "authors": [
          "Areej Jaber",
          "Paloma Mart\u00ednez"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "525",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "529",
        "paper_id": 50,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.50.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.50.jpg",
        "title": "PTUK-HULAT at ArAIEval Shared Task Fine-tuned Distilbert to Predict Disinformative Tweets",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.50",
        "x": -4.568941116333008,
        "y": 2.3798816204071045,
        "year": "2023"
      },
      {
        "abstract": "Propaganda frequently employs sophisticated persuasive strategies in order to influence public opinion and manipulate perceptions. As a result, automating the detection of persuasive techniques is critical in identifying and mitigating propaganda on social media and in mainstream media. This paper proposes a set of transformer-based models for detecting persuasive techniques in tweets and news that incorporate content type information as extra features or as an extra learning objective in a multitask learning setting. In addition to learning to detect the presence of persuasive techniques in text, our best model learns specific syntactic and lexical cues used to express them based on text genre (type) as an auxiliary task. To optimize the model and deal with data imbalance, a focal loss is used. As part of ArabicNLP2023-ArAIEval shared task, this model achieves the highest score in the shared task 1A out of 13 participants, according to the official results, with a micro-F1 of 76.34% and a macro-F1 of 73.21% on the test dataset.",
        "authors": [
          "Khaldi Hadjer",
          "Taqiy Bouklouha"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "502",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "507",
        "paper_id": 46,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.46.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.46.jpg",
        "title": "HTE at ArAIEval Shared Task: Integrating Content Type Information in Binary Persuasive Technique Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.46",
        "x": -4.341548442840576,
        "y": 3.5533125400543213,
        "year": "2023"
      },
      {
        "abstract": "The Wojood Named Entity Recognition (NER) shared task introduces a comprehensive Arabic NER dataset encompassing both flat and nested entity tasks, addressing the challenge of limited Arabic resources. In this paper, we present our teamLIPNapproach to addressing the two subtasks of WojoodNER SharedTask. We frame NER as a span classification problem. We employ a pretrained language model for token representations and neural network classifiers. We use global decoding for flat NER and a greedy strategy for nested NER. Our model secured the first position in flat NER and the fourth position in nested NER during the competition, with an F-score of 91.96 and 92.45 respectively. Our code is publicly available (https://github.com/niamaelkhbir/LIPN-at-WojoodSharedTask).",
        "authors": [
          "Niama El Khbir",
          "Urchade Zaratiana",
          "Nadi Tomeh",
          "Thierry Charnois"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "789",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "796",
        "paper_id": 89,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.89.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.89.jpg",
        "title": "LIPN at WojoodNER shared task: A Span-Based Approach for Flat and Nested Arabic Named Entity Recognition",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.89",
        "x": -4.401556015014648,
        "y": 2.6395180225372314,
        "year": "2023"
      },
      {
        "abstract": "Arabic dialects have extensive global usage owing to their significance and the vast number of Arabic speakers. However, technological progress and globalization are leading to significant transformations within Arabic dialects. They are acquiring new characteristics involving novel vocabulary and integrating of linguistic elements from diverse dialects. Consequently, sentiment analysis of these dialects is becoming more challenging. This study categorizes dialects among 18 countries, as introduced by the Nuanced Arabic Dialect Identification (NADI) shared task competition. Our approach incorporates the utilization of the MARABERT and MARABERT v2 models with a range of methodologies, including a feature extraction process. Our findings reveal that the most effective model is achieved by applying averaging and concatenation to the hidden layers of MARABERT v2, followed by feeding the resulting output into convolutional layers. Furthermore, employing the ensemble method on various methods enhances the model\u2019s performance. Our system secures the 6th position among the top performers in the First subtask, achieving an F1 score of 83.73%.",
        "authors": [
          "Shorouk Adel",
          "Noureldin Elmadany"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "631",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "636",
        "paper_id": 66,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.66.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.66.jpg",
        "title": "ISL-AAST at NADI 2023 shared task: Enhancing Arabic Dialect Identification in the Era of Globalization and Technological Progress",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.66",
        "x": -5.6137189865112305,
        "y": 3.7519924640655518,
        "year": "2023"
      },
      {
        "abstract": "Automatic Arabic Dialect Identification (ADI) of text has gained great popularity since it was introduced in the early 2010s. Multiple datasets were developed, and yearly shared tasks have been running since 2018. However, ADI systems are reported to fail in distinguishing between the micro-dialects of Arabic. We argue that the currently adopted framing of the ADI task as a single-label classification problem is one of the main reasons for that. We highlight the limitation of the incompleteness of the Dialect labels and demonstrate how it impacts the evaluation of ADI systems. A manual error analysis for the predictions of an ADI, performed by 7 native speakers of different Arabic dialects, revealed that\u224866% of the validated errors are not true errors. Consequently, we propose framing ADI as a multi-label classification task and give recommendations for designing new ADI datasets.",
        "authors": [
          "Amr Keleg",
          "Walid Magdy"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "385",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "398",
        "paper_id": 31,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.31.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.31.jpg",
        "title": "Arabic Dialect Identification under Scrutiny: Limitations of Single-label Classification",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.31",
        "x": -2.060145139694214,
        "y": 0.8348662853240967,
        "year": "2023"
      },
      {
        "abstract": "Large Language Models (LLMs) such as ChatGPT and Bard AI have gained much attention due to their outstanding performance on a range of NLP tasks. These models have demonstrated remarkable proficiency across various languages without the necessity for full supervision. Nevertheless, their performance in low-resource languages and dialects, like Arabic dialects in comparison to English, remains to be investigated. In this paper, we conduct a comprehensive evaluation of three LLMs for Dialectal Arabic Sentiment Analysis: namely, ChatGPT based on GPT-3.5 and GPT-4, and Bard AI. We use a Saudi dialect Twitter dataset to assess their capability in sentiment text classification and generation. For classification, we compare the performance of fully fine-tuned Arabic BERT-based models with the LLMs in few-shot settings. For data generation, we evaluate the quality of the generated new sentiment samples using human and automatic evaluation methods. The experiments reveal that GPT-4 outperforms GPT-3.5 and Bard AI in sentiment analysis classification, rivaling the top-performing fully supervised BERT-based language model. However, in terms of data generation, compared to manually annotated authentic data, these generative models often fall short in producing high-quality Dialectal Arabic text suitable for sentiment analysis.",
        "authors": [
          "Abdulmohsen Al-Thubaity",
          "Sakhar Alkhereyf",
          "Hanan Murayshid",
          "Nouf Alshalawi",
          "Maha Omirah",
          "Raghad Alateeq",
          "Rawabi Almutairi",
          "Razan Alsuwailem",
          "Manal Alhassoun",
          "Imaan Alkhanen"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "335",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "349",
        "paper_id": 27,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.27.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.27.jpg",
        "title": "Evaluating ChatGPT and Bard AI on Arabic Sentiment Analysis",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.27",
        "x": -5.267094612121582,
        "y": -0.2818324863910675,
        "year": "2023"
      },
      {
        "abstract": "Dialect identification systems play a significant role in various fields and applications as in speech and language technologies, facilitating language education, supporting sociolinguistic research, preserving linguistic diversity, enhancing text-to-speech systems. In this paper, we provide our findings and results in NADI 2023 shared task for country-level dialect identification and machine translation (MT) from dialect to MSA. The proposed models achieved an F1-score of 86.18 at the dialect identification task, securing second place in first subtask. Whereas for the machine translation task, the submitted model achieved a BLEU score of 11.37 securing fourth and third place in second and third subtask. The proposed model utilizes parameter efficient training methods which achieves better performance when compared to conventional fine-tuning during the experimentation phase.",
        "authors": [
          "Reem Abdel-Salam"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "652",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "657",
        "paper_id": 70,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.70.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.70.jpg",
        "title": "rematchka at NADI 2023 shared task: Parameter Efficient tuning for Dialect Identification and Dialect Machine Translation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.70",
        "x": -5.722777366638184,
        "y": 3.4558820724487305,
        "year": "2023"
      },
      {
        "abstract": "This paper presents the methods we developed for the Nuanced Arabic Dialect Identification (NADI) 2023 shared task, specifically targeting the two subtasks focussed on sentence-level machine translation (MT) of text written in any of four Arabic dialects (Egyptian, Emirati, Jordanian and Palestinian) to Modern Standard Arabic (MSA). Our team, UniManc, employed models based on T5: multilingual T5 (mT5), multi-task fine-tuned mT5 (mT0) and AraT5. These models were trained based on two configurations: joint model training for all regional dialects (J-R) and independent model training for every regional dialect (I-R). Based on the results of the official NADI 2023 evaluation, our I-R AraT5 model obtained an overall BLEU score of 14.76, ranking first in the Closed Dialect-to-MSA MT subtask. Moreover, in the Open Dialect-to-MSA MT subtask, our J-R AraT5 model also ranked first, obtaining an overall BLEU score of 21.10.",
        "authors": [
          "Abdullah Khered",
          "Ingy Abdelhalim",
          "Nadine Abdelhalim",
          "Ahmed Soliman",
          "Riza Theresa Batista-Navarro"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "658",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "664",
        "paper_id": 71,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.71.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.71.jpg",
        "title": "UniManc at NADI 2023 Shared Task: A Comparison of Various T5-based Models for Translating Arabic Dialectical Text to Modern Standard Arabic",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.71",
        "x": -5.413409233093262,
        "y": -0.14976456761360168,
        "year": "2023"
      },
      {
        "abstract": "Although image captioning has a vast array of applications, it has not reached its full potential in languages other than English. Arabic, for instance, although the native language of more than 400 million people, remains largely underrepresented in this area. This is due to the lack of labeled data and powerful Arabic generative models. We alleviate this issue by presenting a novel vision-language model dedicated to Arabic, dubbed Violet. Our model is based on a vision encoder and a Gemini text decoder that maintains generation fluency while allowing fusion between the vision and language components. To train our model, we introduce a new method for automatically acquiring data from available English datasets. We also manually prepare a new dataset for evaluation. Violet performs sizeably better than our baselines on all of our evaluation datasets. For example, it reaches a CIDEr score of 61.2 on our manually annotated dataset and achieves an improvement of 13 points on Flickr8k.",
        "authors": [
          "Abdelrahman Mohamed",
          "Fakhraddin Alwajih",
          "El-Moatez-Billah Nagoudi",
          "Alcides Inciarte",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "1",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "11",
        "paper_id": 1,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.1.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.1.jpg",
        "title": "Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.1",
        "x": -4.573538303375244,
        "y": 4.032142639160156,
        "year": "2023"
      },
      {
        "abstract": "Numerous languages exhibit shared characteristics, especially in morphological features. For instance, Arabic and Russian both belong to the fusional language category. The question arises: Do such common traits influence language comprehension across diverse linguistic backgrounds? This study explores the possibility of transferring comprehension skills across languages to Arabic in a zero-shot scenario. Specifically, we demonstrate that training language models on other languages can enhance comprehension of Arabic, as evidenced by our evaluations in three key tasks: natural language inference, question answering, and named entity recognition. Our experiments reveal that certain morphologically rich languages (MRLs), such as Russian, display similarities to Arabic when assessed in a zero-shot context, particularly in tasks like question answering and natural language inference. However, this similarity is less pronounced in tasks like named entity recognition.",
        "authors": [
          "Zaid Alyafeai",
          "Moataz Ahmed"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "324",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "334",
        "paper_id": 26,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.26.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.26.jpg",
        "title": "Investigating Zero-shot Cross-lingual Language Understanding for Arabic",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.26",
        "x": -3.8143820762634277,
        "y": 3.270150661468506,
        "year": "2023"
      },
      {
        "abstract": "This paper provides a systematic analysis and comparison of the performance of state-of-the-art models on the task of fine-grained Arabic dialect identification using the MADAR parallel corpus. We test approaches based on pre-trained transformer language models in addition to Naive Bayes models with a rich set of various features. Through a comprehensive data- and error analysis, we provide valuable insights into the strengths and weaknesses of both approaches. We discuss which dialects are more challenging to differentiate, and identify potential sources of errors. Our analysis reveals an important problem with identical sentences across dialect classes in the test set of the MADAR-26 corpus, which may confuse any classifier. We also show that none of the tested approaches captures the subtle distinctions between closely related dialects.",
        "authors": [
          "Helene Olsen",
          "Samia Touileb",
          "Erik Velldal"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "370",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "384",
        "paper_id": 30,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.30.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.30.jpg",
        "title": "Arabic dialect identification: An in-depth error analysis on the MADAR parallel corpus",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.30",
        "x": -4.598871231079102,
        "y": 3.965402364730835,
        "year": "2023"
      },
      {
        "abstract": "This paper presents Arabic named entity recognition models by employing the single-task and the multi-task learning paradigms. The models have been developed using character-based contextualized Embeddings from Language Model (ELMo) in the input layers of the bidirectional long-short term memory networks. The ELMo embeddings are quite capable of learning the morphology and contextual information of the tokens in word sequences. The single-task learning models outperformed the multi-task learning models and achieved micro F1-scores of 0.8751 and 0.8884 for the flat and nested annotations, respectively.",
        "authors": [
          "Toqeer Ehsan",
          "Amjad Ali",
          "Ala Al-Fuqaha"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "783",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "788",
        "paper_id": 88,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.88.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.88.jpg",
        "title": "AlphaBrains at WojoodNER shared task: Arabic Named Entity Recognition by Using Character-based Context-Sensitive Word Representations",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.88",
        "x": -4.574339866638184,
        "y": 4.122640609741211,
        "year": "2023"
      },
      {
        "abstract": "We present our system designed for Subtask 1 in the shared task NADI on Arabic Dialect Identification, which is part of ArabicNLP 2023. In our approach, we utilized models such as: MARBERT, MARBERTv2 (A) and MARBERTv2 (B). Subsequently, we created a majority voting ensemble of these models. We used MARBERTv2 with different hyperparameters, which significantly improved the overall performance of the ensemble model. In terms of performance, our systems achieved a competitive an F1 score of84.76. Overall, our system secured the5thposition out of 16 participating teams.",
        "authors": [
          "Dilshod Azizov",
          "Jiyong Li",
          "Shangsong Liang"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "637",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "641",
        "paper_id": 67,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.67.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.67.jpg",
        "title": "Frank at NADI 2023 Shared Task: Trio-Based Ensemble Approach for Arabic Dialect Identification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.67",
        "x": -5.839517116546631,
        "y": 3.6012542247772217,
        "year": "2023"
      },
      {
        "abstract": "This paper describes our submissions to the WojoodNER shared task organized during the first ArabicNLP conference. We participated in the two proposed sub-tasks of flat and nested Named Entity Recognition (NER). Our systems were ranked first over eight and third over eleven in the Nested NER and Flat NER, respectively. All our primary submissions are based on DiffusionNER models (Shen et al., 2023), where the NER task is formulated as a boundary-denoising diffusion process. Experiments on nested WojoodNER achieves the best results with a micro F1-score of 93.73%. For the flat sub-task, our primary system was the third-best system, with a micro F1-score of 91.92%.",
        "authors": [
          "Imen Laouirine",
          "Haroun Elleuch",
          "Fethi Bougares"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "759",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "764",
        "paper_id": 84,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.84.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.84.jpg",
        "title": "ELYADATA at WojoodNER Shared Task: Data and Model-centric Approaches for Arabic Flat and Nested NER",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.84",
        "x": -5.768199443817139,
        "y": 0.9127110242843628,
        "year": "2023"
      },
      {
        "abstract": "Recent advancements in self-supervised speech-representation learning for automatic speech recognition (ASR) approaches have significantly improved the results on many benchmarks with low-cost data labeling. In this paper, we train two self-supervised frameworks for ASR, namely wav2vec, and data2vec, in which we conduct multiple experiments and analyze their results. Furthermore, we introduce Aswat dataset, which covers multiple genres and features speakers with vocal variety. Aswat contains 732 hours of clean Arabic speech that can be used in the pretraining task for learning latent speech representations, which results in achieving a lower word error rate (WER) in Arabic ASR. We report the baseline results and achieve state-of-the-art WERs of 11.7% and 10.3% on Common Voice (CV) and the second round of Multi-Genre Broadcast (MGB-2) respectively, as a result of including our dataset Aswat.",
        "authors": [
          "Lamya Alkanhal",
          "Abeer Alessa",
          "Elaf Almahmoud",
          "Rana Alaqil"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "120",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "127",
        "paper_id": 10,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.10.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.10.jpg",
        "title": "Aswat: Arabic Audio Dataset for Automatic Speech Recognition Using Speech-Representation Learning",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.10",
        "x": -6.372864246368408,
        "y": 1.9231044054031372,
        "year": "2023"
      },
      {
        "abstract": "In this research paper, we undertake a comprehensive examination of several pivotal factors that impact the performance of Arabic Disinformation Detection in the ArAIEval\u20192023 shared task. Our exploration encompasses the influence of surface preprocessing, morphological preprocessing, the FastText vector model, and the weighted fusion of TF-IDF features. To carry out classification tasks, we employ the Linear Support Vector Classification (LSVC) model. In the evaluation phase, our system showcases significant results, achieving an F1micro score of 76.70% and 50.46% for binary and multiple classification scenarios, respectively. These accomplishments closely correspond to the average F1micro scores achieved by other systems submitted for the second subtask, standing at 77.96% and 64.85% for binary and multiple classification scenarios, respectively.",
        "authors": [
          "Mohamed Lichouri",
          "Khaled Lounnas",
          "Aicha Zitouni",
          "Houda Latrache",
          "Rachida Djeradi"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "508",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "512",
        "paper_id": 47,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.47.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.47.jpg",
        "title": "USTHB at ArAIEval\u201923 Shared Task: Disinformation Detection System based on Linguistic Feature Concatenation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.47",
        "x": -4.364562511444092,
        "y": 2.392279624938965,
        "year": "2023"
      },
      {
        "abstract": "The rapid proliferation of disinformation through social media has become one of the most dangerous means to deceive and influence people\u2019s thoughts, viewpoints, or behaviors due to social media\u2019s facilities, such as rapid access, lower cost, and ease of use. Disinformation can spread through social media in different ways, such as fake news stories, doctored images or videos, deceptive data, and even conspiracy theories, thus making detecting disinformation challenging. This paper is a part of participation in the ArAIEval competition that relates to disinformation detection. This work evaluated four models: MARBERT, the proposed ensemble model, and two tests over GPT-4 (zero-shot and Few-shot). GPT-4 achieved micro-F1 79.01% while the ensemble method obtained 76.83%. Despite no improvement in the micro-F1 score on the dev dataset using the ensemble approach, we still used it for the test dataset predictions. We believed that merging different classifiers might enhance the system\u2019s prediction accuracy.",
        "authors": [
          "Ahmed Bahaaulddin",
          "Vian Sabeeh",
          "Hanan Belhaj",
          "Serry Sibaee",
          "Samar Ahmad",
          "Ibrahim Khurfan",
          "Abdullah Alharbi"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "530",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "535",
        "paper_id": 51,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.51.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.51.jpg",
        "title": "AraDetector at ArAIEval Shared Task: An Ensemble of Arabic-specific pre-trained BERT and GPT-4 for Arabic Disinformation Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.51",
        "x": -6.856863021850586,
        "y": -0.14099092781543732,
        "year": "2023"
      },
      {
        "abstract": "The widespread dissemination of propaganda and disinformation on both social media and mainstream media platforms has become an urgent concern, attracting the interest of various stakeholders such as government bodies and social media companies. The challenge intensifies when dealing with understudied languages like Arabic. In this paper, we outline our approach for detecting persuasion techniques in Arabic tweets and news article paragraphs. We submitted our system to ArAIEval 2023 Shared Task 1, covering both subtasks. Our main contributions include utilizing GPT-3 to discern tone and potential persuasion techniques in text, exploring various base language models, and employing a multi-task learning approach for the specified subtasks.",
        "authors": [
          "Utsav Shukla",
          "Manan Vyas",
          "Shailendra Tiwari"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "589",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "593",
        "paper_id": 60,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.60.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.60.jpg",
        "title": "Raphael at ArAIEval Shared Task: Understanding Persuasive Language and Tone, an LLM Approach",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.60",
        "x": -3.7047953605651855,
        "y": 3.327068328857422,
        "year": "2023"
      },
      {
        "abstract": "In this paper, we describe a spoken Arabic dialect identification (ADI) model for Arabic that consistently outperforms previously published results on two benchmark datasets: ADI-5 and ADI-17. We explore two architectural variations: ResNet and ECAPA-TDNN, coupled with two types of acoustic features: MFCCs and features exratected from the pre-trained self-supervised model UniSpeech-SAT Large, as well as a fusion of all four variants. We find that individually, ECAPA-TDNN network outperforms ResNet, and models with UniSpeech-SAT features outperform models with MFCCs by a large margin. Furthermore, a fusion of all four variants consistently outperforms individual models. Our best models outperform previously reported results on both datasets, with accuracies of 84.7% and 96.9% on ADI-5 and ADI-17, respectively.",
        "authors": [
          "Ajinkya Kulkarni",
          "Hanan Aldarmaki"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "435",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "440",
        "paper_id": 37,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.37.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.37.jpg",
        "title": "Yet Another Model for Arabic Dialect Identification",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.37",
        "x": -5.617260932922363,
        "y": 0.8439923524856567,
        "year": "2023"
      },
      {
        "abstract": "Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic (CA), Modern Standard Arabic (MSA), and several country-level dialectal variants. Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist, but on average are better translators of dialects than existing commercial systems. On CA and MSA, instruction-tuned LLMs, however, trail behind commercial systems such as Google Translate. Finally, we undertake a human-centric study to scrutinize the efficacy of the relatively recent model, Bard, in following human instructions during translation tasks. Our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.",
        "authors": [
          "Karima Kadaoui",
          "Samar M. Magdy",
          "Abdul Waheed",
          "Md Tawkat Islam Khondaker",
          "Ahmed Oumar El-Shangiti",
          "El-Moatez-Billah Nagoudi",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "52",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "75",
        "paper_id": 6,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.6.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.6.jpg",
        "title": "TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.6",
        "x": -6.849795818328857,
        "y": -0.03635863959789276,
        "year": "2023"
      },
      {
        "abstract": "Recent advances in the space of Arabic large language models have opened up a wealth of potential practical applications. From optimal training strategies, large scale data acquisition and continuously increasing NLP resources, the Arabic LLM landscape has improved in a very short span of time, despite being plagued by training data scarcity and limited evaluation resources compared to English. In line with contributing towards this ever-growing field, we introduce AlGhafa, a new multiple-choice evaluation benchmark for Arabic LLMs. For showcasing purposes, we train a new suite of models, including a 14 billion parameter model, the largest monolingual Arabic decoder-only model to date. We use a collection of publicly available datasets, as well as a newly introduced HandMade dataset consisting of 8 billion tokens. Finally, we explore the quantitative and qualitative toxicity of several Arabic models, comparing our models to existing public Arabic LLMs.",
        "authors": [
          "Ebtesam Almazrouei",
          "Ruxandra Cojocaru",
          "Michele Baldo",
          "Quentin Malartic",
          "Hamza Alobeidli",
          "Daniele Mazzotta",
          "Guilherme Penedo",
          "Giulia Campesan",
          "Mugariya Farooq",
          "Maitha Alhammadi",
          "Julien Launay",
          "Badreddine Noune"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "244",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "275",
        "paper_id": 21,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.21.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.21.jpg",
        "title": "AlGhafa Evaluation Benchmark for Arabic Language Models",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.21",
        "x": -4.547026634216309,
        "y": 4.40858268737793,
        "year": "2023"
      },
      {
        "abstract": "Motivated by the need for intelligent question answering (QA) systems on the Holy Qur\u2019an and the success of the first Qur\u2019an Question Answering shared task (Qur\u2019an QA 2022 at OSACT 2022), we have organized the second version at ArabicNLP 2023. The Qur\u2019an QA 2023 is composed of two sub-tasks: the passage retrieval (PR) task and the machine reading comprehension (MRC) task. The main aim of the shared task is to encourage state-of-the-art research on Arabic PR and MRC on the Holy Qur\u2019an. Our shared task has attracted 9 teams to submit 22 runs for the PR task, and 6 teams to submit 17 runs for the MRC task. In this paper, we present an overview of the task and provide an outline of the approaches employed by the participating teams in both sub-tasks.",
        "authors": [
          "Rana Malhas",
          "Watheq Mansour",
          "Tamer Elsayed"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "690",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "701",
        "paper_id": 76,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.76.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.76.jpg",
        "title": "Qur\u2019an QA 2023 Shared Task: Overview of Passage Retrieval and Reading Comprehension Tasks over the Holy Qur\u2019an",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.76",
        "x": -5.58607292175293,
        "y": 0.5705852508544922,
        "year": "2023"
      },
      {
        "abstract": "This paper presents the pipeline developed by the AAST-NLP team to address both the persuasion technique detection and disinformation detection shared tasks. The proposed system for all the tasks\u2019 sub-tasks consisted of preprocessing the data and finetuning AraBERT on the given datasets, in addition to several procedures performed for each subtask to adapt to the problems faced in it. The previously described system was used in addition to Dice loss as the loss function for sub-task 1A, which consisted of a binary classification problem. In that sub-task, the system came in eleventh place. We trained the AraBERT for task 1B, which was a multi-label problem with 24 distinct labels, using binary cross-entropy to train a classifier for each label. On that sub-task, the system came in third place. We utilised AraBERT with Dice loss on both subtasks 2A and 2B, ranking second and third among the proposed models for the respective subtasks.",
        "authors": [
          "Ahmed El-Sayed",
          "Omar Nasr",
          "Noureldin Elmadany"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "565",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "569",
        "paper_id": 56,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.56.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.56.jpg",
        "title": "AAST-NLP at ArAIEval Shared Task: Tackling Persuasion technique and Disinformation Detection using Pre-Trained Language Models On Imbalanced Datasets",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.56",
        "x": -7.059018611907959,
        "y": 0.9392598867416382,
        "year": "2023"
      },
      {
        "abstract": "To extract the \u2018meaning\u2019 of a gloss phrase, we build a list of sense-IDs for each word in the phrase which is in our vocabulary. We choose one sense-ID from each list so as to maximise similarity of all the IDs in the chosen subset. We take the meaning of the phrase in semantic space to be the weighted sum of the embedding vectors of the IDs.",
        "authors": [
          "Stephen Taylor"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "461",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "466",
        "paper_id": 40,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.40.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.40.jpg",
        "title": "UWB at Arabic Reverse Dictionary shared task: Computing the meaning of a gloss",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.40",
        "x": -4.395787715911865,
        "y": 3.8760063648223877,
        "year": "2023"
      },
      {
        "abstract": "We present ArTrivia, a new Arabic question-answering dataset consisting of more than 10,000 question-answer pairs along with relevant passages, covering a wide range of 18 diverse topics in Arabic. We created our dataset using a newly proposed pipeline that leverages diverse structured data sources from Arabic Wikipedia. Moreover, we conducted a comprehensive statistical analysis of ArTrivia and assessed the performance of each component in our pipeline. Additionally, we compared the performance of ArTrivia against the existing TyDi QA dataset using various experimental setups. Our analysis highlights the significance of often overlooked aspects in dataset creation, such as answer normalization, in enhancing the quality of QA datasets. Our evaluation also shows that ArTrivia presents more challenging and out-of-distribution questions to TyDi, raising questions about the feasibility of using ArTrivia as a complementary dataset to TyDi.",
        "authors": [
          "Sultan Alrowili",
          "K. Vijay-Shanker"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "191",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "207",
        "paper_id": 17,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.17.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.17.jpg",
        "title": "ArTrivia: Harvesting Arabic Wikipedia to Build A New Arabic Question Answering Dataset",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.17",
        "x": -3.564948797225952,
        "y": 3.133941173553467,
        "year": "2023"
      },
      {
        "abstract": "We present WojoodNER-2023, the first Arabic Named Entity Recognition (NER) Shared Task. The primary focus of WojoodNER 2023 is on Arabic NER, offering a novel NER datasets (i.e., Wojood) and the definition of subtasks designed to facilitate meaningful comparisons between different NER approaches. WojoodNER-2023 encompassed two Subtasks: FlatNER and NestedNER. A total of 45 unique teams registered for this shared task, with 11 of them actively participating in the test phase. Specifically, 11 teams participated in FlatNER, while 8 teams tackled NestedNER. The winning team achieved F1 score of 91.96 and 93.73 in FlatNER and NestedNER respectively.",
        "authors": [
          "Mustafa Jarrar",
          "Muhammad Abdul-Mageed",
          "Mohammed Khalilia",
          "Bashar Talafha",
          "Abdelrahim Elmadany",
          "Nagham Hamad",
          "Alaa\u2019 Omar"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "748",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "758",
        "paper_id": 83,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.83.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.83.jpg",
        "title": "WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.83",
        "x": -4.621765613555908,
        "y": 2.452343463897705,
        "year": "2023"
      },
      {
        "abstract": "This paper introduces a comprehensive system designed to address two natural language processing tasks: Passage Retrieval (Task A) and Reading Comprehension (Task B), applied to datasets related to the Holy Qur\u2019an. Task A was treated as a measurement of a textual similarity problem where the system leverages OpenAI\u2019s \u201ctext-embedding-ada-002\u201d embedding model to transform textual content into numerical representations, with cosine similarity serving as the proximity metric. Task B focuses on the extraction of answers from Qur\u2019anic passages, employing the Generative Pre-trained Transformer-4 (GPT-4) language model. In Task A, the system is evaluated using the Mean Average Precision (MAP) metric, achieving MAP scores of 0.109438 and 0.06426543057 on the development and test datasets with an optimal similarity threshold set at 0.85. Task B evaluation employs partial Average Precision (pAP), where our system surpasses a baseline whole-passage retriever with pAP scores of 0.470 and 0.5393130538 on the development and test datasets, respectively.",
        "authors": [
          "Abdulrezzak Zekiye",
          "Fadi Amroush"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "743",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "747",
        "paper_id": 82,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.82.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.82.jpg",
        "title": "Al-Jawaab at Qur\u2019an QA 2023 Shared Task: Exploring Embeddings and GPT Models for Passage Retrieval and Reading Comprehension",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.82",
        "x": -5.5749616622924805,
        "y": 3.5332908630371094,
        "year": "2023"
      },
      {
        "abstract": "Bilingual Lexical Induction (BLI) is a core challenge in NLP, it relies on the relative isomorphism of individual embedding spaces. Existing attempts aimed at controlling the relative isomorphism of different embedding spaces fail to incorporate the impact of semantically related words in the model training objective. To address this, we propose GARI that combines the distributional training objectives with multiple isomorphism losses guided by the graph attention network. GARI considers the impact of semantical variations of words in order to define the relative isomorphism of the embedding spaces. Experimental evaluation using the Arabic language data set shows that GARI outperforms the existing research by improving the average P@1 by a relative score of up to 40.95% and 76.80% for in-domain and domain mismatch settings respectively.",
        "authors": [
          "Muhammad Ali",
          "Maha Alshmrani",
          "Jianbin Qin",
          "Yan Hu",
          "Di Wang"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "181",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "190",
        "paper_id": 16,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.16.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.16.jpg",
        "title": "GARI: Graph Attention for Relative Isomorphism of Arabic Word Embeddings",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.16",
        "x": -3.402560234069824,
        "y": 0.8667030930519104,
        "year": "2023"
      },
      {
        "abstract": "A reverse dictionary takes a descriptive phrase of a particular concept and returns words with definitions that align with that phrase. While many reverse dictionaries cater to languages such as English and are readily available online or have been developed by researchers, there is a notable lack of similar resources for the Arabic language. This paper describes our participation in the Arabic Reverse Dictionary shared task. Our proposed method consists of two main steps: First, we convert word definitions into multidimensional vectors. Then, we train these encoded vectors using the Semi-Decoder model for our target task. Our system secured 2nd place based on the Rank metric for both embeddings (Electra and Sgns).",
        "authors": [
          "Serry Sibaee",
          "Samar Ahmad",
          "Ibrahim Khurfan",
          "Vian Sabeeh",
          "Ahmed Bahaaulddin",
          "Hanan Belhaj",
          "Abdullah Alharbi"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "467",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "471",
        "paper_id": 41,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.41.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.41.jpg",
        "title": "Qamosy at Arabic Reverse Dictionary shared task: Semi Decoder Architecture for Reverse Dictionary with SBERT Encoder",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.41",
        "x": -7.02142858505249,
        "y": 0.8654007315635681,
        "year": "2023"
      },
      {
        "abstract": "This work explores Arabic disinformation identification, a crucial task in natural language processing, using a state-of-the-art NLP model. We highlight the performance of our system model against baseline models, including multilingual and Arabic-specific ones, and showcase the effectiveness of domain-specific pre-trained models. This work advocates for the adoption of tailored pre-trained models in NLP, emphasizing their significance in understanding diverse languages. By merging advanced NLP techniques with domain-specific pre-training, it advances Arabic disinformation identification.",
        "authors": [
          "Pritam Deka",
          "Ashwathy Revi"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "570",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "575",
        "paper_id": 57,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.57.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.57.jpg",
        "title": "PD-AR at ArAIEval Shared Task: A BERT-Centric Approach to Tackle Arabic Disinformation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.57",
        "x": -6.975342750549316,
        "y": -0.09646826982498169,
        "year": "2023"
      },
      {
        "abstract": "The Holy Qur\u2019an is central to Islam, influencing around two billion Muslims globally, and is known for its linguistic richness and complexity. This article discusses our involvement in the PR task (Task A) of the Qur\u2019an QA 2023 Shared Task. We used two models: one employing the Sentence Transformer and the other using OpenAI\u2019s embeddings for document retrieval. Both models, equipped with a translation feature, help interpret and understand Arabic language queries by translating them, executing the search, and then reverting the results to Arabic. Our results show that incorporating translation functionalities improves the performance in Arabic Question-Answering systems. The model with translation enhancement performed notably better in all metrics compared to the non-translation model.",
        "authors": [
          "Hessa Alawwad",
          "Lujain Alawwad",
          "Jamilah Alharbi",
          "Abdullah Alharbi"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "702",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "707",
        "paper_id": 77,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.77.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.77.jpg",
        "title": "AHJL at Qur\u2019an QA 2023 Shared Task: Enhancing Passage Retrieval using Sentence Transformer and Translation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.77",
        "x": -3.6191937923431396,
        "y": 3.2025551795959473,
        "year": "2023"
      },
      {
        "abstract": "In this work, we approach the problem of Qur\u2019anic information retrieval (IR) in Arabic and English. Using the latest state-of-the-art methods in neural IR, we research what helps to tackle this task more efficiently. Training retrieval models requires a lot of data, which is difficult to obtain for training in-domain. Therefore, we commence with training on a large amount of general domain data and then continue training on in-domain data. To handle the lack of in-domain data, we employed a data augmentation technique, which considerably improved results in MRR@10 and NDCG@5 metrics, setting the state-of-the-art in Qur\u2019anic IR for both English and Arabic. The absence of an Islamic corpus and domain-specific model for IR task in English motivated us to address this lack of resources and take preliminary steps of the Islamic corpus compilation and domain-specific language model (LM) pre-training, which helped to improve the performance of the retrieval models that use the domain-specific LM as the shared backbone. We examined several language models (LMs) in Arabic to select one that efficiently deals with the Qur\u2019anic IR task. Besides transferring successful experiments from English to Arabic, we conducted additional experiments with retrieval task in Arabic to amortize the scarcity of general domain datasets used to train the retrieval models. Handling Qur\u2019anic IR task combining English and Arabic allowed us to enhance the comparison and share valuable insights across models and languages.",
        "authors": [
          "Vera Pavlova"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "76",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "88",
        "paper_id": 7,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.7.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.7.jpg",
        "title": "Leveraging Domain Adaptation and Data Augmentation to Improve Qur\u2019anic IR in English and Arabic",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.7",
        "x": -5.674784183502197,
        "y": 0.895891547203064,
        "year": "2023"
      },
      {
        "abstract": "Understanding Arabic text and generating human-like responses is a challenging task. While many researchers have proposed models and solutions for individual problems, there is an acute shortage of a comprehensive Arabic natural language generation toolkit that is capable of handling a wide range of tasks. In this work, we present a robust Arabic text-to-text Transformer model, namely AraT5v2, methodically trained on extensive and diverse data, utilizing an extended sequence length of 2,048 tokens. We explore various pretraining strategies including unsupervised, supervised, and joint pertaining, under both single and multitask settings. Our models outperform competitive baselines with large margins. We take our work one step further by developing and publicly releasing OCTOPUS, a Python-based package and command-line toolkit tailored for eight Arabic generation tasks all exploiting a single model. We provide a link to the models and the toolkit through our public repository.",
        "authors": [
          "Abdelrahim Elmadany",
          "El-Moatez-Billah Nagoudi",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "232",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "243",
        "paper_id": 20,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.20.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.20.jpg",
        "title": "Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation",
        "type": "main",
        "url": "https://aclanthology.org/2023.arabicnlp-1.20",
        "x": -5.143799781799316,
        "y": 2.0618135929107666,
        "year": "2023"
      },
      {
        "abstract": "In this paper, we share our best performing submission to the Arabic AI Tasks Evaluation Challenge (ArAIEval) at ArabicNLP 2023. Our focus was on Task 1, which involves identifying persuasion techniques in excerpts from tweets and news articles. The persuasion technique in Arabic texts was detected using a training loop with XLM-RoBERTa, a language-agnostic text representation model. This approach proved to be potent, leveraging fine-tuning of a multilingual language model. In our evaluation of the test set, we achieved a micro F1 score of 0.64 for subtask A of the competition.",
        "authors": [
          "Olumide Ojo",
          "Olaronke Adebanji",
          "Hiram Calvo",
          "Damian Dieke",
          "Olumuyiwa Ojo",
          "Seye Akinsanya",
          "Tolulope Abiola",
          "Anna Feldman"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of ArabicNLP 2023",
        "first_page": "594",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "599",
        "paper_id": 61,
        "pdf_url": "https://aclanthology.org/2023.arabicnlp-1.61.pdf",
        "publication_date": "2023/12",
        "thumbnail": "https://aclanthology.org/thumb/2023.arabicnlp-1.61.jpg",
        "title": "Legend at ArAIEval Shared Task: Persuasion Technique Detection using a Language-Agnostic Text Representation Model",
        "type": "shared_task",
        "url": "https://aclanthology.org/2023.arabicnlp-1.61",
        "x": -5.7395853996276855,
        "y": 1.0228067636489868,
        "year": "2023"
      },
      {
        "abstract": "This paper outlines the KSAA-CAD shared task, highlighting the Contemporary Arabic Language Dictionary within the scenario of developing a Reverse Dictionary (RD) system and enhancing Word Sense Disambiguation (WSD) capabilities. The first KSAA-RD (Al-Matham et al., 2023) highlighted significant gaps in the domain of RDs, which are designed to retrieve words by their meanings or definitions. This shared task comprises two tasks: RD and WSD. The RD task focuses on identifying word embeddings that most accurately match a given definition, termed a \u201cgloss,\u201d in Arabic. Conversely, the WSD task involves determining the specific meaning of a word in context, particularly when the word has multiple meanings. The winning team achieved the highest-ranking score of 0.0644 in RD using Electra embeddings. In this paper, we describe the methods employed by the participating teams and provide insights into the future direction of KSAA-CAD.",
        "authors": [
          "Waad Alshammari",
          "Amal Almazrua",
          "Asma Al Wazrah",
          "Rawan Almatham",
          "Muneera Alhoshan",
          "Abdulrahman Alosaimy"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "677",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "685",
        "paper_id": 74,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.74.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.74.jpg",
        "title": "KSAA-CAD Shared Task: Contemporary Arabic Dictionary for Reverse Dictionary and Word Sense Disambiguation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.74",
        "x": -6.366243362426758,
        "y": 1.9279959201812744,
        "year": "2024"
      },
      {
        "abstract": "Dialectal Arabic is the primary spoken language used by native Arabic speakers in daily communication. The rise of social media platforms has notably expanded its use as a written language. However, Arabic dialects do not have standard orthographies. This, combined with the inherent noise in user-generated content on social media, presents a major challenge to NLP applications dealing with Dialectal Arabic. In this paper, we explore and report on the task of CODAfication, which aims to normalize Dialectal Arabic into the Conventional Orthography for Dialectal Arabic (CODA). We work with a unique parallel corpus of multiple Arabic dialects focusing on five major city dialects. We benchmark newly developed pretrained sequence-to-sequence models on the task of CODAfication. We further show that using dialect identification information improves the performance across all dialects. We make our code, data, andpretrained models publicly available.",
        "authors": [
          "Bashar Alhafni",
          "Sarah Al-Towaity",
          "Ziyad Fawzy",
          "Fatema Nassar",
          "Fadhl Eryani",
          "Houda Bouamor",
          "Nizar Habash"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "42",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "54",
        "paper_id": 4,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.4.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.4.jpg",
        "title": "Exploiting Dialect Identification in Automatic Dialectal Text Normalization",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.4",
        "x": -4.494358539581299,
        "y": 2.6020333766937256,
        "year": "2024"
      },
      {
        "abstract": "Development of pre-trained language models has predominantly relied on large amounts of datasets. However, this dependence on abundant data has limited the applicability of these models in low-resource settings. In this work, we investigate the utility of exploiting synthetic datasets acquired from different sources to pre-train language models for Arabic. Namely, we leverage data derived based on four different methods: optical character recognition (OCR), automatic speech recognition (ASR), machine translation (MT), and generative language models. We use these datasets to pre-train models in three different architectures: encoder-only (BERTBase), encoder-decoder (T5), and decoder-only (GPT-2). We test the capabilities of resulting models on Arabic natural language understanding (NLU) tasks using the ORCA benchmark. Our results show that utilizing synthetic data can achieve performance comparable to, or even surpassing, those trained on gold data. For example, our model based on a GPT-2 architecture trained on a combined synthetic dataset surpasses the baseline model ARBERTv2. Overall, our models pre-trained on synthetic data demonstrate robust performance across various tasks. This highlights the potential of synthetic datasets in augmenting language model training in low-resource settings.",
        "authors": [
          "Alcides Alcoba Inciarte",
          "Sang Yun Kwon",
          "El-Moatez-Billah Nagoudi",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "265",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "282",
        "paper_id": 23,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.23.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.23.jpg",
        "title": "On the Utility of Pretraining Language Models on Synthetic Data",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.23",
        "x": -4.3044538497924805,
        "y": 2.4544732570648193,
        "year": "2024"
      },
      {
        "abstract": "The recent growth in Middle Eastern stock markets has intensified the demand for specialized financial Arabic NLP models to serve this sector. This article presents the participation of Team SMASH of The University of Edinburgh in the Multi-dialect Intent Detection task (Subtask 1) of the Arabic Financial NLP (AraFinNLP) Shared Task 2024. The dataset used in the shared task is the ArBanking77 (Jarrar et al., 2023). We tackled this task as a classification problem and utilized several BERT and BART-based models to classify the queries efficiently. Our solution is based on implementing a two-step hierarchical classification model based on MARBERTv2. We fine-tuned the model by using the original queries. Our team, SMASH, was ranked 9th with a macro F1 score of 0.7866, indicating areas for further refinement and potential enhancement of the model\u2019s performance.",
        "authors": [
          "Youssef Al Hariri",
          "Ibrahim Abu Farha"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "403",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "409",
        "paper_id": 35,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.35.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.35.jpg",
        "title": "SMASH at AraFinNLP2024: Benchmarking Arabic BERT Models on the Intent Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.35",
        "x": -5.897550582885742,
        "y": 2.164952516555786,
        "year": "2024"
      },
      {
        "abstract": "This paper details our participation in the FIGNEWS-2024 shared task on bias and propaganda annotation in Gaza conflict news. Our objectives were to develop robust guidelines and annotate a substantial dataset to enhance bias detection. We iteratively refined our guidelines and used examples for clarity. Key findings include the challenges in achieving high inter-annotator agreement and the importance of annotator awareness of their own biases. We also explored the integration of ChatGPT as an annotator to support consistency. This paper contributes to the field by providing detailed annotation guidelines, and offering insights into the subjectivity of bias annotation.",
        "authors": [
          "Jasmin Heierli",
          "Silvia Pareti",
          "Serena Pareti",
          "Tatiana Lando"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "580",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "589",
        "paper_id": 62,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.62.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.62.jpg",
        "title": "Bias Bluff Busters at FIGNEWS 2024 Shared Task: Developing Guidelines to Make Bias Conscious",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.62",
        "x": -5.803843975067139,
        "y": 0.8269805908203125,
        "year": "2024"
      },
      {
        "abstract": "Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR) pose unique challenges due to the cursive and context-sensitive nature of the Arabic script. This study introduces ***Qalam***, a novel foundation model designed for Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder architecture. Our model significantly outperforms existing methods, achieving a Word Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We train ***Qalam*** on a diverse dataset, including over 4.5 million images from Arabic manuscripts and a synthetic dataset comprising 60k image-text pairs. Notably, ***Qalam*** demonstrates exceptional handling of Arabic diacritics, a critical feature in Arabic scripts. Furthermore, it shows a remarkable ability to process high-resolution inputs, addressing a common limitation in current OCR systems. These advancements underscore ***Qalam***\u2019s potential as a leading solution for Arabic script recognition, offering a significant leap in accuracy and efficiency.",
        "authors": [
          "Gagan Bhatia",
          "El-Moatez-Billah Nagoudi",
          "Fakhraddin Alwajih",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "210",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "224",
        "paper_id": 19,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.19.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.19.jpg",
        "title": "Qalam: A Multimodal LLM for Arabic Optical Character and Handwriting Recognition",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.19",
        "x": -5.316353797912598,
        "y": 1.7540388107299805,
        "year": "2024"
      },
      {
        "abstract": "In this study, we present a novel approach to annotating bias and propaganda in social media data by leveraging topic modeling techniques. Utilizing the BERTopic tool, we performed topic modeling on the FIGNEWS Shared-task dataset, which initially comprised 13,500 samples. From this dataset, we identified 35 distinct topics and selected approximately 50 representative samples from each topic, resulting in a subset of 1,812 samples. These selected samples were meticulously annotated for bias and propaganda labels. Subsequently, we employed multiple methods like KNN, SVC, XGBoost, and RAG to develop a classifier capable of detecting bias and propaganda within social media content. Our approach demonstrates the efficacy of using topic modeling for efficient data subset selection and provides a robust foundation for improving the accuracy of bias and propaganda detection in large-scale social media datasets.",
        "authors": [
          "Sadegh Jafari",
          "Mohsen Mahmoodzadeh",
          "Vanooshe Nazari",
          "Razieh Bahmanyar",
          "Kathryn Burrows"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "555",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "560",
        "paper_id": 58,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.58.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.58.jpg",
        "title": "DRAGON at FIGNEWS 2024 Shared Task: a Dedicated RAG for October 7th conflict News",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.58",
        "x": -5.814620494842529,
        "y": 0.6632925868034363,
        "year": "2024"
      },
      {
        "abstract": "The domain of reverse dictionaries (RDs), while advancing in languages like English and Chinese, remains underdeveloped for Arabic. This study attempts to explore a data-driven approach to enhance word retrieval processes in Arabic RDs. The research focuses on the ArabicNLP 2024 Shared Task, named KSAA-CAD, which provides a dictionary dataset of 39,214 word-gloss pairs, each with a corresponding target word embedding. The proposed solution aims to surpass the baseline performance by employing SOTA deep learning models and innovative data expansion techniques. The methodology involves enriching the dataset with contextually relevant examples, training a T5 model to align the words to their glosses in the space, and evaluating the results on the shared task metrics. We find that our model is closely aligned with the baseline performance on bertseg and bertmsa targets, however does not perform well on electra target, suggesting the need for further exploration.",
        "authors": [
          "Mais Alheraki",
          "Souham Meshoul"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "704",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "708",
        "paper_id": 78,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.78.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.78.jpg",
        "title": "Baleegh at KSAA-CAD 2024: Towards Enhancing Arabic Reverse Dictionaries",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.78",
        "x": -6.939369201660156,
        "y": 0.7855110168457031,
        "year": "2024"
      },
      {
        "abstract": "As social media usage continues to rise, the demand for systems to analyze opinions and sentiments expressed in textual data has become more critical. This paper presents our submission to the Stance Detection in Arabic Language Shared Task, in which we evaluated three models: the fine-tuned MARBERT Transformer, the fine-tuned AraBERT Transformer, and an Ensemble of Machine learning Classifiers. Our findings indicate that the MARBERT Transformer outperformed the other models in performance across all targets. In contrast, the Ensemble Classifier, which combines traditional machine learning techniques, demonstrated relatively lower effectiveness.",
        "authors": [
          "Nouf AlShenaifi",
          "Nourah Alangari",
          "Hadeel Al-Negheimish"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "828",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "831",
        "paper_id": 97,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.97.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.97.jpg",
        "title": "Rasid at StanceEval: Fine-tuning MARBERT for Arabic Stance Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.97",
        "x": -2.220094680786133,
        "y": 0.7236356139183044,
        "year": "2024"
      },
      {
        "abstract": "Diacritization plays a pivotal role for meaning disambiguation and improving readability in Arabic texts. Efforts have long focused on marking every eligible character (Full Diacritization). Overlooked in comparison, Partial Diacritzation (\u2018PD\u2018) is the selection of a subset of characters to be annotated to aid comprehension only where needed. Research has indicated that excessive diacritic marks can hinder skilled readers\u2014reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (\u2018CCPD\u2018)\u2014a novel approach to \u2018PD\u2018 which integrates seamlessly with existing Arabic diacritization systems. \u2018CCPD\u2018 processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial diacritization quality to help establish this as a machine learning task. Lastly, we introduce \u2018TD2\u2018, a Transformer-variant of an established model which offers a markedly different performance profile on our proposed indicators compared to all other known systems.",
        "authors": [
          "Muhammad N. ElNokrashy",
          "Badr Alkhamissi"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "89",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "101",
        "paper_id": 8,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.8.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.8.jpg",
        "title": "A Context-Contrastive Inference Approach To Partial Diacritization",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.8",
        "x": -5.7899627685546875,
        "y": 3.5883846282958984,
        "year": "2024"
      },
      {
        "abstract": "Recognizing the nuanced spectrum of dialectness in Arabic text poses a significant challenge for natural language processing (NLP) tasks. Traditional dialect identification (DI) methods treat the task as binary, overlooking the continuum of dialect variation present in Arabic speech and text. In this paper, we describe our submission to the NADI shared Task of ArabicNLP 2024. We participated in Subtask 2 - ALDi Estimation, which focuses on estimating the Arabic Level of Dialectness (ALDi) for Arabic text, indicating how much it deviates from Modern Standard Arabic (MSA) on a scale from 0 to 1, where 0 means MSA and 1 means high divergence from MSA. We explore diverse training approaches, including contrastive learning, applying a random weighted sampler along with fine-tuning a regression task based on the AraBERT model, after adding a linear and non-linear layer on top of its pooled output. Finally, performing a brute force ensemble strategy increases the performance of our system. Our proposed solution achieved a Root Mean Squared Error (RMSE) of 0.1406, ranking second on the leaderboard.",
        "authors": [
          "Abdelrahman Sakr",
          "Marwan Torki",
          "Nagwa M. El-Makky"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "735",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "741",
        "paper_id": 81,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.81.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.81.jpg",
        "title": "AlexUNLP-STM at NADI 2024 shared task: Quantifying the Arabic Dialect Spectrum with Contrastive Learning, Weighted Sampling, and BERT-based Regression Ensemble",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.81",
        "x": -4.136257648468018,
        "y": 2.3216140270233154,
        "year": "2024"
      },
      {
        "abstract": "This paper describes a data augmentation technique for boosting the performance of speech-based diacritic restoration. Our experiments demonstrate the utility of this appraoch, resulting in improved generalization of all models across different test sets. In addition, we describe the first multi-modal diacritic restoration model, utilizing both speech and text as input modalities. This type of model can be used to diacritize speech transcripts. Unlike previous work that relies on an external ASR model, the proposed model is far more compact and efficient. While the multi-modal framework does not surpass the ASR-based model for this task, it offers a promising approach for improving the efficiency of speech-based diacritization, with a potential for improvement using data augmentation and other methods.",
        "authors": [
          "Sara Shatnawi",
          "Sawsan Alqahtani",
          "Shady Shehata",
          "Hanan Aldarmaki"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "160",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "169",
        "paper_id": 15,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.15.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.15.jpg",
        "title": "Data Augmentation for Speech-Based Diacritic Restoration",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.15",
        "x": -7.036054611206055,
        "y": 0.17782315611839294,
        "year": "2024"
      },
      {
        "abstract": "In this paper, a description of the system submitted by BFCAI team to the AraFinNLP2024 shared task has been introduced. Our team participated in the first subtask, which aims at detecting the customer intents of cross-dialectal Arabic queries in the banking domain. Our system follows the common pipeline of text classification models using primary classification algorithms integrated with basic vectorization approach for feature extraction. Multi-layer Perceptron, Stochastic Gradient Descent and Support Vector Machines algorithms have been implemented and support vector machines outperformed all other algorithms with an f-score of 49%. Our submission\u2019s result is appropriate compared to the simplicity of the proposed model\u2019s structure.",
        "authors": [
          "Nsrin Ashraf",
          "Hamada Nayel",
          "Mohammed Aldawsari",
          "Hosahalli Shashirekha",
          "Tarek Elshishtawy"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "446",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "449",
        "paper_id": 42,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.42.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.42.jpg",
        "title": "BFCI at AraFinNLP2024: Support Vector Machines for Arabic Financial Text Classification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.42",
        "x": -3.4031996726989746,
        "y": 3.3030076026916504,
        "year": "2024"
      },
      {
        "abstract": "Detecting propagandistic spans and identifying persuasion techniques are crucial for promoting informed decision-making, safeguarding democratic processes, and fostering a media environment characterized by integrity and transparency. Various machine learning (Logistic Regression, Random Forest, and Multinomial Naive Bayes), deep learning (CNN, CNN+LSTM, CNN+BiLSTM), and transformer-based (AraBERTv2, AraBERT-NER, CamelBERT, BERT-Base-Arabic) models were exploited to perform the task. The evaluation results indicate that CamelBERT achieved the highest micro-F1 score (24.09%), outperforming CNN+LSTM and AraBERTv2. The study found that most models struggle to detect propagandistic spans when multiple spans are present within the same article. Overall, the model\u2019s performance secured a6thplace ranking in the ArAIEval Shared Task-1.",
        "authors": [
          "Symom Shohan",
          "Md. Hossain",
          "Ashraful Paran",
          "Shawly Ahsan",
          "Jawad Hossain",
          "Mohammed Moshiul Hoque"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "518",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "523",
        "paper_id": 54,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.54.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.54.jpg",
        "title": "SemanticCuetSync at ArAIEval Shared Task: Detecting Propagandistic Spans with Persuasion Techniques Identification using Pre-trained Transformers",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.54",
        "x": -6.760157585144043,
        "y": 0.7220644950866699,
        "year": "2024"
      },
      {
        "abstract": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) that aims to identify and classify entities in text into predefined categories.However, when applied to Arabic data, NER encounters unique challenges stemming from the language\u2019s rich morphological inflections, absence of capitalization cues, and spelling variants, where a single word can comprise multiple morphemes.In this paper, we introduce Arabic KNN-NER, our submission to the Wojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the shared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained flat-entity recognition for Arabic text, where we identify a single main entity and possibly zero or multiple sub-entities for each word.Arabic KNN-NER augments the probability distribution of a fine-tuned model with another label probability distribution derived from performing a KNN search over the cached training data. Our submission achieved 91% on the test set on the WojoodFine dataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
        "authors": [
          "Ahmed Abdou",
          "Tasneem Mahmoud"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "894",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "898",
        "paper_id": 107,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.107.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.107.jpg",
        "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest Neighbor Search",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.107",
        "x": -5.795671463012695,
        "y": 0.9938827753067017,
        "year": "2024"
      },
      {
        "abstract": "In this paper, we are exploring mitigating class imbalancein Arabic propaganda detection. Given amultigenre text which could be a news paragraphor a tweet, the objective is to identify the propagandatechnique employed in the text along withthe exact span(s) where each technique occurs. Weapproach this task as a sequence tagging task. Weutilise AraBERT for sequence classification andimplement data augmentation and random truncationmethods to mitigate the class imbalance withinthe dataset. We demonstrate the importance ofconsidering macro-F1 as well as micro-F1 whenevaluating classifier performance in this scenario.",
        "authors": [
          "Mary Fouad",
          "Julie Weeds"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "524",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "529",
        "paper_id": 55,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.55.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.55.jpg",
        "title": "SussexAI at ArAIEval Shared Task: Mitigating Class Imbalance in Arabic Propaganda Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.55",
        "x": -4.6488471031188965,
        "y": 4.1605224609375,
        "year": "2024"
      },
      {
        "abstract": "In this paper, we present our dzFinNlp team\u2019s contribution for intent detection in financial conversational agents, as part of the AraFinNLP shared task. We experimented with various models and feature configurations, including traditional machine learning methods like LinearSVC with TF-IDF, as well as deep learning models like Long Short-Term Memory (LSTM). Additionally, we explored the use of transformer-based models for this task. Our experiments show promising results, with our best model achieving a micro F1-score of 93.02% and 67.21% on the ArBanking77 dataset, in the development and test sets, respectively.",
        "authors": [
          "Mohamed Lichouri",
          "Khaled Lounnas",
          "Amziane Zakaria"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "450",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "455",
        "paper_id": 43,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.43.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.43.jpg",
        "title": "dzFinNlp at AraFinNLP: Improving Intent Detection in Financial Conversational Agents",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.43",
        "x": -2.232116222381592,
        "y": 0.9112981557846069,
        "year": "2024"
      },
      {
        "abstract": "Pre-trained Language Models (PLMs) are integral to many modern natural language processing (NLP) systems. Although multilingual models cover a wide range of languages, they often grapple with challenges like high inference costs and a lack of diverse non-English training data. Arabic-specific PLMs are trained predominantly on modern standard Arabic, which compromises their performance on regional dialects. To tackle this, we construct an Arabic dialectal corpus comprising 3.4M sentences gathered from social media platforms. We utilize this corpus to expand the vocabulary and retrain a BERT-based model from scratch. Named AlcLaM, our model was trained using only 13GB of text, which represents a fraction of the data used by existing models such as CAMeL, MARBERT, and ArBERT, compared to 7.8%%, and 21.3%, respectively. Remarkably, AlcLaM demonstrates superior performance on a variety of Arabic NLP tasks despite the limited training data. AlcLaM is available at: https://github.com/amurtadha/Alclam.",
        "authors": [
          "Murtadha Ahmed",
          "Saghir Alfasly",
          "Bo Wen",
          "Jamal Addeen",
          "Mohammed Ahmed",
          "Yunfeng Liu"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "153",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "159",
        "paper_id": 14,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.14.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.14.jpg",
        "title": "AlclaM: Arabic Dialect Language Model",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.14",
        "x": -4.5848236083984375,
        "y": 2.3897037506103516,
        "year": "2024"
      },
      {
        "abstract": "Navigating the intricacies of machine translation (MT) involves tackling the nuanced disparities between Arabic dialects and Modern Standard Arabic (MSA), presenting a formidable obstacle. In this study, we delve into Subtask 3 of the NADI shared task (CITATION), focusing on the translation of sentences from four distinct Arabic dialects into MSA. Our investigation explores the efficacy of various models, including Jais, NLLB, GPT-3.5, and GPT-4, in this dialect-to-MSA translation endeavor. Our findings reveal that Jais surpasses all other models, boasting an average BLEU score of 19.48 in the combination of zero- and few-shot setting, whereas NLLB exhibits the least favorable performance, garnering a BLEU score of 8.77.",
        "authors": [
          "Anastasiia Demidova",
          "Hanin Atwany",
          "Nour Rabih",
          "Sanad Sha\u2019ban"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "729",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "734",
        "paper_id": 80,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.80.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.80.jpg",
        "title": "Arabic Train at NADI 2024 shared task: LLMs\u2019 Ability to Translate Arabic Dialects into Modern Standard Arabic",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.80",
        "x": -4.260379314422607,
        "y": 2.5753231048583984,
        "year": "2024"
      },
      {
        "abstract": "Intention detection is a crucial aspect of natural language understanding (NLU), focusing on identifying the primary objective underlying user input. In this work, we present a transformer-based method that excels in determining the intent of Arabic text within the banking domain. We explored several machine learning (ML), deep learning (DL), and transformer-based models on an Arabic banking dataset for intent detection. Our findings underscore the challenges that traditional ML and DL models face in understanding the nuances of various Arabic dialects, leading to subpar performance in intent detection. However, the transformer-based methods, designed to tackle such complexities, significantly outperformed the other models in classifying intent across different Arabic dialects. Notably, the AraBERTv2 model achieved the highest micro F1 score of 82.08% in ArBanking77 dataset, a testament to its effectiveness in this context. This achievement, which contributed to our work being ranked 5thin the shared task, AraFinNLP2024, highlights the importance of developing models that can effectively handle the intricacies of Arabic language processing and intent detection.",
        "authors": [
          "Ashraful Paran",
          "Symom Shohan",
          "Md. Hossain",
          "Jawad Hossain",
          "Shawly Ahsan",
          "Mohammed Moshiul Hoque"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "422",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "427",
        "paper_id": 38,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.38.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.38.jpg",
        "title": "SemanticCuetSync at AraFinNLP2024: Classification of Cross-Dialect Intent in the Banking Domain using Transformers",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.38",
        "x": -5.596141815185547,
        "y": -0.2523745596408844,
        "year": "2024"
      },
      {
        "abstract": "This paper introduces Arabic Contrastive Language-Image Pre-training (AraCLIP), a model designed for Arabic image retrieval tasks, building upon the Contrastive Language-Image Pre-training (CLIP) architecture. AraCLIP leverages Knowledge Distillation to transfer cross-modal knowledge from English to Arabic, enhancing its ability to understand Arabic text and retrieve relevant images. Unlike existing multilingual models, AraCLIP is uniquely positioned to understand the intricacies of the Arabic language, including specific terms, cultural nuances, and contextual constructs. By leveraging the CLIP architecture as our foundation, we introduce a novel approach that seamlessly integrates textual and visual modalities, enabling AraCLIP to effectively retrieve images based on Arabic textual queries. We offer an online demonstration allowing users to input Arabic prompts and compare AraCLIP\u2019s performance with state-of-the-art multilingual models. We conduct comprehensive experiments to evaluate AraCLIP\u2019s performance across diverse datasets, including Arabic XTD-11, and Arabic Flicker 8k. Our results showcase AraCLIP\u2019s superiority in image retrieval accuracy, demonstrating its effectiveness in handling Arabic queries. AraCLIP represents a significant advancement in cross-lingual image retrieval, offering promising applications in Arabic language processing and beyond.",
        "authors": [
          "Muhammad Al-Barham",
          "Imad Afyouni",
          "Khalid Almubarak",
          "Ashraf Elnagar",
          "Ayad Turky",
          "Ibrahim Hashem"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "102",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "110",
        "paper_id": 9,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.9.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.9.jpg",
        "title": "AraCLIP: Cross-Lingual Learning for Effective Arabic Image Retrieval",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.9",
        "x": -3.7759764194488525,
        "y": 2.8555524349212646,
        "year": "2024"
      },
      {
        "abstract": "We describe the findings of the fifth Nuanced Arabic Dialect Identification Shared Task (NADI 2024). NADI\u2019s objective is to help advance SoTA Arabic NLP by providing guidance, datasets, modeling opportunities, and standardized evaluation conditions that allow researchers to collaboratively compete on prespecified tasks. NADI 2024 targeted both dialect identification cast as a multi-label task (Subtask 1), identification of the Arabic level of dialectness (Subtask 2), and dialect-to-MSA machine translation (Subtask 3). A total of 51 unique teams registered for the shared task, of whom 12 teams have participated (with 76 valid submissions during the test phase). Among these, three teams participated in Subtask 1, three in Subtask 2, and eight in Subtask 3. The winning teams achieved 50.57 F1 on Subtask 1, 0.1403 RMSE for Subtask 2, and 20.44 BLEU in Subtask 3, respectively. Results show that Arabic dialect processing tasks such as dialect identification and machine translation remain challenging. We describe the methods employed by the participating teams and briefly offer an outlook for NADI.",
        "authors": [
          "Muhammad Abdul-Mageed",
          "Amr Keleg",
          "Abdelrahim Elmadany",
          "Chiyu Zhang",
          "Injy Hamed",
          "Walid Magdy",
          "Houda Bouamor",
          "Nizar Habash"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "709",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "728",
        "paper_id": 79,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.79.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.79.jpg",
        "title": "NADI 2024: The Fifth Nuanced Arabic Dialect Identification Shared Task",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.79",
        "x": -4.418017387390137,
        "y": 4.676922798156738,
        "year": "2024"
      },
      {
        "abstract": "Stance detection, an evolving task in natural language processing, involves understanding a writer\u2019s perspective on certain topics by analyzing his written text and interactions online, especially on social media platforms. In this paper, we outline our submission to the StanceEval task, leveraging the Mawqif dataset featured in The Second Arabic Natural Language Processing Conference. Our task is to detect writers\u2019 stances (Favor, Against, or None) towards three selected topics (COVID-19 vaccine, digital transformation, and women empowerment). We present our approach primarily relying on a contrastive loss ensemble strategy. Our proposed approach achieved an F1-score of 0.8438 and ranked first in the stanceEval 2024 task. The code and checkpoints are availableat https://github.com/MBadran2000/Mawqif.git",
        "authors": [
          "Mohamed Badran",
          "Mo\u2019men Hamdy",
          "Marwan Torki",
          "Nagwa M. El-Makky"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "823",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "827",
        "paper_id": 96,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.96.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.96.jpg",
        "title": "AlexUNLP-BH at StanceEval2024: Multiple Contrastive Losses Ensemble Strategy with Multi-Task Learning For Stance Detection in Arabic",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.96",
        "x": -5.105384826660156,
        "y": 1.8746216297149658,
        "year": "2024"
      },
      {
        "abstract": "News bias is difficult for humans to identify, but even more so for machines. This is largely due to the lack of linguistically appropriate annotated datasets suitable for use by classifier algorithms. The FIGNEWS Subtask 1: Bias Annotation involved classifying bias through manually annotated 1800 headlines from social media. Our proposed guidelines investigated which combinations of keywords available for classification, across sentence and token levels, may be used to detect possible bias in a conflict where neutrality is highly undesirable. Much of the headlines\u2019 percentage required contextual knowledge of events to identify criteria that matched biased or targeted language. The final annotation guidelines paved the way for a theoretical system which uses keyword and hashtag significance to classify major instances of bias. Minor instances with bias undertones or clickbait may require advanced machine learning methods which learn context through scraping user engagements on social media.",
        "authors": [
          "Yousra El-Ghawi",
          "Abeer Marzouk",
          "Aya Khamis"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "561",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "566",
        "paper_id": 59,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.59.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.59.jpg",
        "title": "LexiconLadies at FIGNEWS 2024 Shared Task: Identifying Keywords for Bias Annotation Guidelines of Facebook News Headlines on the Israel-Palestine 2023 War",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.59",
        "x": -4.300865173339844,
        "y": 3.6510612964630127,
        "year": "2024"
      },
      {
        "abstract": "Large language models (LLMs) play a crucial role in a wide range of real world applications. However, concerns about their safety and ethical implications are growing. While research on LLM safety is expanding, there is a noticeable gap in evaluating safety across multiple languages, especially in Arabic and Russian. We address this gap by exploring biases in LLMs across different languages and contexts, focusing on GPT-3.5 and Gemini. Through carefully designed argument-based prompts and scenarios in Arabic, English, and Russian, we examine biases in cultural, political, racial, religious, and gender domains. Our findings reveal biases in these domains. In particular, our investigation uncovers subtle biases where each model tends to present winners as those speaking the primary language the model is prompted with. Our study contributes to ongoing efforts to ensure justice and equality in LLM development and emphasizes the importance of further research towards responsible progress in this field.",
        "authors": [
          "Anastasiia Demidova",
          "Hanin Atwany",
          "Nour Rabih",
          "Sanad Sha\u2019ban",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "193",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "209",
        "paper_id": 18,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.18.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.18.jpg",
        "title": "John vs. Ahmed: Debate-Induced Bias in Multilingual LLMs",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.18",
        "x": -4.705627918243408,
        "y": 4.23016881942749,
        "year": "2024"
      },
      {
        "abstract": "In this paper, we present our approach for FIGNEWS Subtask 1, which focuses on detecting bias in news media narratives about the Israel war on Gaza. We used a Large Language Model (LLM) and prompt engineering, using GPT-3.5 Turbo API, to create a model that automatically flags biased news media content with 99% accuracy. This approach provides Natural Language Processing (NLP) researchers with a robust and effective solution for automating bias detection in news media narratives using supervised learning algorithms. Additionally, this paper provides a detailed analysis of the labeled content, offering valuable insights into media bias in conflict reporting. Our work advances automated content analysis and enhances understanding of media bias.",
        "authors": [
          "Noor Sadiah",
          "Sara Al-Emadi",
          "Sumaya Rahman"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "590",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "600",
        "paper_id": 63,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.63.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.63.jpg",
        "title": "Ceasefire at FIGNEWS 2024 Shared Task: Automated Detection and Annotation of Media Bias Using Large Language Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.63",
        "x": -6.436069011688232,
        "y": 1.8380708694458008,
        "year": "2024"
      },
      {
        "abstract": "The expanding financial markets of the Arab world require sophisticated Arabic NLP tools. To address this need within the banking domain, the Arabic Financial NLP (AraFinNLP) shared task proposes two subtasks: (i) Multi-dialect Intent Detection and (ii) Cross-dialect Translation and Intent Preservation. This shared task uses the updated ArBanking77 dataset, which includes about 39k parallel queries in MSA and four dialects. Each query is labeled with one or more of a common 77 intents in the banking domain. These resources aim to foster the development of robust financial Arabic NLP, particularly in the areas of machine translation and banking chat-bots.A total of 45 unique teams registered for this shared task, with 11 of them actively participated in the test phase. Specifically, 11 teams participated in Subtask 1, while only 1 team participated in Subtask 2. The winning team of Subtask 1 achieved F1 score of 0.8773, and the only team submitted in Subtask 2 achieved a 1.667 BLEU score.",
        "authors": [
          "Sanad Malaysha",
          "Mo El-Haj",
          "Saad Ezzini",
          "Mohammed Khalilia",
          "Mustafa Jarrar",
          "Sultan Almujaiwel",
          "Ismail Berrada",
          "Houda Bouamor"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "393",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "402",
        "paper_id": 34,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.34.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.34.jpg",
        "title": "AraFinNLP 2024: The First Arabic Financial NLP Shared Task",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.34",
        "x": -5.642541885375977,
        "y": 0.8330998420715332,
        "year": "2024"
      },
      {
        "abstract": "Learning morphophonological mappings between the spoken form of a language and its underlying morphological structures is crucial for enriching resources for morphologically rich languages like Arabic. In this work, we focus on Egyptian Arabic as our case study and explore the integration of linguistic knowledge with a neural transformer model. Our approach involves learning to correct the residual errors from hand-crafted rules to predict the spoken form from a given underlying morphological representation. We demonstrate that using a minimal set of rules, we can effectively recover errors even in very low-resource settings.",
        "authors": [
          "Salam Khalifa",
          "Abdelrahim Qaddoumi",
          "Ellen Broselow",
          "Owen Rambow"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "258",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "264",
        "paper_id": 22,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.22.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.22.jpg",
        "title": "Picking Up Where the Linguist Left Off: Mapping Morphology to Phonology through Learning the Residuals",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.22",
        "x": -5.082764625549316,
        "y": 1.8836877346038818,
        "year": "2024"
      },
      {
        "abstract": "We present Team Cher\u2019s submission to the ArabicNLP 2024 KSAA-CAD shared task on the reverse dictionary for Arabic\u2014the retrieval of words using definitions as a query. Our approach is based on a multi-task learning framework that jointly learns reverse dictionary, definition generation, and reconstruction tasks. This work explores different tokenization strategies and compares retrieval performance for each embedding architecture. Evaluation using the KSAA-CAD benchmark demonstrates the effectiveness of our multi-task approach and provides insights into the reverse dictionary task for Arabic. It is worth highlighting that we achieve strong performance without using any external resources in addition to the provided training data.",
        "authors": [
          "Pinzhen Chen",
          "Zheng Zhao",
          "Shun Shao"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "686",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "691",
        "paper_id": 75,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.75.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.75.jpg",
        "title": "Cher at KSAA-CAD 2024: Compressing Words and Definitions into the Same Space for Arabic Reverse Dictionary",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.75",
        "x": -5.7143988609313965,
        "y": 0.9387884736061096,
        "year": "2024"
      },
      {
        "abstract": "This study compares Term Frequency-Inverse Document Frequency (TF-IDF) features with Sentence Transformers for detecting writers\u2019 stances\u2014favorable, opposing, or neutral\u2014towards three significant topics: COVID-19 vaccine, digital transformation, and women empowerment. Through empirical evaluation, we demonstrate that Sentence Transformers outperform TF-IDF features across various experimental setups. Our team, dzStance, participated in a stance detection competition, achieving the 13th position (74.91%) among 15 teams in Women Empowerment, 10th (73.43%) in COVID Vaccine, and 12th (66.97%) in Digital Transformation. Overall, our team\u2019s performance ranked 13th (71.77%) among all participants. Notably, our approach achieved promising F1-scores, highlighting its effectiveness in identifying writers\u2019 stances on diverse topics. These results underscore the potential of Sentence Transformers to enhance stance detection models for addressing critical societal issues.",
        "authors": [
          "Mohamed Lichouri",
          "Khaled Lounnas",
          "Ouaras Rafik",
          "Mohamed ABi",
          "Anis Guechtouli"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "794",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "799",
        "paper_id": 91,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.91.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.91.jpg",
        "title": "dzStance at StanceEval2024: Arabic Stance Detection based on Sentence Transformers",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.91",
        "x": -7.109349727630615,
        "y": 0.3171501159667969,
        "year": "2024"
      },
      {
        "abstract": "Text classification is of paramount importance in a wide range of applications, including information retrieval, extraction and sentiment analysis. The challenge of classifying and labelling text genres, especially in web-based corpora, has received considerable attention. The frequent absence of unambiguous genre information complicates the identification of text types. To address these issues, the Functional Text Dimensions (FTD) method has been introduced to provide a universal set of categories for text classification. This study presents the Arabic Functional Text Dimensions Corpus (AFTD Corpus), a carefully curated collection of documents for evaluating text classification in Arabic. The AFTD Corpus which we are making available to the community, consists of 3400 documents spanning 17 different class categories. Through a comprehensive evaluation using traditional machine learning and neural models, we assess the effectiveness of the FTD approach in the Arabic context. CAMeLBERT, a state-of-the-art model, achieved an impressive F1 score of 0.81 on our corpus. This research highlights the potential of the FTD method for improving text classification, especially for Arabic content, and underlines the importance of robust classification models in web applications.",
        "authors": [
          "Zeyd Ferhat",
          "Abir Betka",
          "Riyadh Barka",
          "Zineddine Kahhoul",
          "Selma Boutiba",
          "Mohamed Tiar",
          "Habiba Dahmani",
          "Ahmed Abdelali"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "352",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "360",
        "paper_id": 29,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.29.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.29.jpg",
        "title": "Functional Text Dimensions for Arabic Text Classification",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.29",
        "x": -6.7501912117004395,
        "y": 0.6189609169960022,
        "year": "2024"
      },
      {
        "abstract": "Large language models (LLMs) have recently emerged as a powerful tool for a wide range of language generation tasks. Nevertheless, this progress has been slower in Arabic. In this work, we focus on the task of generating stories from LLMs. For our training, we use stories acquired through machine translation (MT) as well as GPT-4. For the MT data, we develop a careful pipeline that ensures we acquire high-quality stories. For our GPT-4 data, we introduce crafted prompts that allow us to generate data well-suited to the Arabic context in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian and Moroccan). For example, we generate stories tailored to various Arab countries on a wide host of topics. Our manual evaluation shows that our model fine-tuned on these training datasets can generate coherent stories that adhere to our instructions. We also conduct an extensive automatic and human evaluation comparing our models against state-of-the-art proprietary and open-source models. Our datasets and models will be made publicly available athttps://github.com/UBC-NLP/arastories.",
        "authors": [
          "Ahmed Oumar El-Shangiti",
          "Fakhraddin Alwajih",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "140",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "152",
        "paper_id": 13,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.13.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.13.jpg",
        "title": "Arabic Automatic Story Generation with Large Language Models",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.13",
        "x": -6.130523681640625,
        "y": 0.11276978999376297,
        "year": "2024"
      },
      {
        "abstract": "We present an overview of the second edition of the ArAIEval shared task, organized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In this edition, ArAIEval offers two tasks: (i) detection of propagandistic textual spans with persuasion techniques identification in tweets and news articles, and (ii) distinguishing between propagandistic and non-propagandistic memes. A total of 14 teams participated in the final evaluation phase, with 6 and 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams submitted system description papers. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further provide a brief overview of the participating systems. All datasets and evaluation scripts are released to the research community. We hope this will enable further research on these important tasks in Arabic.",
        "authors": [
          "Maram Hasanain",
          "Md. Arid Hasan",
          "Fatema Ahmad",
          "Reem Suwaileh",
          "Md. Rafiul Biswas",
          "Wajdi Zaghouani",
          "Firoj Alam"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "456",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "466",
        "paper_id": 44,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.44.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.44.jpg",
        "title": "ArAIEval Shared Task: Propagandistic Techniques Detection in Unimodal and Multimodal Arabic Content",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.44",
        "x": -3.6023948192596436,
        "y": 3.39285945892334,
        "year": "2024"
      },
      {
        "abstract": "In recent days, propaganda has started to influence public opinion increasingly as social media usage continues to grow. Our research has been part of the first challenge, Unimodal (Text) Propagandistic Technique Detection of ArAIEval shared task at the ArabicNLP 2024 conference, co-located with ACL 2024, identifying specific Arabic text spans using twenty-three propaganda techniques. We have augmented underrepresented techniques in the provided dataset using synonym replacement and have evaluated various machine learning (RF, SVM, MNB), deep learning (BiLSTM), and transformer-based models (bert-base-arabic, Marefa-NER, AraBERT) with transfer learning. Our comparative study has shown that the transformer model \u201cbert-base-arabic\u201d has outperformed other models. Evaluating the test set, it has achieved the micro-F1 score of 0.2995 which is the highest. This result has secured our team \u201cCUET_sstm\u201d first place among all participants in task 1 of the ArAIEval.",
        "authors": [
          "Momtazul Labib",
          "Samia Rahman",
          "Hasan Murad",
          "Udoy Das"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "507",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "511",
        "paper_id": 52,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.52.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.52.jpg",
        "title": "CUET_sstm at ArAIEval Shared Task: Unimodal (Text) Propagandistic Technique Detection Using Transformer-Based Model",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.52",
        "x": -5.607224464416504,
        "y": 1.152831792831421,
        "year": "2024"
      },
      {
        "abstract": "In this paper, we present a high-performing model for Arabic stance detection on the STANCEEVAL2024 shared task part ofARABICNLP2024. Our model leverages ARABERTV1; a pre-trained Arabic language model, within a single-task learning framework. We fine-tuned the model on stance detection data for three specific topics: COVID19 vaccine, digital transformation, and women empowerment, extracted from the MAWQIF corpus. In terms of performance, our model achieves 73.30 macro-F1 score for women empowerment, 70.51 for digital transformation, and 64.55 for COVID-19 vaccine detection.",
        "authors": [
          "Anas Melhem",
          "Osama Hamed",
          "Thaer Sammar"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "842",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "846",
        "paper_id": 100,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.100.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.100.jpg",
        "title": "TAO at StanceEval2024 Shared Task: Arabic Stance Detection using AraBERT",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.100",
        "x": -5.8283586502075195,
        "y": 0.7235209345817566,
        "year": "2024"
      },
      {
        "abstract": "This research paper presents an in-depth examination of bias identification in media content related to the Israel-Palestine war. Focusing on the annotation guidelines and process developed by our team of researchers, the document outlines a systematic approach to discerning bias in articles. Through meticulous analysis, key indicators of bias such as emotive language, weasel words, and loaded comparisons are identified and discussed. The paper also explores the delineation between facts and opinions, emphasizing the importance of maintaining objectivity in annotation. Ethical considerations, including the handling of sensitive data and the promotion of multipartiality among annotators, are carefully addressed. The annotation guidelines also include other ethical considerations such as identifying rumors, false information, exercising prudence and selective quotations. The research paper offers insights into the annotation experience, highlighting common mistakes and providing valuable guidelines for future research in bias identification. By providing a comprehensive framework for evaluating bias in media coverage of the Israel-Palestine war, this study contributes to a deeper understanding of the complexities inherent in media discourse surrounding contentious geopolitical issues.",
        "authors": [
          "Amanda Chan",
          "Mai A.Baddar",
          "Sofien Baazaoui"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "656",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "671",
        "paper_id": 72,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.72.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.72.jpg",
        "title": "Eagles at FIGNEWS 2024 Shared Task: A Context-informed Prescriptive Approach to Bias Detection in Contentious News Narratives",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.72",
        "x": -4.730180263519287,
        "y": 4.263159275054932,
        "year": "2024"
      },
      {
        "abstract": "The increasing use of artificial intelligence in healthcare requires robust datasets for training and validation, particularly in the domain of medical conversations. However, the creation and accessibility of such datasets in Arabic face significant challenges, especially due to the sensitivity and privacy concerns that are associated with medical conversations. These conversations are rarely recorded or preserved, making the availability of comprehensive Arabic medical dialogue datasets scarce. This limitation slows down not only the development of effective natural language processing models but also restricts the opportunity for open comparison of algorithms and their outcomes. Recent advancements in large language models (LLMs) like ChatGPT, GPT-4, Gemini-pro, and Claude-3 show promising capabilities in generating synthetic data. To address this gap, we introduce a novel Multi-Agent LLM approach capable of generating synthetic Arabic medical dialogues from patient notes, regardless of the original language. This development presents a significant step towards overcoming the barriers in dataset availability, enhancing the potential for broader research and application in AI-driven medical dialogue systems.",
        "authors": [
          "Mariam ALMutairi",
          "Lulwah AlKulaib",
          "Melike Aktas",
          "Sara Alsalamah",
          "Chang-Tien Lu"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "11",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "26",
        "paper_id": 2,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.2.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.2.jpg",
        "title": "Synthetic Arabic Medical Dialogues Using Advanced Multi-Agent LLM Techniques",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.2",
        "x": -6.572758197784424,
        "y": 0.5495544075965881,
        "year": "2024"
      },
      {
        "abstract": "The Coptic language, rooted in the historical landscapes of Egypt, continues to serve as a vital liturgical medium for the Coptic Orthodox and Catholic Churches across Egypt, North Sudan, Libya, and the United States, with approximately ten million speakers worldwide. However, the scarcity of digital resources in Coptic has resulted in its exclusion from digital systems, thereby limiting its accessibility and preservation in modern technological contexts. Our research addresses this issue by developing the most extensive parallel Coptic-centered corpus to date. This corpus comprises over 8,000 parallel sentences between Arabic and Coptic, and more than 24,000 parallel sentences between English and Coptic. We have also developed the first neural machine translation system between Coptic, English, and Arabic. Lastly, we evaluate the capability of leading proprietary Large Language Models (LLMs) to translate to and from Coptic using a few-shot learning approach (in-context learning). Our code and data are available athttps://github.com/UBC-NLP/copticmt.",
        "authors": [
          "Muhammed Saeed",
          "Asim Mohamed",
          "Mukhtar Mohamed",
          "Shady Shehata",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "298",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "308",
        "paper_id": 25,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.25.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.25.jpg",
        "title": "From Nile Sands to Digital Hands: Machine Translation of Coptic Texts",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.25",
        "x": -7.051135540008545,
        "y": -0.07064367830753326,
        "year": "2024"
      },
      {
        "abstract": "Natural Language Understanding (NLU) plays a vital role in Natural Language Processing (NLP) by facilitating semantic interactions. Arabic, with its diverse morphology, poses a challenge as it allows multiple interpretations of words, leading to potential misunderstandings and errors in NLP applications. In this paper, we present our approach for tackling Arabic NLU shared tasks for word sense disambiguation (WSD) and location mention disambiguation (LMD). Various approaches have been investigated from zero-shot inference of large language models (LLMs) to fine-tuning of pre-trained language models (PLMs). The best approach achieved 57% on WSD task ranking third place, while for the LMD task, our best systems achieved 94% MRR@1 ranking first place.",
        "authors": [
          "Reem Abdel-Salam"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "383",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "392",
        "paper_id": 33,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.33.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.33.jpg",
        "title": "rematchka at ArabicNLU2024: Evaluating Large Language Models for Arabic Word Sense and Location Sense Disambiguation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.33",
        "x": -4.79823637008667,
        "y": 3.8812265396118164,
        "year": "2024"
      },
      {
        "abstract": "In today\u2019s digital age, the spread of propaganda through news channels has become a pressing concern. To address this issue, the research community has organized a shared task on detecting propaganda in news posts. This paper aims to present the work carried out at the University of Tripoli for the development and implementation of data annotation guidelines by a team of five annotators. The guidelines were used to annotate 2600 news articles. Each article is labeled as \u201cpropaganda\u201d, \u201cNot propaganda\u201d, \u201cNot Applicable\u201d, or \u201cNot clear\u201d. The shared task results put our efforts in the third position among 6 participating teams in the consistency track.",
        "authors": [
          "Marwa Solla",
          "Hassan Ebrahem",
          "Alya Issa",
          "Harmain Harmain",
          "Abdusalam Nwesri"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "601",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "608",
        "paper_id": 64,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.64.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.64.jpg",
        "title": "Sahara Pioneers at FIGNEWS 2024 Shared Task: Data Annotation Guidelines for Propaganda Detection in News Items",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.64",
        "x": -3.9760613441467285,
        "y": 3.1707351207733154,
        "year": "2024"
      },
      {
        "abstract": "Arabic social media platforms are increasingly using propaganda to deceive or influence people. This propaganda is often spread through multimodal content, such as memes. While substantial research has addressed the automatic detection of propaganda in English content, this paper presents the MODOS team\u2019s participation in the Arabic Multimodal Propagandistic Memes Classification shared task. Our system deploys the Segment Anything Model (SAM) and CLIP for image representation and ARABIAN-GPT embeddings for text. Then, we employ LSTM encoders followed by a weighted fusion strategy to perform binary classification. Our system achieved competitive performance in distinguishing between propagandistic and non-propagandistic memes, scored 0.7290 macro F1, and ranked 6th among the participants.",
        "authors": [
          "Abdelhamid Haouhat",
          "Hadda Cherroun",
          "Slimane Bellaouar",
          "Attia Nehar"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "483",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "488",
        "paper_id": 48,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.48.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.48.jpg",
        "title": "MODOS at ArAIEval Shared Task: Multimodal Propagandistic Memes Classification Using Weighted SAM, CLIP and ArabianGPT",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.48",
        "x": -5.141633987426758,
        "y": -0.16842323541641235,
        "year": "2024"
      },
      {
        "abstract": "This paper investigates the optimization of propaganda technique detection in Arabic text, including tweets & news paragraphs, from ArAIEval shared task 1. Our approach involves fine-tuning the AraBERT v2 model with a neural network classifier for sequence tagging.Experimental results show relying on the first token of the word for technique prediction produces the best performance. In addition, incorporating genre information as a feature further enhances the model\u2019s performance. Our system achieved a score of 25.41, placing us 4th on the leaderboard. Subsequent post-submission improvements further raised our score to 26.68.",
        "authors": [
          "Abrar Abir",
          "Kemal Oflazer"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "489",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "493",
        "paper_id": 49,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.49.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.49.jpg",
        "title": "Nullpointer at ArAIEval Shared Task: Arabic Propagandist Technique Detection with Token-to-Word Mapping in Sequence Tagging",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.49",
        "x": -4.6249823570251465,
        "y": 2.379070281982422,
        "year": "2024"
      },
      {
        "abstract": "In this study, we aimed to identify biased language in a dataset provided by the FIGNEWS 2024 committee on the Gaza-Israel war. We classified entries into seven categories: Unbiased, Biased against Palestine, Biased against Israel, Biased against Others, Biased against both Palestine and Israel, Unclear, and Not Applicable. Our team reviewed the literature to develop a codebook of terminologies and definitions. By coding each example, we sought to detect language tendencies used by media outlets when reporting on the same event. The primary finding was that most examples were classified as \u201cBiased against Palestine,\u201d as all examined language data used one-sided terms to describe the October 7 event. The least used category was \u201cNot Applicable,\u201d reserved for irrelevant examples or those lacking context. It is recommended to use neutral and balanced language when reporting volatile political news.",
        "authors": [
          "Al Manar Al Wardi",
          "Blqees Al Busaidi",
          "Malath Al-Sibani",
          "Hiba Salim Muhammad Al-Siyabi",
          "Najma Al Zidjaly"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "609",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "613",
        "paper_id": 65,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.65.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.65.jpg",
        "title": "BiasGanda at FIGNEWS 2024 Shared Task: A Quest to Uncover Biased Views in News Coverage",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.65",
        "x": -4.531662464141846,
        "y": 2.1867642402648926,
        "year": "2024"
      },
      {
        "abstract": "Disambiguating a word\u2019s intended meaning(sense) in a given context is important in Nat-ural Language Understanding (NLU). WSDaims to determine the correct sense of ambigu-ous words in context. At the same time, LMD(a WSD variation) focuses on disambiguatinglocation mention. Both tasks are vital in Nat-ural Language Processing (NLP) and informa-tion retrieval, as they help correctly interpretand extract information from text. Arabic ver-sion is further challenging because of its mor-phological richness, encompassing a complexinterplay of roots, stems, and affixes. This pa-per describes our solutions to both tasks, em-ploying Llama3 and Cohere-based models un-der Zero-Shot Learning and Re-Ranking, re-spectively. Both the shared tasks were partof the second Arabic Natural Language Pro-cessing Conference co-located with ACL 2024.Overall, we achieved 1st rank in the WSD task(accuracy 78%) and 2nd rank in the LMD task(MRR@1 0.59)",
        "authors": [
          "Pawan Rajpoot",
          "Ashvini Jindal",
          "Ankur Parikh"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "377",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "382",
        "paper_id": 32,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.32.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.32.jpg",
        "title": "Upaya at ArabicNLU Shared-Task: Arabic Lexical Disambiguation using Large Language Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.32",
        "x": -2.2065558433532715,
        "y": 0.8460914492607117,
        "year": "2024"
      },
      {
        "abstract": "Diverging from the trend of the previous rumor verification studies, we introduce the new task of rumor verification using evidence that are exclusively captured from authorities, i.e., entities holding the right and knowledge to verify corresponding information. To enable research on this task for Arabic low-resourced language, we construct and release the first Authority-Rumor-Evidence Dataset (AuRED). The dataset comprises 160 rumors expressed in tweets and 692 Twitter timelines of authorities containing about 34k tweets. Additionally, we explore how existing evidence retrieval and claim verification models for fact-checking perform on our task under both the cross-lingual zero-shot and in-domain fine-tuning setups. Our experiments show that although evidence retrieval models perform relatively well on the task establishing strong baselines, there is still a big room for improvement. However, existing claim verification models perform poorly on the task no matter how good the retrieval performance is. The results also show that stance detection can be useful for evidence retrieval. Moreover, existing fact-checking datasets showed a potential in transfer learning to our task, however, further investigation using different datasets and setups is required.",
        "authors": [
          "Fatima Haouari",
          "Tamer Elsayed",
          "Reem Suwaileh"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "27",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "41",
        "paper_id": 3,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.3.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.3.jpg",
        "title": "AuRED: Enabling Arabic Rumor Verification using Evidence from Authorities over Twitter",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.3",
        "x": -5.096523761749268,
        "y": -0.05079743266105652,
        "year": "2024"
      },
      {
        "abstract": "This article presents the participation of \u201cThe Guideline Specialists\u201d in the FIGNEWS 2024 Shared Task, which aims to unravel bias and propaganda in news media narratives surrounding the Gaza-Israel 2023-2024 war. Leveraging innovative annotation methodologies and drawing on a diverse team of annotators, our approach focuses on meticulously annotating news articles using a linguistic approach to uncover the intricate nuances of bias. By incorporating detailed examples and drawing on related work that show how language structure represented in the use of passive voice or the use of nominalization and the choice of vocabulary carry bias, our findings provide valuable insights into the representation of the Gaza-Israel conflict across various languages and cultures. The guideline we developed detected the bias against Gaza, against Israel and others by setting keywords that are based on linguistic background tested by the AntConc concordance tool. The result was an annotation guideline that have a solid base. Through this collaborative effort, we developed a guideline that contributes to fostering a deeper understanding of media narratives during one of the most critical moments in recent history.",
        "authors": [
          "Ghizlane Bourahouat",
          "Samar Amer"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "672",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "676",
        "paper_id": 73,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.73.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.73.jpg",
        "title": "The Guidelines Specialists at FIGNEWS 2024 Shared Task: An annotation guideline to Unravel Bias in News Media Narratives Using a Linguistic Approach",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.73",
        "x": -5.703115463256836,
        "y": 3.7321524620056152,
        "year": "2024"
      },
      {
        "abstract": "We present WojoodNER-2024, the second Arabic Named Entity Recognition (NER) Shared Task. In WojoodNER-2024, we focus on fine-grained Arabic NER. We provided participants with a new Arabic fine-grained NER dataset called Wojoodfine, annotated with subtypes of entities. WojoodNER-2024 encompassed three subtasks: (i) Closed-Track Flat Fine-Grained NER, (ii) Closed-Track Nested Fine-Grained NER, and (iii) an Open-Track NER for the Israeli War on Gaza. A total of 43 unique teams registered for this shared task. Five teams participated in the Flat Fine-Grained Subtask, among which two teams tackled the Nested Fine-Grained Subtask and one team participated in the Open-Track NER Subtask. The winning teams achievedF1scores of 91% and 92% in the Flat Fine-Grained and Nested Fine-Grained Subtasks, respectively. The sole team in the Open-Track Subtask achieved anF1score of 73.7%.",
        "authors": [
          "Mustafa Jarrar",
          "Nagham Hamad",
          "Mohammed Khalilia",
          "Bashar Talafha",
          "Abdelrahim Elmadany",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "847",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "857",
        "paper_id": 101,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.101.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.101.jpg",
        "title": "WojoodNER 2024: The Second Arabic Named Entity Recognition Shared Task",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.101",
        "x": -4.705744743347168,
        "y": 2.300678014755249,
        "year": "2024"
      },
      {
        "abstract": "The rise of memes as a tool for spreading propaganda presents a significant challenge in the current digital environment. In this paper, we outline our work for the ArAIEval Shared Task2 in ArabicNLP 2024. This study introduces a method for identifying propaganda in Arabic memes using a multimodal system that combines textual and visual indicators to enhance the result. Our approach achieves the first place in text classification with Macro-F1 of 78.69%, the third place in image classification with Macro-F1 of 65.92%, and the first place in multimodal classification with Macro-F1 of 80.51%",
        "authors": [
          "Mohamed Zaytoon",
          "Nagwa M. El-Makky",
          "Marwan Torki"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "512",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "517",
        "paper_id": 53,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.53.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.53.jpg",
        "title": "AlexUNLP-MZ at ArAIEval Shared Task: Contrastive Learning, LLM Features Extraction and Multi-Objective Optimization for Arabic Multi-Modal Meme Propaganda Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.53",
        "x": -6.826107978820801,
        "y": -0.15692271292209625,
        "year": "2024"
      },
      {
        "abstract": "Detecting propaganda in multimodal content, such as memes, is crucial for combating disinformation on social media. This paper presents a novel approach for the ArAIEval 2024 shared Task 2 on Multimodal Propagandistic Memes Classification, involving text, image, and multimodal classification of Arabic memes. For text classification (Task 2A), we fine-tune state-of-the-art Arabic language models and use ChatGPT4-generated synthetic text for data augmentation. For image classification (Task 2B), we fine-tune ResNet18, EfficientFormerV2, and ConvNeXt-tiny architectures with DALL-E-2-generated synthetic images. For multimodal classification (Task 2C), we combine ConvNeXt-tiny and BERT architectures in a fusion layer to enhance binary classification. Our results show significant performance improvements with data augmentation for text and image classification models and with the fusion layer for multimodal classification. We highlight challenges and opportunities for future research in multimodal propaganda detection in Arabic content, emphasizing the need for robust and adaptable models to combat disinformation.",
        "authors": [
          "Uzair Shah",
          "Md. Rafiul Biswas",
          "Marco Agus",
          "Mowafa Househ",
          "Wajdi Zaghouani"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "467",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "472",
        "paper_id": 45,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.45.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.45.jpg",
        "title": "MemeMind at ArAIEval Shared Task: Generative Augmentation and Feature Fusion for Multimodal Propaganda Detection in Arabic Memes through Advanced Language and Vision Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.45",
        "x": -3.887613534927368,
        "y": 3.1324448585510254,
        "year": "2024"
      },
      {
        "abstract": "Speech encoders pretrained through self-supervised learning (SSL) have demonstrated remarkable performance in various downstream tasks, including Spoken Language Understanding (SLU) and Automatic Speech Recognition (ASR). For instance, fine-tuning SSL models for such tasks has shown significant potential, leading to improvements in the SOTA performance across challenging datasets.In contrast to existing research, this paper contributes by comparing the effectiveness of SSL approaches in the context of (i) the low-resource Spoken Tunisian Arabic Dialect and (ii) its combination with a low-resource SLU and ASR scenario, where only a few semantic annotations are available for fine-tuning. We conducted experiments using many SSL speech encoders on the TARIC-SLU dataset. We used speech encoders that were pre-trained on either monolingual or multilingual speech data. Some of them have also been refined without in-domain nor Tunisian data through a multimodal supervised teacher-student learning. The study made in this paper yields numerous significant findings that we will discuss in the paper.",
        "authors": [
          "Salima Mdhaffar",
          "Haroun Elleuch",
          "Fethi Bougares",
          "Yannick Est\u00e8ve"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "130",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "139",
        "paper_id": 12,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.12.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.12.jpg",
        "title": "Performance Analysis of Speech Encoders for Low-Resource SLU and ASR in Tunisian Dialect",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.12",
        "x": -5.28562068939209,
        "y": 0.8101703524589539,
        "year": "2024"
      },
      {
        "abstract": "The proliferation of bias and propaganda onsocial media is an increasingly significant concern,leading to the development of techniquesfor automatic detection. This article presents amultilingual corpus of 12, 000 Facebook postsfully annotated for bias and propaganda. Thecorpus was created as part of the FigNews2024 Shared Task on News Media Narrativesfor framing the Israeli War on Gaza. It coversvarious events during the War from October7, 2023 to January 31, 2024. The corpuscomprises 12, 000 posts in five languages (Arabic,Hebrew, English, French, and Hindi), with2, 400 posts for each language. The annotationprocess involved 10 graduate students specializingin Law. The Inter-Annotator Agreement(IAA) was used to evaluate the annotationsof the corpus, with an average IAA of 80.8%for bias and 70.15% for propaganda annotations.Our team was ranked among the bestperformingteams in both Bias and Propagandasubtasks. The corpus is open-source and availableat https://sina.birzeit.edu/fada",
        "authors": [
          "Lina Duaibes",
          "Areej Jaber",
          "Mustafa Jarrar",
          "Ahmad Qadi",
          "Mais Qandeel"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "640",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "645",
        "paper_id": 69,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.69.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.69.jpg",
        "title": "Sina at FigNews 2024: Multilingual Datasets Annotated with Bias and Propaganda.",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.69",
        "x": -6.732176303863525,
        "y": 0.3128661513328552,
        "year": "2024"
      },
      {
        "abstract": "DA-MSA Machine Translation is a recentchallenge due to the multitude of Arabic dialects and their variations. In this paper, we present our results within the context of Subtask 3 of the NADI-2024 Shared Task(Abdul-Mageed et al., 2024) that is DA-MSA Machine Translation . We utilized the DIALECTS008MSA MADAR corpus (Bouamor et al., 2018),the Emi-NADI corpus for the Emirati dialect (Khered et al., 2023), and we augmented thePalestinian and Jordanian datasets based onNADI 2021. Our approach involves develop013ing sentence-level machine translations fromPalestinian, Jordanian, Emirati, and Egyptiandialects to Modern Standard Arabic (MSA).To016 address this challenge, we fine-tuned models such as (Nagoudi et al., 2022)AraT5v2-msa-small, AraT5v2-msa-base, and (Elmadanyet al., 2023)AraT5v2-base-1024 to comparetheir performance. Among these, the AraT5v2-base-1024 model achieved the best accuracy, with a BLEU score of 0.1650 on the develop023ment set and 0.1746 on the test set.",
        "authors": [
          "Manan AlMusallam",
          "Samar Ahmad"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "764",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "768",
        "paper_id": 86,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.86.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.86.jpg",
        "title": "Alson at NADI 2024 shared task: Alson - A fine-tuned model for Arabic Dialect Translation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.86",
        "x": -1.8477153778076172,
        "y": 0.4794544279575348,
        "year": "2024"
      },
      {
        "abstract": "As part of our study, we worked on three tasks:stance detection, sarcasm detection and senti-ment analysis using fine-tuning techniques onBERT-based models. Fine-tuning parameterswere carefully adjusted over multiple iterationsto maximize model performance. The threetasks are essential in the field of natural lan-guage processing (NLP) and present uniquechallenges. Stance detection is a critical taskaimed at identifying a writer\u2019s stances or view-points in relation to a topic. Sarcasm detectionseeks to spot sarcastic expressions, while senti-ment analysis determines the attitude expressedin a text. After numerous experiments, we iden-tified Arabert-twitter as the model offering thebest performance for all three tasks. In particu-lar, it achieves a macro F-score of 78.08% forstance detection, a macro F1-score of 59.51%for sarcasm detection and a macro F1-score of64.57% for sentiment detection. .Our source code is available at https://github.com/MezghaniAmal/Mawqif",
        "authors": [
          "Mezghani Amal",
          "Rahma Boujelbane",
          "Mariem Ellouze Khemekhem"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "788",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "793",
        "paper_id": 90,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.90.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.90.jpg",
        "title": "ANLP RG at StanceEval2024: Comparative Evaluation of Stance, Sentiment and Sarcasm Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.90",
        "x": -4.481500148773193,
        "y": 4.503652572631836,
        "year": "2024"
      },
      {
        "abstract": "This paper presents our system \u201cmuNERa\u201d, submitted to the WojoodNER 2024 shared task at the second ArabicNLP conference. We participated in two subtasks, the flat and nested fine-grained NER sub-tasks (1 and 2). muNERa achieved first place in the nested NER sub-task and second place in the flat NER sub-task. The system is based on the TANL framework (CITATION),by using a sequence-to-sequence structured language translation approach to model both tasks. We utilize the pre-trained AraT5v2-base model as the base model for the TANL framework. The best-performing muNERa model achieves 91.07% and 90.26% for the F-1 scores on the test sets for the nested and flat subtasks, respectively.",
        "authors": [
          "Nouf Alotaibi",
          "Haneen Alhomoud",
          "Hanan Murayshid",
          "Waad Alshammari",
          "Nouf Alshalawi",
          "Sakhar Alkhereyf"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "858",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "866",
        "paper_id": 102,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.102.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.102.jpg",
        "title": "muNERa at WojoodNER 2024: Multi-tasking NER Approach",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.102",
        "x": -5.93240213394165,
        "y": 3.4782936573028564,
        "year": "2024"
      },
      {
        "abstract": "This paper focuses on detecting propagandistic spans and persuasion techniques in Arabic text from tweets and news paragraphs. Each entry in the dataset contains a text sample and corresponding labels that indicate the start and end positions of propaganda techniques within the text. Tokens falling within a labeled span were assigned \u2019B\u2019 (Begin) or \u2019I\u2019 (Inside) tags, \u2019O\u2019, corresponding to the specific propaganda technique. Using attention masks, we created uniform lengths for each span and assigned BIO tags to each token based on the provided labels. Then, we used AraBERT-base pre-trained model for Arabic text tokenization and embeddings with a token classification layer to identify propaganda techniques. Our training process involves a two-phase fine-tuning approach. First, we train only the classification layer for a few epochs, followed by full model fine-tuning, updating all parameters. This methodology allows the model to adapt to the specific characteristics of the propaganda detection task while leveraging the knowledge captured by the pretrained AraBERT model. Our approach achieved an F1 score of 0.2774, securing the 3rd position in the leaderboard of Task 1.",
        "authors": [
          "Md. Rafiul Biswas",
          "Zubair Shah",
          "Wajdi Zaghouani"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "494",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "500",
        "paper_id": 50,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.50.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.50.jpg",
        "title": "MemeMind at ArAIEval Shared Task: Spotting Persuasive Spans in Arabic Text with Persuasion Techniques Identification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.50",
        "x": -5.586221694946289,
        "y": -0.20540224015712738,
        "year": "2024"
      },
      {
        "abstract": "This paper describes our participation in the ArAIEval Shared Task 2024, focusing on Task 2C, which challenges participants to detect propagandistic elements in multimodal Arabic memes. The challenge involves analyzing both the textual and visual components of memes to identify underlying propagandistic messages. Our approach integrates the capabilities of MARBERT and ResNet50, top-performing pre-trained models for text and image processing, respectively. Our system architecture combines these models through a fusion layer that integrates and processes the extracted features, creating a comprehensive representation that is more effective in detecting nuanced propaganda. Our proposed system achieved significant success, placing second with an F1 score of 0.7987.",
        "authors": [
          "Yasser Alhabashi",
          "Abdullah Alharbi",
          "Samar Ahmad",
          "Serry Sibaee",
          "Omer Nacar",
          "Lahouari Ghouti",
          "Anis Koubaa"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "473",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "477",
        "paper_id": 46,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.46.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.46.jpg",
        "title": "ASOS at ArAIEval Shared Task: Integrating Text and Image Embeddings for Multimodal Propaganda Detection in Arabic Memes",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.46",
        "x": -3.7603397369384766,
        "y": 3.3193702697753906,
        "year": "2024"
      },
      {
        "abstract": "This paper describes our submissions to the Multi-label Country-level Dialect Identification subtask of the NADI2024 shared task, organized during the second edition of the ArabicNLP conference. Our submission is based on the ensemble of fine-tuned BERT-based models, after implementing the Similarity-Induced Mono-to-Multi Label Transformation (SIMMT) on the input data. Our submission ranked first with a Macro-Average (MA) F1 score of 50.57%.",
        "authors": [
          "Amira Karoui",
          "Farah Gharbi",
          "Rami Kammoun",
          "Imen Laouirine",
          "Fethi Bougares"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "758",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "763",
        "paper_id": 85,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.85.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.85.jpg",
        "title": "ELYADATA at NADI 2024 shared task: Arabic Dialect Identification with Similarity-Induced Mono-to-Multi Label Transformation.",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.85",
        "x": -4.602200031280518,
        "y": 3.66819167137146,
        "year": "2024"
      },
      {
        "abstract": "This research explores the effectiveness of using pre-trained language models (PLMs) as feature extractors for Arabic stance detection on social media, focusing on topics like women empowerment, COVID-19 vaccination, and digital transformation. By leveraging sentence transformers to extract embeddings and incorporating aggregation architectures on top of BERT, we aim to achieve high performance without the computational expense of fine-tuning. Our approach demonstrates significant resource and time savings while maintaining competitive performance, scoring an F1-score of 78.62 on the test set. This study highlights the potential of PLMs in enhancing stance detection in Arabic social media analysis, offering a resource-efficient alternative to traditional fine-tuning methods.",
        "authors": [
          "Omar Galal",
          "Abdelrahman Kaseb"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "783",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "787",
        "paper_id": 89,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.89.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.89.jpg",
        "title": "Team_Zero at StanceEval2024: Frozen PLMs for Arabic Stance Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.89",
        "x": -6.91724967956543,
        "y": -0.011778293177485466,
        "year": "2024"
      },
      {
        "abstract": "This paper presents The_CyberEquity_Lab team\u2019s participation in the FIGNEWS 2024 Shared Task (Zaghouani, et al., 2024). The task is to annotate a corpus of Facebook posts into bias and propaganda in covering the Gaza-Israel war. The posts represent news articles written in five different languages. The paper presents the guidelines of annotation that the team has adhered in identifying both bias and propaganda in coverage of this continuous conflict.",
        "authors": [
          "Mohammed Helal",
          "Radi Jarrar",
          "Mohammed Alkhanafseh",
          "Abdallah Karakra",
          "Ruba Awadallah"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "614",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "619",
        "paper_id": 66,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.66.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.66.jpg",
        "title": "The CyberEquity Lab at FIGNEWS 2024 Shared Task: Annotating a Corpus of Facebook Posts to Label Bias and Propaganda in Gaza-Israel War Coverage in Five Languages",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.66",
        "x": -4.47974157333374,
        "y": 3.967329978942871,
        "year": "2024"
      },
      {
        "abstract": "This paper is a part of the FIGNEWS 2024 Datathon Shared Task and it aims to investigate bias and double standards in media coverage of the Gaza-Israel 2023-2024 conflict through a comprehensive analysis of news articles. The methodology integrated both manual labeling as well as the application of a natural language processing (NLP) tool, which is the Facebook/BART-large-MNLI model. The annotation process involved categorizing the dataset based on identified biases, following a set of guidelines in which categories of bias were defined by the team. The findings revealed that most of the media texts provided for analysis included bias against Palestine, whether it was through the use of biased vocabulary or even tone. It was also found that texts written in Hebrew contained the most bias against Palestine. In addition, when comparing annotations done by AAI-1 and AAI-2, the results turned out to be very similar, which might be mainly due to the clear annotation guidelines set by the annotators themselves. Thus, we recommend the use of clear guidelines to facilitate the process of annotation by future researchers.",
        "authors": [
          "Asmahan Al-Mamari",
          "Fatma Al-Farsi",
          "Najma Zidjaly"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "646",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "650",
        "paper_id": 70,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.70.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.70.jpg",
        "title": "SQUad at FIGNEWS 2024 Shared Task: Unmasking Bias in Social Media Through Data Analysis and Annotation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.70",
        "x": -4.4410295486450195,
        "y": 4.796630859375,
        "year": "2024"
      },
      {
        "abstract": "In response to the evolving media representation of the Gaza-Israel conflict, this study aims to categorize news articles based on their bias towards specific entities. Our primary objective is to annotate news articles with labels that indicate their bias: \u201cUnbiased\u201d, \u201cBiased against Palestine\u201d, \u201cBiased against Israel\u201d, \u201cBiased against both Palestine and Israel\u201d, \u201cBiased against others\u201d, \u201cUnclear\u201d, or \u201cNot Applicable\u201d.The methodology involves a detailed annotation process where each article is carefully reviewed and labeled according to predefined guidelines. For instance, an article reporting factual events without derogatory language is labeled as \u201cUnbiased\u201d, while one using inflammatory language against Palestinians is marked as \u201cBiased against Palestine\u201d.Key findings include the identification of various degrees of bias in news articles, highlighting the importance of critical analysis in media consumption. This research contributes to the broader effort of understanding media bias and promoting unbiased journalism. Tools such as Google Drive and Google Sheets facilitated the annotation process, enabling efficient collaboration and data management among the annotators.Our work also includes comprehensive guidelines and examples to ensure consistent annotation, enhancing the reliability of the data.",
        "authors": [
          "Amr Saleh",
          "Huda Mohamed",
          "Hager Sayed"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "651",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "655",
        "paper_id": 71,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.71.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.71.jpg",
        "title": "JusticeLeague at FIGNEWS 2024 Shared Task: Innovations in Bias Annotation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.71",
        "x": -6.901291847229004,
        "y": 0.0792333334684372,
        "year": "2024"
      },
      {
        "abstract": "Names carry important information about our identities and demographics such as gender, nationality, ethnicity, etc. We investigate the use of individual\u2019s name, in both Arabic and English, to predict important attributes, namely country, region, gender, and language. We extract data from Wikidata, and normalize it, to build a comprehensive dataset consisting of more than 1 million entities and their normalized attributes. We experiment with a Linear SVM approach, as well as two Transformers approaches consisting of BERT model fine-tuning and Transformers pipeline. Our results indicate that we can predict the gender, language and region using the name only with a confidence over 0.65. The country attribute can be predicted with less accuracy. The Linear SVM approach outperforms the other approaches for all the attributes. The best performing approach was also evaluated on another dataset that consists of 1,500 names from 15 countries (covering different regions) extracted from Twitter, and yields similar results.",
        "authors": [
          "Samir Abdaljalil",
          "Hamdy Mubarak"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "1",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "10",
        "paper_id": 1,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.1.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.1.jpg",
        "title": "Wikidata as a Source of Demographic Information",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.1",
        "x": -4.3936052322387695,
        "y": 4.69377326965332,
        "year": "2024"
      },
      {
        "abstract": "This paper presents an overview of the Arabic Natural Language Understanding (ArabicNLU 2024) shared task, focusing on two subtasks: Word Sense Disambiguation (WSD) and Location Mention Disambiguation (LMD). The task aimed to evaluate the ability of automated systems to resolve word ambiguity and identify locations mentioned in Arabic text. We provided participants with novel datasets, including a sense-annotated corpus for WSD, called SALMA with approximately 34k annotated tokens, and the dataset with 3,893 annotations and 763 unique location mentions. These are challenging tasks. Out of the 38 registered teams, only three teams participated in the final evaluation phase, with the highest accuracy being 77.8% for WSD and 95.0% for LMD. The shared task not only facilitated the evaluation and comparison of different techniques, but also provided valuable insights and resources for the continued advancement of Arabic NLU technologies.",
        "authors": [
          "Mohammed Khalilia",
          "Sanad Malaysha",
          "Reem Suwaileh",
          "Mustafa Jarrar",
          "Alaa Aljabari",
          "Tamer Elsayed",
          "Imed Zitouni"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "361",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "371",
        "paper_id": 30,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.30.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.30.jpg",
        "title": "ArabicNLU 2024: The First Arabic Natural Language Understanding Shared Task",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.30",
        "x": -5.392040729522705,
        "y": -0.11987245082855225,
        "year": "2024"
      },
      {
        "abstract": "Recently, there has been a growing interest in analyzing user-generated text to understand opinions expressed on social media. In NLP, this task is known as stance detection, where the goal is to predict whether the writer is in favor, against, or has no opinion on a given topic. Stance detection is crucial for applications such as sentiment analysis, opinion mining, and social media monitoring, as it helps in capturing the nuanced perspectives of users on various subjects. As part of the ArabicNLP 2024 program, we organized the first shared task on Arabic Stance Detection, StanceEval 2024. This initiative aimed to foster advancements in stance detection for the Arabic language, a relatively underrepresented area in Arabic NLP research. This overview paper provides a detailed description of the shared task, covering the dataset, the methodologies used by various teams, and a summary of the results from all participants. We received 28 unique team registrations, and during the testing phase, 16 teams submitted valid entries. The highest classification F-score obtained was 84.38.",
        "authors": [
          "Nora Alturayeif",
          "Hamzah Luqman",
          "Zaid Alyafeai",
          "Asma Yamani"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "774",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "782",
        "paper_id": 88,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.88.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.88.jpg",
        "title": "StanceEval 2024: The First Arabic Stance Detection Shared Task",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.88",
        "x": -5.8223876953125,
        "y": 3.6143646240234375,
        "year": "2024"
      },
      {
        "abstract": "This paper introduces the methodology of BSC-LANGTECH team for the FIGNEWS 2024 Shared Task on News Media Narratives. Following the bias annotation subtask, we apply the theory and methods of framing analysis to develop guidelines to annotate bias in the corpus provided by the task organizators. The manual annotation of a subset, with which a moderate IAA agreement has been achieved, is further used in Deep Learning techniques to explore automatic annotation and test the reliability of our framework.",
        "authors": [
          "Valle Ruiz-Fern\u00e1ndez",
          "Jos\u00e9 Saiz",
          "Aitor Gonz\u00e1lez-Agirre"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "620",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "629",
        "paper_id": 67,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.67.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.67.jpg",
        "title": "BSC-LANGTECH at FIGNEWS 2024 Shared Task: Exploring Semi-Automatic Bias Annotation using Frame Analysis",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.67",
        "x": -2.126187801361084,
        "y": 0.7138025164604187,
        "year": "2024"
      },
      {
        "abstract": "This paper presents our submission for the Stance Detection in Arabic Language (StanceEval) 2024 shared task conducted by Team SMASH of the University of Edinburgh. We evaluated the performance of various BERT-based and large language models (LLMs). MARBERT demonstrates superior performance among the BERT-based models, achieving F1 and macro-F1 scores of 0.570 and 0.770, respectively. In contrast, Command R model outperforms all models with the highest overall F1 score of 0.661 and macro F1 score of 0.820.",
        "authors": [
          "Youssef Al Hariri",
          "Ibrahim Abu Farha"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "800",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "806",
        "paper_id": 92,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.92.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.92.jpg",
        "title": "SMASH at StanceEval 2024: Prompt Engineering LLMs for Arabic Stance Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.92",
        "x": -4.296047210693359,
        "y": 4.82323694229126,
        "year": "2024"
      },
      {
        "abstract": "Accurate translation of terminology and adaptation to in-context information is a pillar to high quality translation. Recently, there is a remarkable interest towards the use and the evaluation of Large Language Models (LLMs) particularly for Machine Translation tasks. Nevertheless, despite their recent advancement and ability to understand and generate human-like language, these LLMs are still far from perfect, especially in domain-specific scenarios, and need to be thoroughly investigated. This is particularly evident in automatically translating legal terminology from Arabic into English and French, where, beyond the inherent complexities of legal language and specialised translations, technical limitations of LLMs further hinder accurate generation of text. In this paper, we present a preliminary evaluation of two evolving LLMs, namely GPT-4 Generative Pre-trained Transformer and Gemini, as legal translators of Arabic legislatives to test their accuracy and the extent to which they care for context and terminology across two language pairs (AR\u2192EN / AR\u2192FR). The study targets the evaluation of Zero-Shot prompting for in-context and out-of-context scenarios of both models relying on a gold standard dataset, verified by professional translators who are also experts in the field. We evaluate the results applying the Multidimensional Quality Metrics to classify translation errors. Moreover, we also evaluate the general LLMs outputs to verify their correctness, consistency, and completeness. In general, our results show that the models are far from perfect and recall for more fine-tuning efforts using specialised terminological data in the legal domain from Arabic into English and French.",
        "authors": [
          "Khadija Ait ElFqih",
          "Johanna Monti"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "111",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "122",
        "paper_id": 10,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.10.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.10.jpg",
        "title": "Large Language Models as Legal Translators of Arabic Legislation: Do ChatGPT and Gemini Care for Context and Terminology?",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.10",
        "x": -6.698945045471191,
        "y": -0.0558215007185936,
        "year": "2024"
      },
      {
        "abstract": "This paper presents our system submitted for Task 1 of the ArAIEval Shared Task on Unimodal (Text) Propagandistic Technique Detection in Arabic. Task 1 involves identifying all employed propaganda techniques in a given text from a set of possible techniques or detecting that no propaganda technique is present. Additionally, the task requires identifying the specific spans of text where these techniques occur. We explored the capabilities of a multilingual BERT model for this task, focusing on the effectiveness of using outputs from different hidden layers within the model. By fine-tuning the multilingual BERT, we aimed to improve the model\u2019s ability to recognize and locate various propaganda techniques. Our experiments showed that leveraging the hidden layers of the BERT model enhanced detection performance. Our system achieved competitive results, ranking second in the shared task, demonstrating that multilingual BERT models, combined with outputs from hidden layers, can effectively detect and identify spans of propaganda techniques in Arabic text.",
        "authors": [
          "Md Abdur Razzaq Riyadh",
          "Sara Nabhani"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "478",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "482",
        "paper_id": 47,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.47.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.47.jpg",
        "title": "Mela at ArAIEval Shared Task: Propagandistic Techniques Detection in Arabic with a Multilingual Approach",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.47",
        "x": -5.352066516876221,
        "y": 0.41891828179359436,
        "year": "2024"
      },
      {
        "abstract": "We present the CLTL system designed for the ArAIEval Shared Task 2024 on multimodal propagandistic memes classification in Arabic. The challenge was divided into three subtasks: identifying propagandistic content from textual modality of memes (subtask 2A), from visual modality of memes (subtask 2B), and in a multimodal scenario when both modalities are combined (subtask 2C). We explored various unimodal transformer models for Arabic language processing (subtask 2A), visual models for image processing (subtask 2B), and concatenated text and image embeddings using the Multilayer Perceptron fusion module for multimodal propagandistic memes classification (subtask 2C). Our system achieved 77.96% for subtask 2A, 71.04% for subtask 2B, and 79.80% for subtask 2C, ranking 2nd, 1st, and 3rd on the leaderboard.",
        "authors": [
          "Yeshan Wang",
          "Ilia Markov"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "501",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "506",
        "paper_id": 51,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.51.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.51.jpg",
        "title": "CLTL at ArAIEval Shared Task: Multimodal Propagandistic Memes Classification Using Transformer Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.51",
        "x": -4.260290622711182,
        "y": 2.465775489807129,
        "year": "2024"
      },
      {
        "abstract": "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that focuses on extracting entities such as names of people, organizations, locations, and dates from text. Despite significant advancements due to deep learning and transformer architectures like BERT, NER still faces challenges, particularly in low-resource languages like Arabic. This paper presents a BERT-based NER system that utilizes a two-channel parallel hybrid neural network with an attention mechanism specifically designed for the NER Shared Task 2024. In the competition, our approach ranked second by scoring 90.13% in micro-F1 on the test set. The results demonstrate the effectiveness of combining advanced neural network architectures with contextualized word embeddings in improving NER performance for Arabic.",
        "authors": [
          "Issam Yahia",
          "Houdaifa Atou",
          "Ismail Berrada"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "867",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "873",
        "paper_id": 103,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.103.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.103.jpg",
        "title": "Addax at WojoodNER 2024: Attention-Based Dual-Channel Neural Network for Arabic Named Entity Recognition",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.103",
        "x": -6.454200744628906,
        "y": 1.789971947669983,
        "year": "2024"
      },
      {
        "abstract": "This paper outlines the University of Tripoli\u2019s initiative in creating annotation guidelines to detect bias in news articles concerning the Palestinian-Israeli conflict. Our team participated in the Framing of Israeli Gaza News Media Narrative (FIGNEWS 2024) shared task. We developed annotation guidelines to label bias in news articles. Using those guidelines we managed to annotate 3,900 articles with the aid of our custom-developed annotation tool. Among 16 participating teams, we scored 48.7 on the macro F1 measure in the quality track in which we ranked 4th. In the centrality track we were ranked at the 6th position using the macro F1 avg measure, however, we achieved the 4th best kappa coefficient. Our bias annotation guidelines was ranked in the 9th position.",
        "authors": [
          "Abdusalam Nwesri",
          "Mai Elbaabaa",
          "Fatima Lashihar",
          "Fatma Alalos"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "567",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "572",
        "paper_id": 60,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.60.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.60.jpg",
        "title": "Uot1 at FIGNEWS 2024 Shared Task: Labeling News Bias",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.60",
        "x": -5.3078484535217285,
        "y": 0.0028509204275906086,
        "year": "2024"
      },
      {
        "abstract": "Arabic banking intent detection represents a challenging problem across multiple dialects. It imposes generalization difficulties due to the scarcity of Arabic language and its dialects resources compared to English. We propose a methodology that leverages contrastive training to overcome this limitation. We also augmented the data with several dialects using a translation model. Our experiments demonstrate the ability of our approach in capturing linguistic nuances across different Arabic dialects as well as accurately differentiating between banking intents across diverse linguistic landscapes. This would enhance multi-dialect banking services in the Arab world with limited Arabic language resources. Using our proposed method we achieved second place on subtask 1 leaderboard of the AraFinNLP2024 shared task with micro-F1 score of 0.8762 on the test split.",
        "authors": [
          "Hossam Elkordi",
          "Ahmed Sakr",
          "Marwan Torki",
          "Nagwa M. El-Makky"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "415",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "421",
        "paper_id": 37,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.37.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.37.jpg",
        "title": "AlexuNLP24 at AraFinNLP2024: Multi-Dialect Arabic Intent Detection with Contrastive Learning in Banking Domain",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.37",
        "x": -3.0504884719848633,
        "y": 2.294017791748047,
        "year": "2024"
      },
      {
        "abstract": "Relational entity extraction is key in building knowledge graphs. A relational entity has a source, a tail and a type. In this paper, we consider Arabic text and introduce evidence enrichment which intuitively informs models for better predictions. Relational evidence is an expression in the text that explains how sources and targets relate. This paper augments the existing SREDFMrelational extraction dataset with evidence annotation to its 2.9-million Arabic relations. We leverage the augmented dataset to build AREEj, a relation extraction with evidence model from Arabic documents. The evidence augmentation model we constructed to complete the dataset achieved .82 F1-score (.93 precision, .73 recall). The target AREEj outperformed SOTA mREBEL with .72 F1-score (.78 precision, .66 recall).",
        "authors": [
          "Osama Rakan Al Mraikhat",
          "Hadi Hamoud",
          "Fadi A. Zaraket"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "67",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "72",
        "paper_id": 6,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.6.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.6.jpg",
        "title": "AREEj: Arabic Relation Extraction with Evidence",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.6",
        "x": -2.4696578979492188,
        "y": 0.7079826593399048,
        "year": "2024"
      },
      {
        "abstract": "Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the comprehension of Arabic text by removing ambiguity and minimizing the risk of misinterpretations caused by its absence.It plays a crucial role in improving Arabic text processing, particularly in applications such as text-to-speech and machine translation.This paper introduces a new approach to training ATD models.First, we finetuned two transformers, encoder-only and encoder-decoder, that were initialized from a pretrained character-based BERT.Then, we applied the Noisy-Student approach to boost the performance of the best model.We evaluated our models alongside 11 commercial and open-source models using two manually labeled benchmark datasets: WikiNews and our CATT dataset.Our findings show that our top model surpasses all evaluated models by relative Diacritic Error Rates (DERs) of 30.83% and 35.21% on WikiNews and CATT, respectively, achieving state-of-the-art in ATD.In addition, we show that our model outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36%.We open-source our CATT models and benchmark dataset for the research community .",
        "authors": [
          "Faris Alasmary",
          "Orjuwan Zaafarani",
          "Ahmad Ghannam"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "250",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "257",
        "paper_id": 21,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.21.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.21.jpg",
        "title": "CATT: Character-based Arabic Tashkeel Transformer",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.21",
        "x": -4.0977654457092285,
        "y": 0.7293887734413147,
        "year": "2024"
      },
      {
        "abstract": "This paper outlines our approach to the StanceEval 2024- Arabic Stance Evaluation shared task. The goal of the task was to identify the stance, one out of three (Favor, Against or None) towards tweets based on three topics, namely- COVID-19 Vaccine, Digital Transformation and Women Empowerment. Our approach consists of fine-tuning BERT-based models efficiently for both, Single-Task Learning as well as Multi-Task Learning, the details of which are discussed. Finally, an ensemble was implemented on the best-performing models to maximize overall performance. We achieved a macro F1 score of 78.02% in this shared task. Our codebase is available publicly.",
        "authors": [
          "Ishaan Shukla",
          "Ankit Vaidya",
          "Geetanjali Kale"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "837",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "841",
        "paper_id": 99,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.99.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.99.jpg",
        "title": "PICT at StanceEval2024: Stance Detection in Arabic using Ensemble of Large Language Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.99",
        "x": -1.2596685886383057,
        "y": 1.901888370513916,
        "year": "2024"
      },
      {
        "abstract": "This research paper presents our approach for the KSAA-CAD 2024 competition, focusing on Arabic Reverse Dictionary (RD) task (Alshammari et al., 2024). Leveraging the functionalities of the Arabic Reverse Dictionary, our system allows users to input glosses and retrieve corresponding words. We provide all associated notebooks and developed models on GitHub and Hugging face, respectively. Our task entails working with a dataset comprising dictionary data and word embedding vectors, utilizing three different architectures of contextualized word embeddings: AraELECTRA, AraBERTv2, and camelBERT-MSA. We fine-tune the AraT5v2-base-1024 model for predicting each embedding, considering various hyperparameters for training and validation. Evaluation metrics include ranking accuracy, mean squared error (MSE), and cosine similarity. The results demonstrate the effectiveness of our approach on both development and test datasets, showcasing promising performance across different embedding types.",
        "authors": [
          "Thamer Alharbi"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "692",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "696",
        "paper_id": 76,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.76.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.76.jpg",
        "title": "MISSION at KSAA-CAD 2024: AraT5 with Arabic Reverse Dictionary",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.76",
        "x": -0.9858043193817139,
        "y": 6.652247428894043,
        "year": "2024"
      },
      {
        "abstract": "In this paper, we present our submission for the WojoodNER 2024 Shared Tasks addressing flat and nested sub-tasks (1, 2). We experiment with three different approaches. We train (i) an Arabic fine-tuned version of BLOOMZ-7b-mt, GEMMA-7b, and AraBERTv2 on multi-label token classifications task; (ii) two AraBERTv2 models, on main types and sub-types respectively; and (iii) one model for main types and four for the four sub-types. Based on the Wojood NER 2024 test set results, the three fine-tuned models performed similarly with AraBERTv2 favored (F1: Flat=.8780 Nested=.9040). The five model approach performed slightly better (F1: Flat=.8782 Nested=.9043).",
        "authors": [
          "Hadi Hamoud",
          "Chadi Abou Chakra",
          "Nancy Hamdan",
          "Osama Rakan Al Mraikhat",
          "Doha Albared",
          "Fadi A. Zaraket"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "874",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "879",
        "paper_id": 104,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.104.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.104.jpg",
        "title": "DRU at WojoodNER 2024: A Multi-level Method Approach",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.104",
        "x": -5.104048728942871,
        "y": 2.514995813369751,
        "year": "2024"
      },
      {
        "abstract": "We present an overview of the FIGNEWSshared task, organized as part of the Arabic-NLP 2024 conference co-located with ACL2024. The shared task addresses bias and pro-paganda annotation in multilingual news posts.We focus on the early days of the Israel War onGaza as a case study. The task aims to fostercollaboration in developing annotation guide-lines for subjective tasks by creating frame-works for analyzing diverse narratives high-lighting potential bias and propaganda. In aspirit of fostering and encouraging diversity,we address the problem from a multilingualperspective, namely within five languages: En-glish, French, Arabic, Hebrew, and Hindi. Atotal of 17 teams participated in two annota-tion subtasks: bias (16 teams) and propaganda(6 teams). The teams competed in four evalua-tion tracks: guidelines development, annotationquality, annotation quantity, and consistency.Collectively, the teams produced 129,800 datapoints. Key findings and implications for thefield are discussed.",
        "authors": [
          "Wajdi Zaghouani",
          "Mustafa Jarrar",
          "Nizar Habash",
          "Houda Bouamor",
          "Imed Zitouni",
          "Mona Diab",
          "Samhaa R. El-Beltagy",
          "Muhammed AbuOdeh"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "530",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "547",
        "paper_id": 56,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.56.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.56.jpg",
        "title": "The FIGNEWS Shared Task on News Media Narratives",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.56",
        "x": -1.103859782218933,
        "y": 6.536249160766602,
        "year": "2024"
      },
      {
        "abstract": "This paper presents our results for the Arabic Financial NLP (AraFinNLP) shared task at the Second Arabic Natural Language Processing Conference (ArabicNLP 2024). We participated in the first sub-task, Multi-dialect Intent Detection, which focused on cross-dialect intent detection in the banking domain. Our approach involved fine-tuning an encoder-only T5 model, generating synthetic data, and model ensembling. Additionally, we conducted an in-depth analysis of the dataset, addressing annotation errors and problematic translations. Our model was ranked third in the shared task, achieving a F1-score of 0.871.",
        "authors": [
          "Murhaf Fares",
          "Samia Touileb"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "433",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "440",
        "paper_id": 40,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.40.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.40.jpg",
        "title": "BabelBot at AraFinNLP2024: Fine-tuning T5 for Multi-dialect Intent Detection with Synthetic Data and Model Ensembling",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.40",
        "x": -3.112177848815918,
        "y": 2.253586769104004,
        "year": "2024"
      },
      {
        "abstract": "Native Language Identification (NLI) is concerned with predicting the native language of an author writing in a second language. We investigate NLI for Arabic, with a focus on the types of linguistic information given that Arabic is morphologically rich. We use the Arabic Learner Corpus (ALC) foro training and testing along with a linear SVM. We explore lexical, morpho-syntactic, and syntactic features. Results show that the best single type of information is character n-grams ranging from 2 to 6. Using this model, we achieve an accuracy of 61.84%, thus outperforming previous results (Ionesco, 2015) by 11.74% even though we use an additional 2 L1s. However, when using prefix and suffix sequences, we reach an accuracy of 53.95%, showing that an approximation of unlexicalized features still reaches solid results.",
        "authors": [
          "Yasmeen Bassas",
          "Sandra K\u00fcbler"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "183",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "192",
        "paper_id": 17,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.17.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.17.jpg",
        "title": "Investigating Linguistic Features for Arabic NLI",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.17",
        "x": -3.6746041774749756,
        "y": 3.9340808391571045,
        "year": "2024"
      },
      {
        "abstract": "This study undertakes a comprehensive investigation of transformer-based models to advance Arabic language processing, focusing on two pivotal aspects: the estimation of Arabic Level of Dialectness and dialectal sentence-level machine translation into Modern Standard Arabic. We conducted various evaluations of different sentence transformers across a proposed regression model, showing that the MARBERT transformer-based proposed regression model achieved the best root mean square error of 0.1403 for Arabic Level of Dialectness estimation. In parallel, we developed bi-directional translation models between Modern Standard Arabic and four specific Arabic dialects\u2014Egyptian, Emirati, Jordanian, and Palestinian\u2014by fine-tuning and evaluating different sequence-to-sequence transformers. This approach significantly improved translation quality, achieving a BLEU score of 0.1713. We also enhanced our evaluation capabilities by integrating MSA predictions from the machine translation model into our Arabic Level of Dialectness estimation framework, forming a comprehensive pipeline that not only demonstrates the effectiveness of our methodologies but also establishes a new benchmark in the deployment of advanced Arabic NLP technologies.",
        "authors": [
          "Omer Nacar",
          "Serry Sibaee",
          "Abdullah Alharbi",
          "Lahouari Ghouti",
          "Anis Koubaa"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "748",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "753",
        "paper_id": 83,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.83.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.83.jpg",
        "title": "ASOS at NADI 2024 shared task: Bridging Dialectness Estimation and MSA Machine Translation for Arabic Language Enhancement",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.83",
        "x": -3.055767297744751,
        "y": 1.4882874488830566,
        "year": "2024"
      },
      {
        "abstract": "We report the approaches submitted to the NADI 2024 Subtask 1: Multi-label country-level Dialect Identification (MLDID). The core part was to adapt the information from multi-class data for a multi-label dialect classification task. We experimented with supervised and unsupervised strategies to tackle the task in this challenging setting. Under the supervised setup, we used the model trained using NADI 2023 data and devised approaches to convert the multi-class predictions to multi-label by using information from the confusion matrix or using calibrated probabilities. Under unsupervised settings, we used the Arabic-based sentence encoders and multilingual cross-encoders to retrieve similar samples from the training set, considering each test input as a query. The associated labels are then assigned to the input query. We also tried different variations, such as co-occurring dialects derived from the provided development set. We obtained the best validation performance of 48.5% F-score using one of the variations with an unsupervised approach and the same approach yielded the best test result of 43.27% (Ranked 2).",
        "authors": [
          "Vani Kanjirangat",
          "Tanja Samardzic",
          "Ljiljana Dolamic",
          "Fabio Rinaldi"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "742",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "747",
        "paper_id": 82,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.82.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.82.jpg",
        "title": "NLP_DI at NADI 2024 shared task: Multi-label Arabic Dialect Classifications with an Unsupervised Cross-Encoder",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.82",
        "x": -2.0514204502105713,
        "y": 0.6113672256469727,
        "year": "2024"
      },
      {
        "abstract": "We study dependency parsing for four Arabic dialects (Gulf, Levantine, Egyptian, and Maghrebi). Since no syntactically annotated data exist for Arabic dialects, we train the parser on a Modern Standard Arabic (MSA) corpus, which creates an out-of-domain setting.We investigate methods to close the gap between the source (MSA) and target data (dialects), e.g., by training on syntactically similar sentences to the test data. For testing, we manually annotate a small data set from a dialectal corpus. We focus on parsing two linguistic phenomena, which are difficult to parse: Idafa and coordination. We find that we can improve results by adding in-domain MSA data while adding dialectal embeddings only results in minor improvements.",
        "authors": [
          "Noor Abo Mokh",
          "Daniel Dakota",
          "Sandra K\u00fcbler"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "170",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "182",
        "paper_id": 16,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.16.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.16.jpg",
        "title": "Out-of-Domain Dependency Parsing for Dialects of Arabic: A Case Study",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.16",
        "x": -2.4441802501678467,
        "y": 1.1971545219421387,
        "year": "2024"
      },
      {
        "abstract": "This paper presents our team\u2019s contribution to the FIGNEWS 2024 Shared Task, which involved annotating bias and propaganda in news coverage of the Israel-Palestine conflict. We developed comprehensive guidelines and employed a rigorous methodology to analyze 2,200 news posts from several official Facebook accounts of news websites in multiple languages. Our team, Narrative Navigators, achieved third place in both the Bias Guidelines and Bias Consistency tracks, demonstrating the effectiveness of our approach. We achieved an IAA Kappa score of 39.4 for bias annotation and 12.8 for propaganda detection. These findings and our performance underscore the need for enhanced media literacy and further research to counter the impact of biased and misleading information on public understanding of the conflict.",
        "authors": [
          "Maryam AlEmadi",
          "Jana ElMesselmani",
          "Lyna Bermak",
          "Goumana Abdullah",
          "Esra\u2019a Sharqawi",
          "Anissa Jrad",
          "Zied Zouabi",
          "Wajdi Zaghouani"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "548",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "554",
        "paper_id": 57,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.57.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.57.jpg",
        "title": "Narrative Navigators at FIGNEWS 2024 Shared Task: New Frontiers in Bias and Propaganda Annotation Techniques",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.57",
        "x": -1.137679100036621,
        "y": 1.8820197582244873,
        "year": "2024"
      },
      {
        "abstract": "This paper describes the approach and results of Bangor University\u2019s participation in the WojoodNER 2024 shared task, specifically for Subtask-1: Closed-Track Flat Fine-Grain NER. We present a system utilizing a transformer-based model called bert-base-arabic-camelbert-mix, fine-tuned on the Wojood-Fine corpus. A key enhancement to our approach involves adding a linear layer on top of the bert-base-arabic-camelbert-mix to classify each token into one of 51 different entity types and subtypes, as well as the \u2018O\u2019 label for non-entity tokens. This linear layer effectively maps the contextualized embeddings produced by BERT to the desired output labels, addressing the complex challenges of fine-grained Arabic NER. The system achieved competitive results in precision, recall, and F1 scores, thereby contributing significant insights into the application of transformers in Arabic NER tasks.",
        "authors": [
          "Norah Alshammari"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "880",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "884",
        "paper_id": 105,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.105.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.105.jpg",
        "title": "Bangor University at WojoodNER 2024: Advancing Arabic Named Entity Recognition with CAMeLBERT-Mix",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.105",
        "x": -3.220496654510498,
        "y": 4.697131156921387,
        "year": "2024"
      },
      {
        "abstract": "Semantic search tasks have grown extremely fast following the advancements in large language models, including the Reverse Dictionary and Word Sense Disambiguation in Arabic. This paper describes our participation in the Contemporary Arabic Dictionary Shared Task. We propose two models that achieved first place in both tasks. We conducted comprehensive experiments on the latest five multilingual sentence transformers and the Arabic BERT model for semantic embedding extraction. We achieved a ranking score of 0.06 for the reverse dictionary task, which is double than last year\u2019s winner. We had an accuracy score of 0.268 for the Word Sense Disambiguation task.",
        "authors": [
          "Serry Sibaee",
          "Abdullah Alharbi",
          "Samar Ahmad",
          "Omer Nacar",
          "Anis Koubaa",
          "Lahouari Ghouti"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "697",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "703",
        "paper_id": 77,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.77.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.77.jpg",
        "title": "ASOS at KSAA-CAD 2024: One Embedding is All You Need for Your Dictionary",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.77",
        "x": -1.725156545639038,
        "y": 2.7996904850006104,
        "year": "2024"
      },
      {
        "abstract": "Training LLMs in low resources languages usually utilizes machine translation (MT) data augmentation from English language. However, translation brings a number of challenges: there are large costs attached to translating and curating huge amounts of content with high-end machine translation solutions; the translated content carries over cultural biases; and if the translation is not faithful and accurate, the quality of the data degrades causing issues in the trained model. In this work, we investigate the role of translation and synthetic data in training language models. We translate TinyStories, a dataset of 2.2M short stories for 3-4 year old children, from English to Arabic using the open NLLB-3B MT model. We train a number of story generation models of size 1M-33M parameters using this data. We identify a number of quality and task-specific issues in the resulting models. To rectify these issues, we further pre-train the models with a small dataset of synthesized high-quality stories generated by a capable LLM in Arabic, representing 1% of the original training data. We show, using GPT-4 as a judge and dictionary learning analysis from mechanistic interpretability, that the suggested approach is a practical means to resolve some of the translation pitfalls. We illustrate the improvement through case studies of linguistic and cultural bias issues.",
        "authors": [
          "Sabri Boughorbel",
          "Md. Rizwan Parvez",
          "Majd Hawasly"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "73",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "88",
        "paper_id": 7,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.7.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.7.jpg",
        "title": "Improving Language Models Trained on Translated Data with Continual Pre-Training and Dictionary Learning Analysis",
        "type": "main",
        "url": "https://aclanthology.org/2024.arabicnlp-1.7",
        "x": -3.2763614654541016,
        "y": 4.874003887176514,
        "year": "2024"
      },
      {
        "abstract": "In the financial industry, identifying user intent from text inputs is crucial for various tasks such as automated trading, sentiment analysis, and customer support. One important component of natural language processing (NLP) is intent detection, which is significant to the finance sector. Limited studies have been conducted in the field of finance using languages with limited resources like Arabic, despite notable works being done in high-resource languages like English. To advance Arabic NLP in the financial domain, the organizer of AraFinNLP 2024 has arranged a shared task for detecting banking intents from the queries in various Arabic dialects, introducing a novel dataset named ArBanking77 which includes a collection of banking queries categorized into 77 distinct intents classes. To accomplish this task, we have presented a hierarchical approach called Dual-Phase-BERT in which the detection of dialects is carried out first, followed by the detection of banking intents. Using the provided ArBanking77 dataset, we have trained and evaluated several conventional machine learning, and deep learning models along with some cutting-edge transformer-based models. Among these models, our proposed Dual-Phase-BERT model has ranked7thout of all competitors, scoring 0.801 on the scale of F1-score on the test set.",
        "authors": [
          "Md. Sajid Alam Chowdhury",
          "Mostak Mahmud Chowdhury",
          "Anik Mahmud Shanto",
          "Hasan Murad",
          "Udoy Das"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "410",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "414",
        "paper_id": 36,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.36.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.36.jpg",
        "title": "Fired_from_NLP at AraFinNLP 2024: Dual-Phase-BERT - A Fine-Tuned Transformer-Based Model for Multi-Dialect Intent Detection in The Financial Domain for The Arabic Language",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.36",
        "x": -1.0714353322982788,
        "y": 1.9126332998275757,
        "year": "2024"
      },
      {
        "abstract": "In this paper, we present our methodology and findings from participating in the FIGNEWS 2024 shared task on annotating news fragments on the Gaza-Israel war for bias and propaganda detection. The task aimed to refine the FIGNEWS 2024 annotation guidelines and to contribute to the creation of a comprehensive dataset to advance research in this field. Our team employed a multi-faceted approach to ensure high accuracy in data annotations. Our results highlight key challenges in detecting bias and propaganda, such as the need for more comprehensive guidelines. Our team ranked first in all tracks for propaganda annotation. For Bias, the team stood in first place for the Guidelines and IAA tracks, and in second place for the Quantity and Consistency tracks.",
        "authors": [
          "Sadaf Abdul-Rauf",
          "Huda Sarfraz",
          "Saadia Nauman",
          "Arooj Fatima",
          "SadafZiafat SadafZiafat",
          "Momina Ishfaq",
          "Alishba Suboor",
          "Hammad Afzal",
          "Seemab Latif"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of the Second Arabic Natural Language Processing Conference",
        "first_page": "573",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "579",
        "paper_id": 61,
        "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.61.pdf",
        "publication_date": "2024/8",
        "thumbnail": "https://aclanthology.org/thumb/2024.arabicnlp-1.61.jpg",
        "title": "NLPColab at FigNews 2024 Shared Task: Challenges in Bias and Propaganda Annotation for News Media",
        "type": "shared_task",
        "url": "https://aclanthology.org/2024.arabicnlp-1.61",
        "x": -4.589537143707275,
        "y": 0.3537243604660034,
        "year": "2024"
      },
      {
        "abstract": "Arabic is one of the most widely spoken languages in the world, yet efforts to develop and evaluate Large Language Models (LLMs) for Arabic remain relatively limited. Most existing Arabic benchmarks focus on linguistic, cultural, or religious content, leaving a significant gap in areas like STEM and coding domains that are increasingly relevant for real-world LLM applications. To help bridge this gap, we present3LM, a suite ofthreebenchmarks designed specifically for Arabic. The first is a set of STEM-related question-answer pairs, naturally sourced from Arabic textbooks and educational worksheets. The second consists of synthetically generated STEM questions, created using the same sources. The third benchmark focuses on code generation, built through a careful translation of two widely used code benchmarks, incorporating a human-in-the-loop process with several rounds of review to ensure high-quality and faithful translations. We release all three benchmarks publicly to support the growth of Arabic LLM research in these essential but underrepresented areas.",
        "authors": [
          "Basma El Amel Boussaha",
          "Leen Al Qadi",
          "Mugariya Farooq",
          "Shaikha Alsuwaidi",
          "Giulia Campesan",
          "Ahmed Alzubaidi",
          "Mohammed Alyafeai",
          "Hakim Hacid"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "42",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "63",
        "paper_id": 4,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.4.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.4.jpg",
        "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.4",
        "x": -2.6108508110046387,
        "y": 0.09435053169727325,
        "year": "2025"
      },
      {
        "abstract": "Video-to-text and text-to-video retrieval are dominated by English benchmarks (e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet Arabic remains underserved, lacking localized evaluation metrics. We introduce a three-stage framework, AutoArabic, utilizing state-of-the-art large language models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic, reducing the manual revision required by nearly fourfold. The framework incorporates an error detection module that automatically flags potential translation errors with 97% accuracy. Applying the framework to DiDeMo, a video retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent Arabic descriptions. An analysis of the translation errors is provided and organized into an insightful taxonomy to guide future Arabic localization efforts. We train a CLIP-style baseline with identical hyperparameters on the Arabic and English variants of the benchmark, finding a moderate performance gap (\ud835\udee5 \u2248 3pp at Recall@1), indicating that Arabic localization preserves benchmark difficulty. We evaluate three post-editing budgets (zero/ flagged-only/ full) and find that performance improves monotonically with more post-editing, while the raw LLM output (zero-budget) remains usable. To ensure reproducibility to other languages, we made the code available at https://github.com/Tahaalshatiri/AutoArabic.",
        "authors": [
          "Mohamed Eltahir",
          "Osamah Sarraj",
          "Abdulrahman M. Alfrihidi",
          "Taha Alshatiri",
          "Mohammed Khurd",
          "Mohammed Bremoo",
          "Tanveer Hussain"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "288",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "297",
        "paper_id": 23,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.23.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.23.jpg",
        "title": "AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.23",
        "x": -1.0498970746994019,
        "y": 1.8505380153656006,
        "year": "2025"
      },
      {
        "abstract": "We present Octopus, a first family of modular speech-language models designed for Arabic-English ASR, dialect identification, and speech translation. Built on Whisper-V3 and enhanced with large language models like ALLaM, LLaMA, and DeepSeek, Octopus bridges speech and text through a lightweight projection layer and Q-Former. To broaden its scope beyond speech, Octopus integrates BEATs, a general-purpose audio encoder allowing it to understand both linguistic and acoustic events. Despite its simplicity, this dual-encoder design supports robust performance across multilingual and code-switched scenarios. We also introduce TinyOctopus, a distilled variant using smaller models (Distil-Whisper + LLaMA3-1B / DeepSeek-1.5B), achieving competitive results with just a fraction of the parameters. Fine-tuning on synthetic code-switched data further boosts its performance. Octopus demonstrates the power of compact, extensible architectures in Arabic-centric speech modeling and sets the stage for unified multilingual audio-language understanding.",
        "authors": [
          "Sara Althubaiti",
          "Vasista Sai Lodagala",
          "Tjad Clark",
          "Yousseif Ahmed Elshahawy",
          "Daniel Izham",
          "Abdullah Alrajeh",
          "Aljawahrah Bin Tamran",
          "Ahmed Ali"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "425",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "435",
        "paper_id": 35,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.35.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.35.jpg",
        "title": "Octopus: Towards Building the Arabic Speech LLM Suite",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.35",
        "x": -5.712965488433838,
        "y": 3.262995719909668,
        "year": "2025"
      },
      {
        "abstract": "Automated Essay Scoring (AES) has shown significant advancements in educational assessment. However, under-resourced languages like Arabic have received limited attention. To bridge this gap and enable robust Arabic AES, this paper introduces thefirst publicly-availablecomprehensive set of engineered features tailored for Arabic AES, covering surface-level, readability, lexical, syntactic, and semantic features. Experiments are conducted on a dataset of 620 Arabic essays, each annotated with both holistic and trait-specific scores. Our findings demonstrate that the proposed feature set is effective across different models and competitive with recent NLP advances including LLMs, establishing the state-of-the-art performance and providing strong baselines for future Arabic AES research. Moroever, the resulting feature set offers a reusable and foundational resource, contributing towards the development of more effective Arabic AES systems.",
        "authors": [
          "Marwan Sayed",
          "Sohaila Eltanbouly",
          "May Bashendy",
          "Tamer Elsayed"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "231",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "245",
        "paper_id": 19,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.19.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.19.jpg",
        "title": "Feature Engineering is not Dead: A Step Towards State of the Art for Arabic Automated Essay Scoring",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.19",
        "x": -1.7531064748764038,
        "y": 0.5025696754455566,
        "year": "2025"
      },
      {
        "abstract": "Classical Arabic represents a significant era that encompasses the golden age of Arab culture, philosophy, and scientific literature. With a broad consensus on the importance of translating these literatures to enrich knowledge dissemination across communities, the advent of large language models (LLMs) and translation systems offers promising tools to facilitate this goal. However, we have identified a scarcity of translation datasets in Classical Arabic, which are often limited in scope and topics, hindering the development of high-quality translation systems. In response, we present the ATHAR dataset, which comprises 66,000 high-quality classical Arabic to English translation samples that cover a wide array of topics including science, culture, and philosophy. Furthermore, we assess the performance of current state-of-the-art LLMs under various settings, concluding that there is a need for such datasets in current systems. Our findings highlight how models can benefit from fine-tuning or incorporating this dataset into their pretraining pipelines. The dataset is publicly available on the HuggingFace Data Hub. To preserve anonymity during review, we additionally provide an anonymized snapshot at https://drive.google.com/drive/folders/1c_ElsblaOJzQ0TW_M1DugjR2o3Xv9RUo.",
        "authors": [
          "Mohammed Sabry Mohammed",
          "Mohammed Khalil"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "97",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "106",
        "paper_id": 8,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.8.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.8.jpg",
        "title": "ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.8",
        "x": -3.893216609954834,
        "y": 4.11832857131958,
        "year": "2025"
      },
      {
        "abstract": "Content-grounded dialogue evaluation for Arabic remains under-resourced, particularly across Modern Standard (MSA), Egyptian, and Maghrebi varieties. We introduce Shawarma Chats, a benchmark of 30,000 six-turn conversations grounded in Wikipedia content, evenly split across the three dialects. To build this corpus, we prompt five frontier LLMs GPT-4o, Gemini 2.5 Flash, Qwen-Plus, DeepSeek-Chat, and Mistral Large to generate 1,500 seed dialogues. Native Arabic speakers evaluate these outputs to select the most effective generator and most human-aligned grader. Sub-A dialogues undergo a two-pass, rationale-driven self-repair loop where the grader critiques and the generator revises; unresolved cases are manually corrected. We apply this pipeline to 10,000 Wikipedia paragraphs to create 30,000 high-quality conversations 10,000 per dialect\u2014at modest human cost. To validate the benchmark, we LoRA-fine-tune six open LLMs (1\u201324 B parameters) on Shawarma Chats and observe consistent gains in automatic-grader scores, BERTScore, BLEU and ROUGE particularly for models larger than 7 B parameters. Shawarma Chats thus establishes the first large-scale, dialect-aware, content-grounded dialogue benchmark for Arabic.",
        "authors": [
          "Kamyar Zeinalipour",
          "Mohamed Zaky Saad",
          "Oumaima Attafi",
          "Marco Maggini",
          "Marco Gori"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "472",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "524",
        "paper_id": 39,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.39.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.39.jpg",
        "title": "Shawarma Chats: A Benchmark Exact Dialogue & Evaluation Platter in Egyptian, Maghrebi & Modern Standard Arabic\u2014A Triple-Dialect Feast for Hungry Language Models",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.39",
        "x": -0.934747576713562,
        "y": 6.610941410064697,
        "year": "2025"
      },
      {
        "abstract": "This paper presents a methodology for inserting phrases in Arabic poems to conform to a specific rhythm using ByT5, a byte-level multilingual transformer-based model. Our work discusses a rule-based grapheme-to-beat transformation tailored for extracting the rhythm from fully diacritized Arabic script. Our approach employs a conditional denoising objective to fine-tune ByT5, where the model reconstructs masked words to match a target rhythm. We adopt a curriculum learning strategy, pre-training on a general Arabic dataset before fine-tuning on poetic dataset, and explore cross-lingual transfer from English to Arabic. Experimental results demonstrate that our models achieve high rhythmic alignment while maintaining semantic coherence. The proposed model has the potential to be used in co-creative applications in the process of composing classical Arabic poems.",
        "authors": [
          "Mohamad Elzohbi",
          "Richard Zhao"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "194",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "202",
        "paper_id": 15,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.15.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.15.jpg",
        "title": "Tahdib: A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.15",
        "x": -4.779072284698486,
        "y": -0.16557689011096954,
        "year": "2025"
      },
      {
        "abstract": "Information Extraction tasks such as Named Entity Recognition and Relation Extraction are often developed using diverse tagsets and annotation guidelines. This presents major challenges for model generalization, cross-dataset evaluation, tool interoperability, and broader industry adoption. To address these issues, we propose an information extraction ontology, , which covers a wide range of named entity types and relations. serves as a semantic mediation framework that facilitates alignment across heterogeneous tagsets and annotation guidelines. We propose two ontology-based mapping methods: (i) as a set of mapping rules for uni-directional tagset alignment; and (ii) as ontology-based prompting, which incorporates the ontology concepts directly into prompts, enabling large language models (LLMs) to perform more effective and bi-directional mappings. Our experiments show a 15% improvement in out-of-domain mapping accuracy when using ontology-based prompting compared to rule-based methods. Furthermore, is aligned with Schema.org and Wikidata, enabling interoperability with knowledge graphs and facilitating broader industry adoption. The is open source and available athttps://sina.birzeit.edu/wojood.",
        "authors": [
          "Alaa Aljabari",
          "Nagham Hamad",
          "Mohammed Khalilia",
          "Mustafa Jarrar"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "179",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "193",
        "paper_id": 14,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.14.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.14.jpg",
        "title": "WojoodOntology: Ontology-Driven LLM Prompting for Unified Information Extraction Tasks",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.14",
        "x": -0.924911618232727,
        "y": 6.76923942565918,
        "year": "2025"
      },
      {
        "abstract": "Grapheme-to-phoneme (G2P) models are essential components in text-to-speech (TTS) and pronunciation assessment applications. While standard forms of languages have gained attention in that regard, dialectal speech, which often serves as the primary means of spoken communication for many communities, as it is the case for Arabic, has not received the same level of focus. In this paper, we introduce an end-to-end dialectal G2P for Egyptian Arabic, a dialect without standard orthography. Our novel architecture accomplishes three tasks: (i) restores short vowels of the diacritical marks for the dialectal text; (ii) maps certain characters that happen only in the spoken version of the dialectal Arabic to their dialect-specific character transcriptions; and finally (iii) converts the previous step output to the corresponding phoneme sequence. We benchmark G2P on a modular cascaded system, a large language model, and our multi-task end-to-end architecture.",
        "authors": [
          "Majd Hawasly",
          "Hamdy Mubarak",
          "Ahmed Abdelali",
          "Ahmed Ali"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "466",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "471",
        "paper_id": 38,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.38.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.38.jpg",
        "title": "DialG2P: Dialectal Grapheme-to-Phoneme. Arabic as a Case Study",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.38",
        "x": -1.1640558242797852,
        "y": 1.8848512172698975,
        "year": "2025"
      },
      {
        "abstract": "We present an end-to-end, self-evolving adversarial workflow for long-context Question-Answer (QA) Generation in Arabic. By orchestrating multiple specialized LVLMs: a question generator, an evaluator, and a swarm of answer generators, our system iteratively refines its own performance without any human intervention. Starting from raw, multi-page Arabic documents across diverse domains, the question generator produces fine-grained, context-aware queries to be tackled by the answer generator swarm, and the evaluator assesses and feeds back quality metrics. This closed-loop cycle enables continuous learning: low-confidence outputs trigger automated re-generation and model updates, progressively enhancing question difficulty and relevance. Moreover, we set the quality metrics as a tunable hyperparameter, enabling question generation at controllable and customizable difficulty levels. We releaseAraLongBench, a large-scale Arabic benchmark of single- and multi-page challenges spanning hundreds of pages, and demonstrate that our self-evolving workflow substantially outperform static pipelines, markedly boosting the long-context comprehension capabilities of leading Arabic Large Vision Language Models (LVLMs). Lastly, we also meticulously architect a fully automated agentic workflow for long-context Arabic document collection.",
        "authors": [
          "Kesen Wang",
          "Daulet Toibazar",
          "Pedro J Moreno Mengibar"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "107",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "116",
        "paper_id": 9,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.9.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.9.jpg",
        "title": "A-SEA3\ud835\udc0b-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.9",
        "x": -3.0144309997558594,
        "y": 1.214660882949829,
        "year": "2025"
      },
      {
        "abstract": "We present the first publicly available, multidimensional corpus of Qatari Arabic that captures intra-dialectal variation across Urban and Bedouin speakers. While often grouped under the label of \u201cGulf Arabic\u201d, Qatari Arabic exhibits rich phonological, lexical, and discourse-level differences shaped by gender, age, and sociocultural identity. Our dataset includes aligned speech and transcriptions from 255 speakers, stratified by gender and age, and collected through structured interviews on culturally salient topics such as education, heritage, and social norms. The corpus reveals systematic variation in pronunciation, vocabulary, and narrative style, offering insights for both sociolinguistic analysis and computational modeling. We also demonstrate its utility through preliminary experiments in the prediction of dialects and genders. This work provides the first large-scale, demographically balanced corpus of Qatari Arabic, laying a foundation for both sociolinguistic research and the development of dialect-aware NLP systems.",
        "authors": [
          "Houda Bouamor",
          "Sara Al-Emadi",
          "Zeinab Ibrahim",
          "Hany Fazzaa",
          "Aisha Al-Sultan"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "219",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "230",
        "paper_id": 18,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.18.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.18.jpg",
        "title": "Capturing Intra-Dialectal Variation in Qatari Arabic: A Corpus of Cultural and Gender Dimensions",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.18",
        "x": -3.021650552749634,
        "y": 2.3257718086242676,
        "year": "2025"
      },
      {
        "abstract": "While Knowledge Editing (KE) has been widely explored in English, its behavior in morphologically rich languages like Arabic remains underexamined. In this work, we present the first study of Arabic KE. We evaluate four methods (ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact benchmarks, analyzing both multilingual and cross-lingual settings. Our experiments on Llama-2-7B-chat show show that parameter-based methods struggle with cross-lingual generalization, while instruction-tuned methods perform more robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show that joint Arabic-English training improves both editability and transfer. We release Arabic KE benchmarks and multilingual training for LTE data to support future research.",
        "authors": [
          "Basel Mousi",
          "Nadir Durrani",
          "Fahim Dalvi"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "417",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "424",
        "paper_id": 34,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.34.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.34.jpg",
        "title": "An Exploration of Knowledge Editing for Arabic",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.34",
        "x": -3.7012104988098145,
        "y": 4.204748630523682,
        "year": "2025"
      },
      {
        "abstract": "We introduce TuniFra, a novel and comprehensive corpus developed to advance research in Automatic Speech Recognition (ASR) and Speech-to-Text Translation (STT) for Tunisian Arabic, a notably low-resourced language variety. The TuniFra corpus comprises 15 hours of native Tunisian Arabic speech, carefully transcribed and manually translated into French. While the development of ASR and STT systems for major languages is supported by extensive datasets, low-resource languages such as Tunisian Arabic face significant challenges due to limited training data, particularly for speech technologies. TuniFra addresses this gap by offering a valuable resource tailored for both ASR and STT tasks in the Tunisian dialect. We describe our methodology for data collection, transcription, and annotation, and present initial baseline results for both Tunisian Arabic speech recognition and Tunisian Arabic\u2013French speech translation.",
        "authors": [
          "Alex Choux",
          "Marko Avila",
          "Josep M. Crego",
          "Fethi Bougares",
          "Antoine Laurent"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "64",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "68",
        "paper_id": 5,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.5.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.5.jpg",
        "title": "TuniFra: A Tunisian Arabic Speech Corpus with Orthographic Transcriptions and French Translations",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.5",
        "x": -3.3560750484466553,
        "y": 2.8651134967803955,
        "year": "2025"
      },
      {
        "abstract": "In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The. collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research studying Tunisian Dialect.",
        "authors": [
          "Fethi Bougares",
          "Salima Mdhaffar",
          "Haroun Elleuch",
          "Yannick Est\u00e8ve"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "278",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "287",
        "paper_id": 22,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.22.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.22.jpg",
        "title": "TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.22",
        "x": -4.2119221687316895,
        "y": 0.3306499421596527,
        "year": "2025"
      },
      {
        "abstract": "Despite the growing importance of online discourse, Arabic-speaking communities lack platforms that support structured, culturally grounded debate. Mainstream social media rarely fosters constructive engagement, often leading to polarization and superficial exchanges. This paper proposes the development of a culturally aware debate platform tailored to the values and traditions of Arabic-speaking users, with a focus on leveraging advances in natural language processing (NLP). We present findings from a user survey that explores experiences with existing debate tools and expectations for future platforms. Besides, we analyze 30,000 English-language debate topics using large language models (LLMs) to assess their cultural relevance and appropriateness for Arab audiences. We further examine the ability of LLMs to generate new culturally resonant debate topics, contributing to the emerging tasks of culture-aware topic assessment and generation. Finally, we propose a theoretical and technical framework for building an NLP-supported Arabic debate platform. Our work highlights the urgent need for culturally sensitive NLP resources that foster critical thinking, digital literacy, and meaningful deliberation in Arabic.",
        "authors": [
          "Khalid Al Khatib",
          "Mohammad Khader"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "359",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "374",
        "paper_id": 29,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.29.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.29.jpg",
        "title": "Toward Culturally-Aware Arabic Debate Platforms with NLP Support",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.29",
        "x": -3.1824679374694824,
        "y": 4.186919689178467,
        "year": "2025"
      },
      {
        "abstract": "Prompt relevance is a critical yet underexplored dimension in Arabic Automated Essay Scoring (AES). We present the first systematic study of binary prompt-essay relevance classification, supporting both AES scoring and dataset annotation. To address data scarcity, we built a synthetic dataset of on-topic and off-topic pairs and evaluated multiple models, including threshold-based classifiers, SVMs, causal LLMs, and a fine-tuned masked SBERT model. For real-data evaluation, we combined QAES with ZAEBUC, creating off-topic pairs via mismatched prompts. We also tested prompt expansion strategies using AraVec, CAMeL, and GPT-4o. Our fine-tuned SBERT achieved 98% F1 on synthetic data and strong results on QAES+ZAEBUC, outperforming SVMs and threshold-based baselines and offering a resource-efficient alternative to LLMs. This work establishes the first benchmark for Arabic prompt relevance and provides practical strategies for low-resource AES.",
        "authors": [
          "Chatrine Qwaider",
          "Kirill Chirkunov",
          "Bashar Alhafni",
          "Nizar Habash",
          "Ted Briscoe"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "162",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "178",
        "paper_id": 13,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.13.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.13.jpg",
        "title": "Evaluating Prompt Relevance in Arabic Automatic Essay Scoring: Insights from Synthetic and Real-World Data",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.13",
        "x": -3.3397340774536133,
        "y": 4.608194828033447,
        "year": "2025"
      },
      {
        "abstract": "ArabJobs is a publicly available corpus of Arabic job advertisements collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates. Comprising over 8,500 postings and more than 550,000 words, the dataset captures linguistic, regional, and socio-economic variation in the Arab labour market. We present analyses of gender representation and occupational structure, and highlight dialectal variation across ads, which offers opportunities for future research. We also demonstrate applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification. The findings show the utility of ArabJobs for fairness-aware Arabic NLP and labour market research. The dataset is publicly available on GitHub: https://github.com/drelhaj/ArabJobs.",
        "authors": [
          "Mo El-Haj"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "16",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "25",
        "paper_id": 2,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.2.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.2.jpg",
        "title": "ArabJobs: A Multinational Corpus of Arabic Job Ads",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.2",
        "x": -3.6138668060302734,
        "y": 4.033102512359619,
        "year": "2025"
      },
      {
        "abstract": "We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for Egyptian dialect, uniquely designed to understand and generate texts written in both Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we introduce a novel language adaptation approach by leveraging the Branch-Train-MiX strategy to merge script-specialized experts, into a single MoE model. Our Nile-Chat models significantly outperform leading multilingual and Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced Egyptian evaluation benchmarks, which span both understanding and generative tasks. Notably, our 12B model delivers a 14.4% performance gain over Qwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly available. We believe this work presents a comprehensive methodology for adapting LLMs to a single language with dual-script usage, addressing an often overlooked aspect in contemporary LLM development.",
        "authors": [
          "Guokan Shang",
          "Hadi Abdine",
          "Ahmad Chamma",
          "Amr Mohamed",
          "Mohamed Anwar",
          "Abdelaziz Bounhar",
          "Omar El Herraoui",
          "Preslav Nakov",
          "Michalis Vazirgiannis",
          "Eric Xing"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "306",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "322",
        "paper_id": 25,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.25.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.25.jpg",
        "title": "Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.25",
        "x": -0.8182303309440613,
        "y": 6.635997295379639,
        "year": "2025"
      },
      {
        "abstract": "Argument mining for Arabic remains underexplored, largely due to the scarcity of annotated corpora. To address this gap, we examine the effectiveness of cross-lingual transfer from English. Using the English Persuasive Essays (PE) corpus, annotated with argumentative components (Major Claim, Claim, and Premise), we explore several transfer strategies: training encoder-based multilingual and monolingual models on English data, machine-translated Arabic data, and their combination. We further assess the impact of annotation noise introduced during translation by manually correcting portions of the projected training data. In addition, we investigate the potential of prompting large language models (LLMs) for the task. Experiments on a manually corrected Arabic test set show that monolingual models trained on translated data achieve the strongest performance, with further improvements from small-scale manual correction of training examples.",
        "authors": [
          "Sara Nabhani",
          "Khalid Al Khatib"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "407",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "416",
        "paper_id": 33,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.33.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.33.jpg",
        "title": "Transfer or Translate? Argument Mining in Arabic with No Native Annotations",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.33",
        "x": -4.743285655975342,
        "y": 0.6524969935417175,
        "year": "2025"
      },
      {
        "abstract": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the reasoning capabilities of large language models (LLMs) within the Arabic legal domain. While existing Arabic benchmarks cover some knowledge-intensive tasks such as retrieval and understanding, substantial datasets focusing specifically on multistep reasoning for Arabic LLMs, especially in open-ended contexts, are lacking. The dataset comprises over 13K commercial court cases from Saudi Arabia, with each case including the facts presented, the reasoning of the court, the verdict, as well the cited clauses extracted from the regulatory documents. We define a set of challenging tasks leveraging this dataset and reflecting the complexity of real-world legal reasoning, including verdict prediction, completion of reasoning chains in multistep legal arguments, and identification of relevant regulations based on case facts. We benchmark a representative selection of current open and closed Arabic LLMs on these tasks and demonstrate the dataset\u2019s utility for instruction tuning. Notably, we show that instruction tuning a modest 12B parameter model using ALARB significantly enhances its performance in verdict prediction and Arabic verdict generation, reaching a level comparable to that of GPT-4o.",
        "authors": [
          "Harethah Abu Shairah",
          "Somayah S. Alharbi",
          "Abdulaziz A. AlHussein",
          "Sameer Alsabea",
          "Omar Shaqaqi",
          "Hebah A. Alshamlan",
          "Omar Knio",
          "George Turkiyyah"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "389",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "406",
        "paper_id": 32,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.32.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.32.jpg",
        "title": "ALARB: An Arabic Legal Argument Reasoning Benchmark",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.32",
        "x": -3.620957612991333,
        "y": 0.04440984129905701,
        "year": "2025"
      },
      {
        "abstract": "The morphological structure of Semitic languages, such as Arabic, is based on non-concatenative roots and templates. This complex word structure used by humans is obscured to neural models that employ traditional tokenization algorithms, such as byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994). In this work, we present and evaluate Semitic Root Encoding (SRE), a tokenization method that represents both concatenative and non-concatenative structures in Semitic words with sequences of root, template stem, and BPE tokens. We apply the method to neural machine translation (NMT) and find that SRE tokenization yields an average increase of 1.15 BLEU over the baseline. SRE tokenization is also robust against generating combinations of roots with template stems that do not occur in nature. Finally, we compare the performance of SRE to tokenization based on non-linguistic root and template structures and tokenization based on stems, providing evidence that NMT models are capable of leveraging tokens based on non-concatenative Semitic morphology.",
        "authors": [
          "Brendan T. Hatch",
          "Stephen D. Richardson"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "26",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "41",
        "paper_id": 3,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.3.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.3.jpg",
        "title": "Semitic Root Encoding: Tokenization Based on the Templatic Morphology of Semitic Languages in NMT",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.3",
        "x": -1.9972445964813232,
        "y": 1.9282771348953247,
        "year": "2025"
      },
      {
        "abstract": "Arabic presents unique challenges for sense level language understanding due to its rich morphology and semantic ambiguity. This paper benchmarks large generative language models (LLMs) for Arabic Word Sense Disambiguation (WSD) under both zero-shot and fine-tuning conditions. We evaluate one proprietary model (GPT-4o) and three opensource models (LLaMA 3.1-8B, Qwen 2.5-7B, and Gemma 2-9B) on two publicly available datasets. In zero-shot settings, GPT-4o achieved the highest overall performance, with comparable results across both datasets, reaching 79% accuracy and an average macro-F1 score of 66.08%. Fine-tuning, however, notably elevated all open models beyond GPT4o\u2019s zero-shot results. Qwen achieved the top scores on one dataset, with an accuracy of 90.77% and a macro-F1 score of 83.98%, while LLaMA scored highest on the other, reaching an accuracy of 88.51% and a macroF1 score of 69.41%. These findings demonstrate that parameter-efficient supervised adaptation can close much of the performance gap and establish strong, reproducible baselines for Arabic WSD using open-source, relatively medium-sized models. Full code is publicly available.",
        "authors": [
          "Yossra Noureldien",
          "Abdelrazig Mohamed",
          "Farah Attallah"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "298",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "305",
        "paper_id": 24,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.24.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.24.jpg",
        "title": "Zero-Shot and Fine-Tuned Evaluation of Generative LLMs for Arabic Word Sense Disambiguation",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.24",
        "x": -0.9621882438659668,
        "y": 6.643077850341797,
        "year": "2025"
      },
      {
        "abstract": "Recently, extensive research on the hallucination of the large language models (LLMs) has mainly focused on the English language. Despite the growing number of multilingual and Arabic-specific LLMs, evaluating LLMs\u2019 hallucination in the Arabic context remains relatively underexplored. The knowledge gap is particularly pressing given Arabic\u2019s widespread use across many regions and its importance in global communication and media. This paper presents the first comprehensive hallucination evaluation of Arabic and multilingual LLMs on two critical Arabic natural language generation tasks: generative question answering (GQA) and summarization. This study evaluates a total of 12 LLMs, including 4 Arabic pre-trained models, 4 multilingual models, and 4 reasoning-based models. To assess the factual consistency and faithfulness of LLMs\u2019 outputs, we developed a fine-grained hallucination evaluation framework consisting of 12 fine-grained hallucination indicators that represent the varying characteristics of each task. The results reveal that factual hallucinations are more prevalent than faithfulness errors across all models and tasks. Notably, the Arabic pre-trained model Allam consistently demonstrates lower hallucination rates than multilingual models and a comparative performance with reasoning-based models. The code is available at: https://github.com/aishaalansari57/AraHalluEval",
        "authors": [
          "Aisha Alansari",
          "Hamzah Luqman"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "148",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "161",
        "paper_id": 12,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.12.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.12.jpg",
        "title": "AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.12",
        "x": -3.7413711547851562,
        "y": 4.990150451660156,
        "year": "2025"
      },
      {
        "abstract": "Tool calling is a critical capability that allows Large Language Models (LLMs) to interact with external systems, significantly expanding their utility. However, research and resources for tool calling are predominantly English-centric, leaving a gap in our understanding of how to enable this functionality for other languages, such as Arabic. This paper investigates three key research questions: (1) the necessity of in-language (Arabic) tool-calling data versus relying on cross-lingual transfer, (2) the effect of general-purpose instruction tuning on tool-calling performance, and (3) the value of fine-tuning on specific, high-priority tools. To address these questions, we conduct extensive experiments using base and post-trained variants of an open-weight Arabic LLM. To enable this study, we bridge the resource gap by translating and adapting two open-source tool-calling datasets into Arabic. Our findings provide crucial insights into the optimal strategies for developing robust tool-augmented agents for Arabic.",
        "authors": [
          "As\u0131m Ersoy",
          "Enes Altinisik",
          "Kareem Mohamed Darwish",
          "Husrev Taha Sencar"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "347",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "358",
        "paper_id": 28,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.28.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.28.jpg",
        "title": "Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.28",
        "x": -3.310453176498413,
        "y": 4.827523231506348,
        "year": "2025"
      },
      {
        "abstract": "For effective use in specific countries, Large Language Models (LLMs) need a strong grasp of local culture and core knowledge to ensure socially appropriate, context-aware, and factually correct responses. Existing Arabic and Saudi benchmarks are limited, focusing mainly on dialects or lifestyle, with little attention to deeper cultural or domain-specific alignment from authoritative sources. To address this gap and the challenge LLMs face with non-Western cultural nuance, this study introduces the Saudi-Alignment Benchmark. It consists of 874 manually curated questions across two core cultural dimensions: Saudi Cultural and Ethical Norms, and Saudi Domain Knowledge. These questions span multiple subcategories and use three formats to assess different goals with verified sources. Our evaluation reveals significant variance in LLM alignment. GPT-4 achieved the highest overall accuracy (83.3%), followed by ALLaM-7B (81.8%) and Llama-3.3-70B (81.6%), whereas Jais-30B exhibited a pronounced shortfall at 21.9%. Furthermore, multilingual LLMs excelled in norms; ALLaM-7B in domain knowledge. Considering the effect of question format, LLMs generally excelled in selected-response formats but showed weaker results on generative tasks, indicating that recognition-based benchmarks alone may overestimate cultural and contextual alignment. These findings highlight the need for tailored benchmarks and reveal LLMs\u2019 limitations in achieving cultural grounding, particularly in underrepresented contexts like Saudi Arabia.",
        "authors": [
          "Manal Alhassoun",
          "Imaan Mohammed Alkhanen",
          "Nouf Alshalawi",
          "Ibtehal Baazeem",
          "Waleed Alsanie"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "130",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "147",
        "paper_id": 11,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.11.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.11.jpg",
        "title": "Saudi-Alignment Benchmark: Assessing LLMs Alignment with Cultural Norms and Domain Knowledge in the Saudi Context",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.11",
        "x": -0.751255452632904,
        "y": 6.6823272705078125,
        "year": "2025"
      },
      {
        "abstract": "We address the task of reverse dictionary modeling in Arabic, where the goal is to retrieve a target word given its definition. The task comprises two subtasks: (1) generating embeddings for Arabic words based on Arabic glosses, and (2) a cross-lingual setting where the gloss is in English and the target embedding is for the corresponding Arabic word. Prior approaches have largely relied on BERT models such as CAMeLBERT or MARBERT trained with mean squared error loss. In contrast, we propose a novel ensemble architecture that combines MARBERTv2 with the encoder of AraBART, and we demonstrate that the choice of loss function has a significant impact on performance. We apply contrastive loss to improve representational alignment, and introduce structural and center losses to better capture the semantic distribution of the dataset. This multi-loss framework enhances the quality of the learned embeddings and leads to consistent improvements in both monolingual and cross-lingual settings. Our system achieved the best rank metric in both subtasks compared to the previous approaches. These results highlight the effectiveness of combining architectural diversity with task-specific loss functions in representational tasks for morphologically rich languages like Arabic.",
        "authors": [
          "Engy Ibrahim",
          "Farhah Adel",
          "Marwan Torki",
          "Nagwa M. El-Makky"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "384",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "388",
        "paper_id": 31,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.31.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.31.jpg",
        "title": "Learning Word Embeddings from Glosses: A Multi-Loss Framework for Arabic Reverse Dictionary Tasks",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.31",
        "x": -2.3046374320983887,
        "y": 1.9267908334732056,
        "year": "2025"
      },
      {
        "abstract": "The linguistic fragmentation of Arabic, with over 30 dialects exhibiting low mutual intelligibility, presents a critical challenge for deploying natural language processing (NLP) in healthcare. Conventional fine-tuning of large language models (LLMs) for each dialect is computationally prohibitive and operationally unsustainable. In this study, we explore model merging as a scalable alternative by integrating three pre-trained LLMs\u2014a medical domain expert, an Egyptian Arabic model, and a Moroccan Darija model\u2014into a unified system without additional fine-tuning. We introduce a novel evaluation framework that assesses both dialectal fidelity via dual evaluation: LLM-based automated scoring and human assessments by native speakers. Our results demonstrate that the merged model effectively handles cross-dialect medical scenarios, such as interpreting Moroccan Darija inputs for Egyptian Arabic-speaking clinicians, while maintaining high clinical relevance. The merging process reduced computational cost by over 60% compared to per-dialect fine-tuning, highlighting its viability for resource-constrained settings. This work offers a promising path for building dialect-aware medical LLMs at scale, with implications for broader deployment across linguistically diverse regions.",
        "authors": [
          "Ahmed Ibrahim",
          "Abdullah Hosseini",
          "Hoda Helmy",
          "Wafa Lakhdhar",
          "Ahmed Serag"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "338",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "346",
        "paper_id": 27,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.27.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.27.jpg",
        "title": "Bridging Dialectal Gaps in Arabic Medical LLMs through Model Merging",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.27",
        "x": -2.752953052520752,
        "y": 4.299931526184082,
        "year": "2025"
      },
      {
        "abstract": "Under-represented languages suffer from a lack of data, and as a result, there are few LLMs that support them. Extending an existing LLM to a new language is a practical option for startups, university labs, and organizations with limited budgets. This process involves several steps. In this paper, we describe how we adapted the Falcon3-7B model to Arabic, covering everything from data collection and training to evaluation. Falcon-Arabic was trained exclusively on native data to better capture the cultural and linguistic aspects of the language. Our evaluations show that Falcon-Arabic achieves state-of-the-art results on a range of Arabic benchmarks.",
        "authors": [
          "Basma El Amel Boussaha",
          "Mohammed Alyafeai",
          "Ahmed Alzubaidi",
          "Leen Al Qadi",
          "Shaikha Alsuwaidi",
          "Hakim Hacid"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "1",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "15",
        "paper_id": 1,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.1.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.1.jpg",
        "title": "Adapting Falcon3-7B Language Model for Arabic: Methods, Challenges, and Outcomes",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.1",
        "x": -0.7165042757987976,
        "y": 6.84153938293457,
        "year": "2025"
      },
      {
        "abstract": "Post-training has emerged as a crucial technique for aligning pre-trained Large Language Models (LLMs) with human instructions, significantly enhancing their performance across a wide range of tasks. Central to this process is the quality and diversity of post-training datasets. This paper presents a review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: (1) LLM Capabilities (e.g., Question Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation, and Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3) Alignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness. Each dataset is rigorously evaluated based on popularity, practical adoption, recency and maintenance, documentation and annotation quality, licensing transparency, and scientific contribution. Our review revealed critical gaps in the development of Arabic post-training datasets, including limited task diversity, inconsistent or missing documentation and annotation, and low adoption across the community. Finally, the paper discusses the implications of these gaps on the progress of Arabic-centric LLMs and applications while providing concrete recommendations for future efforts in Arabic post-training dataset development.",
        "authors": [
          "Mohammed Alkhowaiter",
          "Saied Alshahrani",
          "Norah F Alshahrani",
          "Reem I. Masoud",
          "Alaa Alzahrani",
          "Deema Alnuhait",
          "Emad A. Alghamdi",
          "Khalid Almubarak"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "323",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "337",
        "paper_id": 26,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.26.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.26.jpg",
        "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.26",
        "x": -1.4051448106765747,
        "y": 2.9412591457366943,
        "year": "2025"
      },
      {
        "abstract": "Processing North African Arabic dialects presents significant challenges due to high lexical variability, frequent code-switching with French, and the use of both Arabic and Latin scripts. We address this with a phonemebased normalization strategy that maps Arabic and French text into a simplified representation (Arabic rendered in Latin script), reflecting native reading patterns. Using this method, we pretrain BERTbased models on normalized Modern Standard Arabic and French only and evaluate them on Named Entity Recognition (NER) and text classification. Experiments show that normalized standard-language corpora yield competitive performance on North African dialect tasks; in zero-shot NER, Ar_20k surpasses dialectpretrained baselines. Normalization improves vocabulary alignment, indicating that normalized standard corpora can suffice for developing dialect-supportive",
        "authors": [
          "Yassine Toughrai",
          "Kamel Smaili",
          "David Langlois"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "375",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "383",
        "paper_id": 30,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.30.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.30.jpg",
        "title": "Modeling North African Dialects from Standard Languages",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.30",
        "x": -3.675281047821045,
        "y": 5.040920257568359,
        "year": "2025"
      },
      {
        "abstract": "Lemmatization for dialectal Arabic poses many challenges due to the lack of orthographic standards and limited morphological analyzers. This work explores the effectiveness of Seq2Seq models for lemmatizing dialectal Arabic, both without analyzers and with their integration. We assess how well these models generalize across dialects and benefit from related varieties. Focusing on Egyptian, Gulf, and Levantine dialects with varying resource levels, our analysis highlights both the potential and limitations of data-driven approaches. The proposed method achieves significant gains over baselines, performing well in both low-resource and dialect-rich scenarios.",
        "authors": [
          "Mostafa Saeed",
          "Nizar Habash"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "117",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "129",
        "paper_id": 10,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.10.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.10.jpg",
        "title": "Lemmatizing Dialectal Arabic with Sequence-to-Sequence Models",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.10",
        "x": -3.703494071960449,
        "y": 4.969118118286133,
        "year": "2025"
      },
      {
        "abstract": "In this paper, we present the Arabic Multimodal Crawl (AMCrawl), the first native-based Arabic multimodal dataset to our knowledge, derived from the Common Crawl corpus and rigorously filtered for quality and safety. Image-text pair datasets are the standard choice for pretraining multimodal large language models. However, they are often derived from image alt-text metadata, which is typically brief and context-poor, disconnecting images from their broader meaning. Although significant advances have been made in building interleaved image-text datasets for English, such as the OBELICS dataset, a substantial gap remains for native Arabic content. Our processing covered 8.6 million Arabic web pages, yielding 5.8 million associated images and 1.3 billion text tokens. The final dataset includes interleaved image-text documents and question-answer pairs, featuring 2.8 million high-quality interleaved documents and 5 million QA pairs. Alongside the dataset, we release the complete pipeline and code, ensuring reproducibility and encouraging further research and development. To demonstrate the effectiveness of AMCrawl, we introduce a publicly available native Arabic Vision Language model, trained with 13 billion parameters. These models achieve competitive results when benchmarked against publicly available datasets. AMCrawl bridges a critical gap in Arabic multimodal resources, providing a robust foundation for developing Arabic multimodal large language models and fostering advancements in this underrepresented area. Code: github.com/shahad-aboukozzana/AMCrawl",
        "authors": [
          "Shahad Aboukozzana",
          "Muhammad Kamran J Khan",
          "Ahmed Ali"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "448",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "465",
        "paper_id": 37,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.37.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.37.jpg",
        "title": "AMCrawl: An Arabic Web-Scale Dataset of Interleaved Image-Text Documents and Image-Text Pairs",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.37",
        "x": -3.5828945636749268,
        "y": 1.038707971572876,
        "year": "2025"
      },
      {
        "abstract": "Cross-lingual retrieval-augmented generation (RAG) is a critical capability for retrieving and generating answers across languages. Prior work in this context has mostly focused on generation and relied on benchmarks derived from open-domain sources, most notably Wikipedia. In such settings, retrieval challenges often remain hidden due to language imbalances, overlap with pretraining data, and memorized content. To address this gap, we study Arabic-English RAG in a domain-specific setting using benchmarks derived from real-world corporate datasets. Our benchmarks include all combinations of languages for the user query and the supporting document, drawn independently and uniformly at random. This enables a systematic study of multilingual retrieval behavior.Our findings reveal that retrieval is a critical bottleneck in cross-lingual domain-specific scenarios, with substantial performance drops occurring when the user query and supporting document languages differ. A key insight is that these failures stem primarily from the retriever\u2019s difficulty in ranking documents across languages. Finally, we propose two simple retrieval strategies that address this source of failure by enforcing equal retrieval from both languages or by translating the query, resulting in substantial improvements in cross-lingual and overall performance. These results highlight meaningful opportunities for improving multilingual retrieval, particularly in practical, real-world RAG applications.",
        "authors": [
          "Chen Amiraz",
          "Yaroslav Fyodorov",
          "Elad Haramaty",
          "Zohar Karnin",
          "Liane Lewin-Eytan"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "69",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "83",
        "paper_id": 6,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.6.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.6.jpg",
        "title": "The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.6",
        "x": -0.889583170413971,
        "y": 6.7992963790893555,
        "year": "2025"
      },
      {
        "abstract": "The impressive advancement of Large Language Models (LLMs) in English has not been matched across all languages. In particular, LLM performance in Arabic lags behind, due to data scarcity, linguistic diversity of Arabic and its dialects, morphological complexity, etc. Progress is further hindered by the quality of Arabic benchmarks, which typically rely on static, publicly available data, lack comprehensive task coverage, or do not provide dedicated platforms with blind test sets. This makes it challenging to measure actual progress and to mitigate data contamination. Here, we aim to bridge these gaps. In particular, we introduce BALSAM, a comprehensive, community-driven benchmark aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP tasks from 14 broad categories, with 52K examples divided into 37K test and 15K development, and a centralized, transparent platform for blind evaluation. We envision BALSAM as a unifying platform that sets standards and promotes collaborative research to advance Arabic LLM capabilities.",
        "authors": [
          "Rawan Nasser Almatham",
          "Kareem Mohamed Darwish",
          "Raghad Al-Rasheed",
          "Waad Thuwaini Alshammari",
          "Muneera Alhoshan",
          "Amal Almazrua",
          "Asma Al Wazrah",
          "Mais Alheraki",
          "Firoj Alam",
          "Preslav Nakov",
          "Norah A. Alzahrani",
          "Eman Albilali",
          "Nizar Habash",
          "Abdelrahman Mustafa El-Sheikh",
          "Muhammad Elmallah",
          "Hamdy Mubarak",
          "Zaid Alyafeai",
          "Mohamed Anwar",
          "Haonan Li",
          "Ahmed Abdelali",
          "Nora Altwairesh",
          "Maram Hasanain",
          "Abdulmohsen Al-Thubaity",
          "Shady Shehata",
          "Bashar Alhafni",
          "Injy Hamed",
          "Go Inoue",
          "Khalid N. Elmadani",
          "Ossama Obeid",
          "Fatima Haouari",
          "Tamer Elsayed",
          "Emad A. Alghamdi",
          "Khalid Almubarak",
          "Saied Alshahrani",
          "Ola Aljareh",
          "Safa Alajlan",
          "Areej Alshaqarawi",
          "Maryam Alshihri",
          "Sultana Alghurabi",
          "Atikah Alzeghayer",
          "Afrah Altamimi",
          "Abdullah Alfaifi",
          "Abdulrahman M Alosaimy"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "258",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "277",
        "paper_id": 21,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.21.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.21.jpg",
        "title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.21",
        "x": -2.547093152999878,
        "y": 0.2580101788043976,
        "year": "2025"
      },
      {
        "abstract": "Speech emotion recognition is vital for human-computer interaction, particularly for low-resource languages like Arabic, which face challenges due to limited data and research. We introduce ArabEmoNet, a lightweight architecture designed to overcome these limitations and deliver state-of-the-art performance. Unlike previous systems relying on discrete MFCC features and 1D convolutions, which miss nuanced spectro-temporal patterns, ArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving critical emotional cues often lost in traditional methods. While recent models favor large-scale architectures with millions of parameters, ArabEmoNet achieves superior results with just 1 million parameters\u201490 times smaller than HuBERT base and 74 times smaller than Whisper. This efficiency makes it ideal for resource-constrained environments. ArabEmoNet advances Arabic speech emotion recognition, offering exceptional performance and accessibility for real-world applications.",
        "authors": [
          "Ali Abouzeid",
          "Bilal Elbouardi",
          "Mohamed Maged",
          "Shady Shehata"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "211",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "218",
        "paper_id": 17,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.17.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.17.jpg",
        "title": "ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for Robust Arabic Speech Emotion Recognition",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.17",
        "x": -4.006129264831543,
        "y": 4.197030067443848,
        "year": "2025"
      },
      {
        "abstract": "The Holy Qur\u2019an provides timeless guidance, addressing modern challenges and offering answers to many important questions. The Qur\u2019an QA 2023 shared task introduced the Qur\u2019anic Passage Retrieval (QPR) task, which involves retrieving relevant passages in response to MSA questions. In this work, we evaluate the ability of seven pre-trained large language models (LLMs) to retrieve relevant passages from the Qur\u2019an in response to given questions, considering zero-shot and several few-shot scenarios. Our experiments show that the best model, Claude, significantly outperforms the state-of-the-art QPR model by 28 points on MAP and 38 points on MRR, exhibiting an impressive improvement of about 113% and 82%, respectively.",
        "authors": [
          "Sohaila Eltanbouly",
          "Salam Albatarni",
          "Shaimaa Hassanein",
          "Tamer Elsayed"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "203",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "210",
        "paper_id": 16,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.16.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.16.jpg",
        "title": "Can LLMs Directly Retrieve Passages for Answering Questions from Qur\u2019an?",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.16",
        "x": -1.610828161239624,
        "y": 2.6479074954986572,
        "year": "2025"
      },
      {
        "abstract": "Conversational question-answering (CQA) plays a crucial role in bridging the gap between human language and machine understanding, enabling more natural and interactive interactions with AI systems. In this work, we present the first results on open-domain Arabic CQA using deep learning. We introduce AraQReCC, a large-scale Arabic CQA dataset containing 9K conversations with 62K question-answer pairs, created by translating a subset of the QReCC dataset. To ensure data quality, we used COMET-based filtering and manual ratings from large language models (LLMs), such as GPT-4 and LLaMA, selecting conversations with COMET scores, along with LLM ratings of 4 or more. AraQReCC facilitates advanced research in Arabic CQA, improving clarity and relevance through question rewriting. We applied AraT5 for question rewriting and used BM25 and Dense Passage Retrieval (DPR) for passage retrieval. AraT5 is also used for question answering, completing the end-to-end system. Our experiments show that the best performance is achieved with DPR, attaining an F1 score of 21.51% on the test set. While this falls short of the human upper bound of 40.22%, it underscores the importance of question rewriting and quality-controlled data in enhancing system performance.",
        "authors": [
          "Mariam E. Hassib",
          "Nagwa M. El-Makky",
          "Marwan Torki"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "84",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "96",
        "paper_id": 7,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.7.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.7.jpg",
        "title": "Open-domain Arabic Conversational Question Answering with Question Rewriting",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.7",
        "x": -3.273672103881836,
        "y": 4.880426406860352,
        "year": "2025"
      },
      {
        "abstract": "This paper evaluates the knowledge and reasoning capabilities of Large Language Models in Islamic inheritance law, \u02bfilm al-maw\u0101r\u012bth. We assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice questions covering diverse inheritance scenarios, designed to test each model\u2019s ability\u2014from understanding the inheritance context to computing the distribution of shares prescribed by Islamic jurisprudence. The results show a wide performance gap among models. o3 and Gemini 2.5 achieved accuracies above 90%, while ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect important differences in reasoning ability and domain adaptation.We conduct a detailed error analysis to identify recurring failure patterns across models, including misunderstandings of inheritance scenarios, incorrect application of legal rules, and insufficient domain knowledge. Our findings highlight the limitations of current models in handling structured legal reasoning and suggest directions for improving their performance in Islamic legal reasoning.",
        "authors": [
          "Abdessalam Bouchekif",
          "Samer Rashwani",
          "Heba Sbahi",
          "Shahd Gaben",
          "Mutaz Al Khatib",
          "Mohammed Ghaly"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "246",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "257",
        "paper_id": 20,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.20.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.20.jpg",
        "title": "Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.20",
        "x": -3.7454121112823486,
        "y": 4.9164605140686035,
        "year": "2025"
      },
      {
        "abstract": "The quality of training data plays a critical role in the performance of large language models (LLMs). This is especially true for low-resource languages where high-quality content is relatively scarce. Inspired by the success of FineWeb-Edu for English, we construct a native Arabic educational-quality dataset using similar methodological principles. We begin by sampling 1 million Arabic web documents from Common Crawl and labeling them into six quality classes (0\u20135) with Qwen-2.5-72B-Instruct model using a classification prompt adapted from FineWeb-Edu. These labeled examples are used to train a robust classifier capable of distinguishing educational content from general web text. We train a classification head on top of a multilingual 300M encoder model, then use this classifier to filter a large Arabic web corpus, discarding documents with low educational value. To evaluate the impact of this curation, we pretrain from scratch two bilingual English-Arabic 7B LLMs on 800 billion tokens using the filtered and unfiltered data and compare their performance across a suite of benchmarks. Our results show a significant improvement when using the filtered educational dataset, validating the effectiveness of quality filtering as a component in a balanced data mixture for Arabic LLM development. This work addresses the scarcity of high-quality Arabic training data and offers a scalable methodology for curating educational quality content in low-resource languages.",
        "authors": [
          "Majd Hawasly",
          "Muhammad Tasnim Mohiuddin",
          "Hamdy Mubarak",
          "Sabri Boughorbel"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference",
        "first_page": "436",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "447",
        "paper_id": 36,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-main.36.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-main.36.jpg",
        "title": "ArabicWeb-Edu: Educational Quality Data for Arabic LLM Training",
        "type": "main",
        "url": "https://aclanthology.org/2025.arabicnlp-main.36",
        "x": -1.5808945894241333,
        "y": 0.7166782021522522,
        "year": "2025"
      },
      {
        "abstract": "This paper presents the Isnad AI system de- veloped for the IslamicEval 2025 Shared T ask 1A, which focuses on identifying character- level spans of Quranic verses (A yahs) and Prophetic sayings (Hadiths) within Large Lan- guage Model (LLM) outputs. This task is formulated as a token classification problem using a fine-tuned AraBERT v2 model. The primary contribution is a novel rule-based data preprocessing and augmentation pipeline, through which a large-scale, high-quality train- ing corpus is systematically generated from raw religious texts. Through comprehensive ablation studies, it is demonstrated that the controlled synthetic data generation approach significantly outperforms traditional database lookup methods and basic fine-tuning ap- proaches. The system achieved an F1 score of 66.97% in the official test set, demonstrating the effectiveness of principled synthetic data generation for specialized religious text verifi- cation tasks. T o support reproducibility and fu- ture research in Islamic citation detection, all code, generated datasets, and experimental re- sources are made publicly available on GitHub and Hugging Face.",
        "authors": [
          "Fatimah Mohamed Emad Elden"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "540",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "559",
        "paper_id": 74,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.74.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.74.jpg",
        "title": "Isnad AI at IslamicEval 2025: A Rule-Based System for Identifying Religious Texts in LLM Outputs",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.74",
        "x": -3.6782612800598145,
        "y": 4.0884504318237305,
        "year": "2025"
      },
      {
        "abstract": "The intersection of Arabic linguistic complex- ity and specialized reasoning presents a key challenge for Islamic question-answering sys- tems, particularly in the under-addressed area of inheritance law . This paper presents our methodology for the QIAS2025 shared task, assessing LLM capabilities in Islamic knowl- edge through two subtasks: Inheritance Rea- soning (\u02bfilm al-maw\u0101r\u012bth) and General Islamic Assessment. A zero-shot, prompt-based ap- proach with DeepSeek-R1 (deepseek-reasoner) addresses the former, while a three-stage RAG pipeline handles the latter. Our approaches achieved competitive results, with an accu- racy of 0.704 for inheritance reasoning (10th place/15 teams) and 0.9272 for general Islamic assessment (2nd place/10 teams), demonstrat- ing the efficacy of tailored model strategies for religious QA. These insights pave the way for more culturally and linguistically adaptive AI systems in Islamic scholarly applications.",
        "authors": [
          "Yossra Noureldien",
          "Hassan Suliman",
          "Farah Attallah",
          "Abdelrazig Mohamed",
          "Sara Abdalla"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "914",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "922",
        "paper_id": 126,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.126.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.126.jpg",
        "title": "Athar at QIAS2025: LLM-based Question Answering Systems for Islamic Inheritance and Classical Islamic Knowledge",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.126",
        "x": -0.7233068943023682,
        "y": 6.882570266723633,
        "year": "2025"
      },
      {
        "abstract": "We address the problem of distinguishing be- tween human-authored and AI-generated text in low-resource languages, particularly Ara- bic. We present the LMSA 1 team\u2019s participa- tion in the ARATECT (Arabic AI-Generated Text Detection) subtask of the AraGenEval 2 shared task, which targets the detection of AI-generated Arabic texts. We propose an ensemble-based classification framework that integrates multilingual and Arabic-specific pre-trained language models, namely Fanar, AraBERT, and XLM-R, optimized through a dedicated fine-tuning pipeline. The approach is evaluated on the balanced Arabic text dataset provided by the shared task organizers. Our sys- tem achieved an F1-score of 0.864 and ranked first among all participating teams.",
        "authors": [
          "Kaoutar Zita",
          "Attia Nehar",
          "Abdelkader Khelil",
          "Slimane Bellaouar",
          "Hadda Cherroun"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "26",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "31",
        "paper_id": 4,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.4.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.4.jpg",
        "title": "LMSA at AraGenEval Shared Task: Ensemble-Based Detection of AI-Generated Arabic Text Using Multilingual and Arabic-Specific Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.4",
        "x": -0.7824939489364624,
        "y": 6.876694679260254,
        "year": "2025"
      },
      {
        "abstract": "Pre-trained language models (PLMs) show po- tential for advancing mental health care, yet their effectiveness in Arabic mental health con- texts is underexplored. This study evaluates PLMs on two multi-label classification tasks from the AraHealthQA 2025 shared task Track 1: question categorization and answer strat- egy classification. We systematically evalu- ate several LLMs spanning Arabic-specialized, multilingual, and general-purpose architectures using zero-shot inference, with comparative analysis revealing Qwen3-14B\u2019s superior per- formance. Our approach combines prompt- based inference, label mapping, and strategi- cally crafted Arabic prompts. Experiments on 350 training and 150 test samples demonstrate competitive performance, securing 4 th place in both tasks (Question F1: 0.52, Answer F1: 0.76; Question Jaccard: 0.41, Answer Jaccard: 0.66). These findings reveal strengths and limi- tations of current PLMs for detecting complex intents in Arabic mental health contexts.",
        "authors": [
          "Adiba Fairooz Chowdhury",
          "Md Sagor Chowdhury"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "155",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "163",
        "paper_id": 23,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.23.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.23.jpg",
        "title": "Quasar at AraHealthQA Track 1 : Leveraging Zero-Shot Large Language Models for Question and Answer Categorization in Arabic Mental Health",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.23",
        "x": -0.759466826915741,
        "y": 6.705541133880615,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our participation in the QIAS 2025 shared tasks, namely Islamic Inheritance Reasoning and Islamic Knowledge Assessment sub-tasks. We propose an Islamic Retrieval- Augmented Generation (RAG) system that inte- grates multiple knowledge sources and seman- tic retrieval methods. Our evaluation compares multilingual general-purpose models and Arabic- centric models, using the accuracy metric. Results show that multilingual models consistently outper- form Arabic-language models. The Mistral-large achieved the highest accuracy in Task 1 (72%) us- ing basic RAG with our augmented knowledge resource, while GPT-4o with RAG and K2R re- trieval achieved the best score in Task 2 (87.71%). These \ufb01ndings highlight the effectiveness of RAG in enhancing LLM performance for complex Is- lamic reasoning and knowledge assessment tasks.",
        "authors": [
          "Sanaa Alowaidi"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "940",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "946",
        "paper_id": 130,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.130.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.130.jpg",
        "title": "SEA-Team at QIAS 2025: Enhancing LLMs for Question Answering in Islamic Texts",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.130",
        "x": -1.3690086603164673,
        "y": 0.7807007431983948,
        "year": "2025"
      },
      {
        "abstract": "We describe our system submission toTask 1 (Sentence-level Readability Assessment) of the BAREC Shared Task 2025 (Elmadani et al., 2025a), in thestrict track. Task 1 requires pre- dicting the readability level of an Arabic sen- tence on a scale from 1 (easiest) to 19 (hardest), reflecting reading difficulty. Our approach in- tegrates contextual and syntactic information by combining pretrained BERT embeddings (Devlin et al., 2019) with a Graph Neural Net- work (GNN) (Zhou et al., 2021) over depen- dency parse trees (Kipf and Welling, 2017). Our hypothesis is that readability is influenced not only by word choice but also by syntac- tic complexity\u2014especially in morphologically rich languages like Arabic (Habash, 2010). To capture both aspects, we represent each sen- tence as a dependency graph with BERT token embeddings as node features, and use a GNN to model the syntactic structure. Experimental results show that our syntax-aware model im- proves over a strong BERT baseline, highlight- ingthevalueofstructurallinguisticinformation for fine-grained readability classification.1",
        "authors": [
          "Ahmed Bahloul"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "253",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "260",
        "paper_id": 35,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.35.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.35.jpg",
        "title": "Syntaxa at BAREC Shared Task 2025: BERTnParse - Fusion of BERT and Dependency Graphs for Readability Prediction",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.35",
        "x": -2.1406588554382324,
        "y": 1.8439699411392212,
        "year": "2025"
      },
      {
        "abstract": "This paper details our submission\u2013Hafs2Vec\u2013 to the Iqra\u2019Eval 2025 shared task on Arabic mispronunciation detection. Our system is built upon a wav2vec2-xls-r-1b model, enhanced by two key contributions: a strategic data mixing approach and a custom Qur\u2019anic phonemizer. We augment the official Iqra\u2019Eval training data with 94 hours of professional Qur\u2019anic recita- tions, creating a balanced dataset that combines learner speech with high-quality acoustic refer- ences. To accurately label the reciter data, we developed a custom, Tajweed-aware phonem- izer that captures the specific articulation rules of Qur\u2019anic recitation. On the QuranMB test set, our system achieved an F1-score of 46.50% and a high recall of 79.20%.",
        "authors": [
          "Ahmed Ibrahim"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "453",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "456",
        "paper_id": 62,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.62.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.62.jpg",
        "title": "Hafs2Vec: A System for the IqraEval Arabic and Qur\u2019anic Phoneme-level Pronunciation Assessment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.62",
        "x": -3.848832607269287,
        "y": 4.076902389526367,
        "year": "2025"
      },
      {
        "abstract": "Arabic image captioning remains underex- plored in vision\u2013language research due to lim- ited resources and the linguistic complexity of Arabic. In the ImageEval 2025 Shared Task, we evaluated three models, AIN, BLIP-Arabic- Flickr-8k, and Qwen 2.5, across zero-shot, fine-tuning, retrieval-augmented, and ensem- ble setups. Our official submission, fine-tuned BLIP with retrieval augmentation, ranked 5th overall based on both cosine similarity and LLM-as-a-judge scores. Post-submission ex- periments showed that ensemble captioning yielded the strongest captions across metrics. These findings demonstrate that even modest fine-tuning combined with retrieval augmenta- tion can substantially improve Arabic caption- ing quality, which is significant in light of the limited resources for the language.",
        "authors": [
          "Rana Gaber",
          "Seif Eldin Amgad",
          "Ahmed Sherif Nasri",
          "Mohamed Ibrahim Ragab",
          "Ensaf Hussein Mohamed"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "419",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "431",
        "paper_id": 58,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.58.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.58.jpg",
        "title": "NU_Internship team at ImageEval 2025: From Zero-Shot to Ensembles: Enhancing Grounded Arabic Image Captioning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.58",
        "x": -0.8663891553878784,
        "y": 6.74786376953125,
        "year": "2025"
      },
      {
        "abstract": "The detection of harmful online content, in- cluding hate speech and propaganda, is partic- ularly challenging in multimodal and multilin- gual contexts such as Arabic social media. This work addresses Sub-task 1: Text-based Hate and Hope Speech Classification in the MA- HED2025 (Zaghouani et al., 2025) challenge, where the goal is to classify Arabic text into hate, hope, or not_applicable. We develop a system based on pre-trained Arabic BERT mod- els with three fine-tuning strategies, combined with a custom preprocessing pipeline for noise removal, normalization, and diacritic stripping. To address class imbalance and lexical sparsity, we augment the training data with synthetically generated paraphrases via the OpenAI API. Ex- perimental results on the official test set demon- strate that our best configuration, BERT-base- AraBERTv02 + NN with cleaning and gener- ated data, achieves a macro-F1 score of 0.6747 F1. Error analysis reveals that mislabeled train- ing instances significantly limit model perfor- mance, suggesting that future improvements may be achieved through systematic dataset refinement. Our approach highlights the im- portance of preprocessing, augmentation, and careful architectural choices for robust Arabic text classification.",
        "authors": [
          "Yasser Alhabashi",
          "Serry Sibaee",
          "Omer Nacar",
          "Adel Ammar",
          "Wadii Boulila"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "590",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "594",
        "paper_id": 78,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.78.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.78.jpg",
        "title": "ANLPers at MAHED2025: From Hate to Hope: Boosting Arabic Text Classification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.78",
        "x": -3.6450746059417725,
        "y": 3.965076446533203,
        "year": "2025"
      },
      {
        "abstract": "",
        "authors": [
          "Wafaa S. El-Kassas",
          "Enas A. Hakim Khalil"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "706",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "711",
        "paper_id": 97,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.97.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.97.jpg",
        "title": "AraNLP at MAHED 2025 Shared Task: Using AraBERT for Text-based Hate and Hope Speech Classification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.97",
        "x": -4.5026774406433105,
        "y": -0.25098490715026855,
        "year": "2025"
      },
      {
        "abstract": "Authorship style transfer enables the gener- ation of text that imitates a specific writer\u2019s linguistic and stylistic patterns, a challeng- ing task in morphologically rich languages like Arabic. We tackle this problem in the AraGenEval 2025 shared task, exploring conditioning strategies to guide a fine-tuned UBC-NLP/AraT5v2-base-1024 model in pro- ducing text aligned with target authors\u2019 styles. Our investigation compares implicit modeling, numeric and descriptive author tokens, and ex- plicit prompt engineering in Arabic. Explicit natural language instructions proved most effec- tive, achieving the highest competition scores with BLEU of 24.58 and chrF of 59.01, secur- ing first place, while demonstrating that inter- pretable approaches can rival or surpass more opaque methods.",
        "authors": [
          "Omer Nacar",
          "Mahmoud Reda",
          "Serry Sibaee",
          "Yasser Alhabashi",
          "Adel Ammar",
          "Wadii Boulila"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "49",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "53",
        "paper_id": 8,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.8.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.8.jpg",
        "title": "ANLPers at AraGenEval Shared Task: Descriptive Author Tokens for Transparent Arabic Authorship Style Transfer",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.8",
        "x": -3.2672436237335205,
        "y": 4.736086368560791,
        "year": "2025"
      },
      {
        "abstract": "The BAREC 2025 Shared Task on Arabic read- ability targets 19 levels of ordinal prediction at the sentence and document levels under strict training. This paper describes a two stages sys- tem that basically starts with BAREC-tuned AraBERT checkpoints and then specializes on the Strict splits with Weighted Kappa Loss (WKL), an objective aligned with Quadratic Weighted Kappa (QWK). A single architecture with inputs specific to each track is utilized for both tracks. On the Strict setting, our best sys- tems reach 0.842/0.841 QWK (public/blind) at the sentence level and 0.828/0.790 QWK at the document level.",
        "authors": [
          "Shimaa Ibrahim",
          "Md. Rafiul Biswas",
          "Mabrouka Bessghaier",
          "Wajdi Zaghouani"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "274",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "279",
        "paper_id": 39,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.39.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.39.jpg",
        "title": "MarsadLab at BAREC Shared Task 2025: Strict-Track Readability Prediction with Specialized AraBERT Models on BAREC",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.39",
        "x": -3.915839195251465,
        "y": 4.895673751831055,
        "year": "2025"
      },
      {
        "abstract": "Detecting hate and hope speech in Arabic so- cial media remains a critical challenge in the MAHED 2025 Shared Task (Zaghouani et al., 2025) due to the complex diglossia, diverse di- alects, and prevalent orthographic noise in user- generated texts. We introduce a multilingual transformer ensemble that integrates three com- plementary encoders\u2014AraBERTv2, AraBERT- Twitter, and XLM-RoBERTa\u2014using a uniform soft voting approach (Salur and Ayd\u0131n, 2022). Each model is fine-tuned with a balanced data augmentation strategy, combining 70% back- translation and 30% Easy Data Augmentation (EDA), followed by noise induction to mimic real-world textual perturbations (Bayer et al., 2022). Hyperparameters are optimized via Op- tuna (Akiba et al., 2019) to maximize macro-F1 performance. Our method achieves a macro-F1 score of 0.65 on the official test set, surpass- ing the strongest single model by 0.04 and out- performing competitive multilingual baselines such as mBERT and LLaMA-based Arabic large language models. These results demon- strate that combining complementary linguistic representations with targeted augmentation sub- stantially improves robustness across dialects and addresses class imbalance in Arabic hate and hope speech classification.",
        "authors": [
          "Trinh Tran Tran",
          "Th\u00ecn \u0110\u1eb7ng V\u0103n"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "603",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "607",
        "paper_id": 81,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.81.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.81.jpg",
        "title": "TranTranUIT at MAHED Shared Task: Multilingual Transformer Ensemble with Advanced Data Augmentation and Optuna-based Hyperparameter Optimization",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.81",
        "x": -1.7199815511703491,
        "y": 2.7580978870391846,
        "year": "2025"
      },
      {
        "abstract": "We present !MSA\u2019s winning system for the BAREC 2025 Shared Task on fine-grained Arabic readability assessment, achieving first place in six of six tracks. Our approach is a confidence-weighted ensemble of four com- plementary transformer models (AraBERTv2, AraELECTRA, MARBERT, and CAMeL- BERT) each fine-tuned with distinct loss func- tions to capture diverse readability signals. To tackle severe class imbalance and data scarcity, we applied weighted training, advanced pre- processing, SAMER corpus relabeling with our strongest model, and synthetic data gener- ation via Gemini 2.5 Flash, adding 10k rare- level samples. A targeted post-processing step corrected the prediction distribution skew, de- livering a 6.3% Quadratic Weighted Kappa (QWK) gain. Our system reached 87.5% QWK at the sentence level and 87.4% at the document level, demonstrating the power of model and loss diversity, confidence-informed fusion, and intelligent augmentation for robust Arabic readability prediction.1",
        "authors": [
          "Mohamed Basem",
          "Mohamed Younes",
          "Seif Ahmed",
          "Abdelrahman Moustafa"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "297",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "305",
        "paper_id": 42,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.42.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.42.jpg",
        "title": "!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.42",
        "x": -0.7051272392272949,
        "y": 6.755367755889893,
        "year": "2025"
      },
      {
        "abstract": "This paper presents a new Arabic image cap- tioning dataset created for the ImageEval 2025 Shared Task. The dataset focuses on images related to con\ufb02ict, resistance, and everyday life under occupation. Each image is paired with a Modern Standard Arabic caption of 40\u201370 words that describes what is shown and adds cultural or emotional context. To help anno- tators write rich and consistent captions, we used prompt-based guidelines, including step- by-step reasoning and writing from speci\ufb01c roles such as journalists or humanitarian ob- servers. This method produced captions that are both descriptive and meaningful. The dataset \ufb01lls an important gap in Arabic resources, espe- cially for sensitive and historically signi\ufb01cant topics. It can be used to train and evaluate Ara- bic vision language models, test multilingual AI systems, and support applications in journalism, education, and cultural preservation.",
        "authors": [
          "Mohammed Alkhanafseh",
          "Ola Surakhi",
          "Abdallah Abedaljalill"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "395",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "400",
        "paper_id": 54,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.54.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.54.jpg",
        "title": "BZU-AUM@ImageEval2025: An Arabic Image Captioning Dataset for Conflict Narratives with Human Annotation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.54",
        "x": -1.1561402082443237,
        "y": 1.6817866563796997,
        "year": "2025"
      },
      {
        "abstract": "We present GEMM3N-DR, a multimodal sys- tem for NADI 2025 Subtask 3 (Spoken Arabic Diacritic Restoration). GEMM3N-DR fine- tunes the Gemma 3N LLM via Low-Rank Adaptation (LoRA) using only the official NADI training data, taking both audio and un- diacritized text as input and generating fully diacritized output. We apply data augmentation with the nlpaug and the CATT diacritization model. At inference time, we use a structured Arabic instruction and 7-shot examples. Our system achieved a Word Error Rate (WER) of 64% and Character Error Rate (CER) of 15% on the hidden test set, ranking in 2nd place in the competition. We provide a detailed analysis of model performance, including common er- ror types such as hallucination and incomplete outputs.",
        "authors": [
          "Mohamed Lotfy Elrefai"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "767",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "773",
        "paper_id": 106,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.106.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.106.jpg",
        "title": "Unicorn at NADI 2025 Subtask 3: GEMM3N-DR: Audio-Text Diacritic Restoration via Fine-tuning Multimodal Arabic LLM",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.106",
        "x": -2.2722227573394775,
        "y": 3.1607956886291504,
        "year": "2025"
      },
      {
        "abstract": "Large Language Models (LLMs) inherently re- flect the vast data distributions they encounter during their pre-training phase. As this data is predominantly sourced from the web, there is a high chance it will be skewed towards high-resourced languages and cultures, such as those of the West. Consequently, LLMs of- ten exhibit a diminished understanding of cer- tain communities, a gap that is particularly evi- dent in their knowledge of Arabic and Islamic cultures. This issue becomes even more pro- nounced with increasingly under-represented topics. To address this critical challenge, we introduce PalmX 2025, the first shared task de- signed to benchmark the cultural competence of LLMs in these specific domains. The task is composed of two subtasks featuring multiple- choice questions (MCQs) in Modern Standard Arabic (MSA): General Arabic Culture and General Islamic Culture. These subtasks cover a wide range of topics, including traditions, food, history, religious practices, and language expressions from across 22 Arab countries. The initiative drew considerable interest, with 26 teams registering for Subtask 1 and 19 for Subtask 2, culminating in nine and six valid submissions, respectively. Our findings re- veal that task-specific fine-tuning substantially boosts performance over baseline models. The top-performing systems achieved an accuracy of 72.15% on cultural questions and 84.22% on Islamic knowledge. Parameter-efficient fine- tuning emerged as the predominant and mos",
        "authors": [
          "Fakhraddin Alwajih",
          "Abdellah El Mekki",
          "Hamdy Mubarak",
          "Majd Hawasly",
          "Abubakr Mohamed",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "774",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "789",
        "paper_id": 107,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.107.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.107.jpg",
        "title": "PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.107",
        "x": -3.5579659938812256,
        "y": 1.8856534957885742,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our approach to the Im- ageEval Shared Task for Arabic image caption- ing, with a focus on the Captioning with Re- gion Features Transformer (CRAFT) model. The system combines Faster R-CNN-based re- gion feature extraction with a custom vision transformer encoder and transformer decoder, trained on a custom, human-annotated dataset with a Palestinian context. To ensure fair- ness in evaluation, we compare CRAFT with an alternative Vision-Encoder\u2013Decoder system (AraViT-GPT). Performance was assessed us- ing BLEU, ROUGE, cosine similarity, and an LLM-based semantic evaluation. Results show that CRAFT achieved the highest cosine sim- ilarity (56.22 on the test set), indicating su- perior semantic fidelity to reference captions, while AraViT-GPT showed marginally better n- gram precision and LLM-judge scores. These findings demonstrate the advantages of region- focused visual encoding for Arabic caption gen- eration, particularly in the context of context- rich and historically significant imagery.",
        "authors": [
          "Rabee Al-Qasem",
          "Mohannad Hendi"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "401",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "407",
        "paper_id": 55,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.55.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.55.jpg",
        "title": "ImpactAi at ImageEval 2025 Shared Task: Region-Aware Transformers for Arabic Image Captioning \u2013 A Case Study on the Palestinian Narrative",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.55",
        "x": -3.8386390209198,
        "y": 4.147336959838867,
        "year": "2025"
      },
      {
        "abstract": "In this paper, we report our participation to the PalmX cultural evaluation shared task. Our system, CultranAI, focused on data augmen- tation and LoRA fine-tuning of large language models (LLMs) for Arabic cultural knowledge representation. We benchmarked several LLMs to identify the best-performing model for the task. In addition to utilizing the PalmX dataset, we augmented it by incorporating the Palm dataset and curated a new dataset of over 22K culturally grounded multiple-choice questions (MCQs). Our experiments showed that the Fanar-1-9B-Instruct model achieved the highest performance. We fine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the blind test set, our submitted system ranked 5th with an accuracy of 70.50%, while on the PalmX development set, it achieved an accuracy of 84.1%. We made experimental scripts publicly available for the community.1",
        "authors": [
          "Hunzalah Hassan Bhatti",
          "Youssef Ahmed",
          "Md. Arid Hasan",
          "Firoj Alam"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "809",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "817",
        "paper_id": 111,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.111.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.111.jpg",
        "title": "CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.111",
        "x": -3.0834600925445557,
        "y": 2.3375704288482666,
        "year": "2025"
      },
      {
        "abstract": "This study investigates the impact of bigram- based data augmentation on the joint classifica- tion of hate speech, hope speech, and neutral content in multilingual social media contexts, with a particular focus on Arabic. While pre- vious research has shown the benefits of aug- mentation in text classification, its effective- ness in nuanced domains such as hate and hope speech remains underexplored. Using the anno- tated MAHED dataset, we compare three sce- narios: a baseline without augmentation, global bigram augmentation, and classwise bigram augmentation. The baseline achieved 68.25% accuracy (macro-F1 = 0.6729) on the test set. Global bigram augmentation slightly reduced accuracy to 63.0% (macro-F1 = 0.62), showing no improvement over the baseline. Classwise augmentation achieved 93% accuracy on the validation set but dropped sharply to 59.65% accuracy (macro-F1 = 0.4726) on the test set, indicating severe overfitting. These results sug- gest that bigram-based methods are sensitive to class imbalance and may harm generalisa- tion when applied unevenly across classes. We conclude by highlighting the need for more bal- anced, context-aware augmentation strategies in socially impactful NLP tasks.",
        "authors": [
          "Tolulope Olalekan Abiola",
          "Oluwatobi Joseph Abiola",
          "Ogunleye Temitope Dasola",
          "Tewodros Achamaleh",
          "Obiadoh Augustine Ekenedilichukwu"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "599",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "602",
        "paper_id": 80,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.80.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.80.jpg",
        "title": "CIC-NLP at MAHED 2025 TASK 1:Assessing the Role of Bigram Augmentation in Multiclass Arabic Hate and Hope Speech Classification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.80",
        "x": -1.5856742858886719,
        "y": 3.00492525100708,
        "year": "2025"
      },
      {
        "abstract": "Authorship identification in Arabic is a chal- lenging task due to the language\u2019s morpho- logical richness, orthographic variation, and stylistic diversity across genres and authors. In this paper, we present our submission to Subtask 2: Authorship Identification of the AraGenEval 2025 Shared Task at ArabicNLP, which aims to identify the author of a given Arabic paragraph among a set of 21 authors. This task is important for applications such as digital forensics, plagiarism detection, lit- erary analysis, and AI-generated content veri- fication, where reliably linking text to its au- thor can provide critical insights. We em- ploy transformer-based encoders and address the dataset\u2019s class imbalance by leveraging an ensemble of two capable Arabic language un- derstanding models: AraBERT and AraELEC- TRA. Our approach combines the pre-softmax logits of both models before the final soft- max layer, effectively capturing complemen- tary strengths in their predictions. Using our proposed method, we achieved third place on the Subtask 2 leaderboard of the AraGenEval Shared Task (Abudalfa et al., 2025), with a Macro-F1 score of 0.85968 and accuracy of 0.89516 on the test split.",
        "authors": [
          "Mohamed Amin",
          "Mahmoud Rady",
          "Mariam Hossam",
          "Sara Gaballa",
          "Eman Samir",
          "Maria Bassem",
          "Nisreen Hisham",
          "Ayman Khalafallah"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "54",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "58",
        "paper_id": 9,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.9.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.9.jpg",
        "title": "Athership at AraGenEval Shared Task: Identifying Arabic Authorship with a Dual-Model Logit Fusion",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.9",
        "x": -0.9133427143096924,
        "y": 6.6315131187438965,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our system for Sub-task 1 in MAHED 2025 (Zaghouani et al., 2025) shared task: Text-based Hate and Hope Speech Classification. We propose a robust pipeline built upon the bert-base-arabertv02-twitter model, leveraging domain-specific preprocess- ing, hyperparameter optimization with Optuna, and a K-Fold ensemble strategy. This system ranked 4 th among all participating teams on the leaderboard. We discuss technical design choices, the results of ablation studies, and the impact of preprocessing and model selection on final performance.",
        "authors": [
          "Nguy\u1ec5n Thi\u00ean B\u1ea3o",
          "Dang Van Thin"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "595",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "598",
        "paper_id": 79,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.79.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.79.jpg",
        "title": "LoveHeaven at MAHED 2025: Text-based Hate and Hope Speech Classification Using AraBERT-Twitter Ensemble",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.79",
        "x": -1.1658726930618286,
        "y": 1.7797973155975342,
        "year": "2025"
      },
      {
        "abstract": "In recent years, online social life has become an integral part of the global landscape, with so- cial media platforms enabling users to express a wide range of emotions and opinions. In the Arabic-speaking world, navigating the dual na- ture of content\u2014encompassing both hate and hope speech\u2014remains challenging due to lin- guistic and cultural complexities. The MAHED 2025 shared task at ArabicNLP 2025 addressed this by focusing on detecting both hate and hope speech in Arabic social media. This pa- per describes our approach for subtask 1, uti- lizing various machine learning, deep learn- ing, and transformer models for classification. AraBERT-large-v2 yielded the highest macro f1-score of 0.698, earning 8th place on the leaderboard.",
        "authors": [
          "Walisa Alam",
          "Mehreen Rahman",
          "Shawly Ahsan",
          "Mohammed Moshiul Hoque"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "700",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "705",
        "paper_id": 96,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.96.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.96.jpg",
        "title": "CUET_Zahra_Duo@Mahed 2025: Hate and Hope Speech Detection in Arabic Social Media Content using Transformer",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.96",
        "x": -1.6141196489334106,
        "y": 0.6343360543251038,
        "year": "2025"
      },
      {
        "abstract": "Image captioning aims to generate natural lan- guage descriptions of images, combining visual understanding with language generation. This task is particularly challenging in low-resource settings such as Arabic, where annotated data is limited and captions must reflect both cultural and linguistic nuances. In this system paper, we present our approach for the ImageEval 2025 Arabic Image Captioning Shared Task. Our system is based on the Qwen2.5-VL-7B vision- language model, enhanced with quality-aware data augmentation, a two-stage description-to- caption pipeline, and post-processing for im- proved fluency. In the official evaluation, our approach ranked first in the LLM as a Judge metric with a score of 33.97, second in Cosine Similarity with a score of 58.55, and first in the manual evaluation phase conducted by the organizers.",
        "authors": [
          "Mariam Saeed",
          "Sarah Elshabrawy",
          "Abdelrahman Hagrass",
          "Mazen Yasser",
          "Ayman Khalafallah"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "432",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "437",
        "paper_id": 59,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.59.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.59.jpg",
        "title": "Averroes at ImageEval 2025 Shared Task: Advancing Arabic Image Captioning with Augmentation and Two-Stage Generation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.59",
        "x": -3.1510956287384033,
        "y": 1.3711540699005127,
        "year": "2025"
      },
      {
        "abstract": "We introduce AraHealthQA 2025, the Com- prehensive Arabic Health Question Answer- ing Shared Task, held in conjunction with ArabicNLP 2025 (co-located with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic medical QA resources by offering two complementary tracks: Men- talQA, focusing on Arabic mental health Q&A (e.g., anxiety, depression, stigma reduction), and MedArabiQ, covering broader medical do- mains such as internal medicine, pediatrics, and clinical decision making. Each track comprises multiple subtasks, evaluation datasets, and stan- dardized metrics, facilitating fair benchmark- ing. The task was structured to promote mod- eling under realistic, multilingual, and cultur- ally nuanced healthcare contexts. We outline the dataset creation, task design and evaluation framework, participation statistics, baseline sys- tems, and summarize the overall outcomes. We conclude with reflections on the performance trends observed and prospects for future itera- tions in Arabic health QA1.",
        "authors": [
          "Hassan Alhuzali",
          "Farah E. Shamout",
          "Muhammad Abdul-Mageed",
          "Chaimae Abouzahir",
          "Mouath Abu Daoud",
          "Ashwag Alasmari",
          "Walid Al-Eisawi",
          "Renad Al-Monef",
          "Ali Alqahtani",
          "Lama Ayash",
          "Nizar Habash",
          "Leen Kharouf"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "107",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "118",
        "paper_id": 18,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.18.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.18.jpg",
        "title": "AraHealthQA 2025: The First Shared Task on Arabic Health Question Answering",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.18",
        "x": -1.5272201299667358,
        "y": 0.5018603801727295,
        "year": "2025"
      },
      {
        "abstract": "Preserving the integrity of Qur\u2019anic recitation requires accurate pronunciation, as even subtle mispronunciations can alter meaning. Auto- matic assessment of Qur\u2019anic recitation at the phoneme level is therefore a critical and chal- lenging task. We present ShallowTransformer, a lightweight and computationally efficient transformer model leveraging Wav2vec2.0 fea- tures and trained with CTC loss for phoneme- level mispronunciation detection. Evaluated on the Iqra\u2019Eval benchmark (QuranMB.v2), our model outperforms published BiLSTM base- lines on QuranMB.v1 while achieving com- petitive performance relative to the official Iqra\u2019Eval challenge baselines, which are not yet fully documented. Such improvements are particularly important in assisted Qur\u2019an learning, as accurate phonetic feedback sup- ports correct recitation and preserves textual integrity. These results highlight the effective- ness of transformer architectures in capturing subtle pronunciation errors while remaining de- ployable for practical applications.",
        "authors": [
          "Mohamed Nadhir Daoud",
          "Mohamed Anouar Ben Messaoud"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "457",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "463",
        "paper_id": 63,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.63.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.63.jpg",
        "title": "Phoneme-level mispronunciation detection in Quranic recitation using ShallowTransformer",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.63",
        "x": -2.7579293251037598,
        "y": 0.5094278454780579,
        "year": "2025"
      },
      {
        "abstract": "We present the results and findings of the BAREC Shared Task 2025 on Arabic Read- ability Assessment, organized as part of the Third Arabic Natural Language Processing Conference (ArabicNLP 2025). The BAREC 2025 shared task focuses on automatic read- ability assessment using the BAREC Corpus (Elmadani et al., 2025), addressing fine-grained classification into 19 readability levels. The shared task includes two sub-tasks: sentence- level classification and document-level classifi- cation, and three tracks: (1) Strict Track, where only the BAREC Corpus is allowed; (2) Con- strained Track, restricted to the BAREC Cor- pus, SAMER Corpus (Alhafni et al., 2024), and SAMER Lexicon (Al Khalil et al., 2020), and (3) Open Track, allowing any external re- sources. A total of 22 teams from 12 countries registered for the task. Among these, 17 teams submitted system description papers. The win- ning team achieved 87.5 QWK on the sentence- level task and 87.4 QWK on the document-level task.1",
        "authors": [
          "Khalid N. Elmadani",
          "Bashar Alhafni",
          "Hanada Taha",
          "Nizar Habash"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "239",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "252",
        "paper_id": 34,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.34.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.34.jpg",
        "title": "BAREC Shared Task 2025 on Arabic Readability Assessment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.34",
        "x": -0.8377418518066406,
        "y": 6.841507911682129,
        "year": "2025"
      },
      {
        "abstract": "Collaborative approaches have proven effec- tive in addressing complex problems, from hu- man and socio-economic challenges to multi- agent systems. These methods rely on the principle that combining perspectives enhances problem-solving. In this paper, we propose a collaborative large language models (LLM) framework to solve Islamic inheritance prob- lems, which demand precise mathematical rea- soning and strict adherence to legal rules for fair distribution among heirs. The system im- plements a collaborative voting mechanism involving multiple LLMs, namely ALLaM- 7B-Instruct-preview, Deepseek-reasoner, and Gemini-2.5-Flash. Each independently an- swered multiple-choice inheritance questions. The final answer is determined by majority vote. To improve accuracy and domain grounding, we integrate Retrieval-Augmented Generation (RAG). A curated database of solved inheri- tance cases in JSON format is indexed using TF-IDF. For each query, the most similar cases are retrieved and appended as contextual in- formation to the prompt before being submit- ted to the LLMs. Experimental results demon- strate that this collaborative RAG-enhanced framework outperforms individual LLMs. The ensemble achieved 88% accuracy, surpassing the best-performing single models: the fine- tuned ALLaM-7B-Instruct-preview (79.50%), Deepeek-reasoner (71.80%), and Gemini-2.5- Flash (83.50%).",
        "authors": [
          "Jihad R\u2019baiti",
          "Chouaib El Hachimi",
          "Youssef Hmamouche",
          "Amal Seghrouchni"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "947",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "952",
        "paper_id": 131,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.131.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.131.jpg",
        "title": "MorAI at QIAS 2025: Collaborative LLM via Voting and Retrieval-Augmented Generation for Solving Complex Inheritance Problems",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.131",
        "x": -1.7374448776245117,
        "y": 2.8750131130218506,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our approach to the Ara- GenEval 2025 shared task on Arabic author- ship attribution (Task 2). We developed an enhanced traditional machine learning system that combines word-level and character-level TF-IDF features with multiple classification algorithms. Our system achieved 88.90% accu- racy and 82.74% macro F1-score on the official test set using Logistic Regression. During de- velopment, we evaluated multiple models on the validation set, where Linear SVM achieved the highest performance with 93.22% accuracy and 87.52% macro F1-score. The approach demonstrates the effectiveness of feature engi- neering and proper text preprocessing for Ara- bic authorship attribution tasks without relying on deep learning architectures.",
        "authors": [
          "Amr Sabaa",
          "Mohamed Sabaa"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "32",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "36",
        "paper_id": 5,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.5.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.5.jpg",
        "title": "Amr&MohamedSabaa at AraGenEval shared task: Arabic Authorship Identification using Term Frequency \u2013 Inverse Document Frequency Features with Supervised Machine Learning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.5",
        "x": -2.708378553390503,
        "y": 2.4015536308288574,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our submission to the Ara- HealthQA 2025 shared task (Alhuzali et al., 2025), Sub-task 3: Arabic Mental Health Ques- tion Answering. We evaluated four large lan- guage models\u2014GPT-4o, Gemini, Allam, and Qwen\u2014using various prompting strategies. A simple 3-shot prompt, instructing the model to respond in Arabic, consistently outperformed zero-shot, 5-shot, and more complex meth- ods. GPT-4o achieved the best results, with a BERTScore F1 of 0.670 on the official hid- den test set, ranking 2nd overall. The system required no fine-tuning or external data, relying solely on prompt design and consistent evalua- tion.",
        "authors": [
          "Nejood Abdulaziz Bin Eshaq"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "149",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "154",
        "paper_id": 22,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.22.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.22.jpg",
        "title": "MindLLM at AraHealthQA 2025 Track 1: Leveraging Large Language Models for Mental Health Question Answering",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.22",
        "x": -4.0828070640563965,
        "y": 0.27625057101249695,
        "year": "2025"
      },
      {
        "abstract": "We present ADAPT\u2013MTU HAI\u2019s submission to Subtask 1 of the QIAS 2025 shared task, which focuses on Arabic multiple-choice question answer- ing (MCQ) for Islamic inheritance law. This do- main presents unique challenges, requiring models to navigate precise fractional computations, exclu- sion rules, and doctrinal nuances under strict for- mat constraints. Our system employs a dual-expert architecture based on ALLaM-7B, integrating a LoRA-\ufb01ne-tuned model specialised for inheritance reasoning with its generalist base counterpart. A custom constrained decoding mechanism ensures output compliance, while arbitration between the two models enhances answer stability. Our system achieves 60.0% accuracy on the development set and 54.7% on the o\ufb03cial blind test set\u2014substan- tially improving upon the baseline. We analyse common failure modes and discuss implications for structured legal reasoning using large language models.",
        "authors": [
          "Shehenaz Hossain",
          "Haithem Afli"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "923",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "928",
        "paper_id": 127,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.127.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.127.jpg",
        "title": "ADAPT\u2013MTU HAI at QIAS2025: Dual-Expert LLM Fine-Tuning and Constrained Decoding for Arabic Islamic Inheritance Reasoning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.127",
        "x": -1.008868932723999,
        "y": 1.921907663345337,
        "year": "2025"
      },
      {
        "abstract": "This paper presents the MAHED 2025 Shared Task on Multimodal Detection of Hope and Hate Emotions in Arabic Content, comprising three subtasks: (1) text-based classification of Arabic content into hate and hope,(2) multi- task learning for joint prediction of emotions, offensive content, and hate speech and (3) mul- timodal detection of hateful content in Arabic memes. We provide three high-quality datasets totaling over 22,000 instances sourced from social media platforms, annotated by native Arabic speakers with Cohen\u2019s Kappa exceed- ing 0.85. Our evaluation attracted 46 leader- board submissions from participants, with sys- tems leveraging Arabic-specific pre-trained lan- guage models (AraBERT, MARBERT), large language models (GPT-4, Gemini), and mul- timodal fusion architectures combining CLIP vision encoders with Arabic text models. The best-performing systems achieved macro F1- scores of 0.723 (Task 1), 0.578 (Task 2), and 0.796 (Task 3), with top teams employing en- semble methods, class-weighted training, and OCR-aware multimodal fusion. Analysis re- veals persistent challenges in dialectal robust- ness, minority class detection for hope speech, and highlights key directions for future Arabic content moderation research.",
        "authors": [
          "Wajdi Zaghouani",
          "Md. Rafiul Biswas",
          "Mabrouka Bessghaier",
          "Shimaa Ibrahim",
          "George Mikros",
          "Abul Hasnat",
          "Firoj Alam"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "560",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "574",
        "paper_id": 75,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.75.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.75.jpg",
        "title": "MAHED Shared Task: Multimodal Detection of Hope and Hate Emotions in Arabic Content",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.75",
        "x": -0.7585858106613159,
        "y": 6.905617713928223,
        "year": "2025"
      },
      {
        "abstract": "We presentFAHMNI, a unified system for Ara- bic mental-health question answering devel- oped for the AraHealthQA 2025 MentalQA Shared Task (Track 1).FAHMNIevaluates large language models (LLMs) on all subtasks: (1) multi-label classification of question types and (2) answer strategies, and (3) grounded an- swer generation. For Subtasks 1\u20132, we sys- tematically compare Arabic-capable LLM fam- ilies (Qwen3, SILMA) under zero-shot and few-shot prompting, few-shot learning with a frozen backbone, parameter-efficient fine- tuning (PEFT), and instruction tuning. To sup- port Subtask 3, we implement a multi-agent, retrieval-augmented generation pipeline that routes queries between curated domain sources and controlled web search; an answer-style controller predicts the required strategy (In- formation, Direct Guidance, Emotional Sup- port) and conditions the generator accord- ingly. Our best LLM configurations reach 0.507/0.404 (weighted-F1/Jaccard) on Subtask 1 with Qwen3+PEFT and 0.750/0.600 on Sub- task 2 with SILMA+PEFT, while a strong fine- tuned MARBERT baseline remains competi- tive at 0.541/0.494 (Subtask 1) and 0.805/0.727 (Subtask 2). For Subtask 3, our multi-agent RAG system with SILMA attains an 0.652 BERTScore F1 and yields a 0.06 hallucination rate under our manual audit. These findings highlight both the viability and current limits of Arabic-capable LLMs for mental-health QA, and they motivate grounded, style-aware gener- ation as a practical path for safe deployment",
        "authors": [
          "Caroline Sabty",
          "Mohamad Rasmy",
          "Mohamed Eyad Badran",
          "Nourhan Sakr",
          "Alia El Bolock"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "204",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "212",
        "paper_id": 29,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.29.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.29.jpg",
        "title": "Fahmni at AraHealthQA Track 1: Multi-Agent Retrieval-Augmented Generation and Multi-Label Classification for Arabic Mental Health Q&A",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.29",
        "x": -1.664096474647522,
        "y": 0.6514933705329895,
        "year": "2025"
      },
      {
        "abstract": "In this paper, we present our contribution to the IslamicEval 2025 shared task. More specifi- cally, we address subtask 2, which is a passage retrieval (PR) system for Qur\u2019an and Hadith, the two central bodies of text in Islam. Bas- ing off of a fine-tuned BERT-based sentence transformer retrieval model, we explore several approaches, including pipelined fine-tuning of cross-encoders, as well as using a state- of-the-art LLM for reranking of relevant pas- sages, and identification of zero-answer ques- tions. Our best-performing system achieves a MAP@10 of 0.1809, MAP_Q@5 of 0.2334, and MAP_H@5 of 0.1923 on the test set.",
        "authors": [
          "Serag Amin",
          "Ranwa Aly",
          "Yara Allam",
          "Yomna Eid",
          "Ensaf Hussein Mohamed"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "494",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "502",
        "paper_id": 68,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.68.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.68.jpg",
        "title": "NUR at IslamicEval 2025 Shared Task: Retrieval-Augmented LLMs for Qur\u2019an and Hadith QA",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.68",
        "x": -2.0926051139831543,
        "y": 2.521481990814209,
        "year": "2025"
      },
      {
        "abstract": "In this paper, we present our system for Sub- task 1 of the MAHED 2025 shared task, which involves classifying Arabic text into three categories: Hate, Hope, and not_applicable. Our methodology integrates XLM-RoBERTa embeddings with supervised ML and deep learning techniques. After applying Arabic- specific preprocessing, we extract contextual embeddings and mitigate class imbalance using SMOTE . We then train LR and LSTM classi- fiers on the augmented features space, supple- mented by a similarity calculation with Zero- Shot for prediction validation. The system was evaluated in two phases: using the initial val- idation set, and the official updated datasets. Results show competitive performance, partic- ularly in boosting recall for minority classes with a macro score of 0.60.",
        "authors": [
          "Yasmine El Abed",
          "Mariem Ben Arbia",
          "Saoussen Ben Chaabene",
          "Omar Trigui"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "639",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "644",
        "paper_id": 87,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.87.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.87.jpg",
        "title": "ANLP-UniSo at MAHED Shared Task: Detection of Hate and Hope Speech in Arabic Social Media based on XLM-RoBERTa and Logistic Regression",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.87",
        "x": -5.159230709075928,
        "y": 0.9831990003585815,
        "year": "2025"
      },
      {
        "abstract": "Authorship identification and AI-generated text detection have recently emerged as pivotal ar- eas of research in natural language processing (NLP), with particular urgency for languages such as Arabic that exhibit complex morpho- logical and orthographic structures. Despite growing interest, most prior work has cen- tered on English and other Indo-European lan- guages, leaving a gap in effective approaches tailored to Arabic\u2019s linguistic challenges. This paper presents our participation in two shared tasks: Arabic authorship identification and Ara- bic AI-generated text detection. For Task2, we fine-tuned transformer-based architectures on a corpus of 21 authors, leveraging paral- lelized, semantically segmented book data to better capture stylistic variation. For Task3, we trained models on a balanced dataset of human-written and AI-generated news arti- cles produced by multiple large language mod- els. Our approach achieved competitive results across both tasks, underscoring the potential of domain-adapted transformers for morpholog- ically rich languages. We also highlight key limitations, including domain sensitivity and difficulties in distinguishing closely aligned stylistic features, and propose directions for enhancing cross-domain robustness and gener- alization.",
        "authors": [
          "Sadia Tasnim Meem",
          "Azmine Toushik Wasi"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "77",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "81",
        "paper_id": 13,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.13.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.13.jpg",
        "title": "CIOL at AraGenEval shared task: Authorship Identification and AI Generated Text Detection in Arabic using Pretrained Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.13",
        "x": -3.7880642414093018,
        "y": 2.037607431411743,
        "year": "2025"
      },
      {
        "abstract": "We present a simple, model-agnostic post- processing technique for fine-grained Arabic readability classification in the BAREC 2025 Shared Task (19 ordinal levels). Our method applies conformal prediction to generate pre- diction sets with coverage guarantees, then computes weighted averages using softmax- renormalized probabilities over the confor- mal sets. This uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing high-penalty misclassifications to nearer levels. Our approach shows consistent QWK improvements of 1-3 points across dif- ferent base models. In the strict track, our sub- mission achieves QWK scores of 84.9%(test) and 85.7% (blind test) for sentence level, and 73.3% for document level. For Arabic educa- tional assessment, this enables human review- ers to focus on a handful of plausible levels, combining statistical guarantees with practical usability.",
        "authors": [
          "Ahmed Abdou"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "312",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "319",
        "paper_id": 44,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.44.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.44.jpg",
        "title": "mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.44",
        "x": -1.8564214706420898,
        "y": 0.5321341753005981,
        "year": "2025"
      },
      {
        "abstract": "We present ImageEval 2025, the first shared task dedicated to Arabic image captioning. The task addresses the critical gap in multi- modal Arabic NLP by focusing on two com- plementary subtasks: (1) creating the first open-source, manually-captioned Arabic im- age dataset through a collaborative datathon, and (2) developing and evaluating Arabic im- age captioning models. A total of 44 teams registered, of which eight submitted during the test phase, producing 111 valid submis- sions. Evaluation was conducted using au- tomatic metrics, LLM-based judgment, and human assessment. In Subtask 1, the best- performing system achieved a cosine similarity of 65.5, while in Subtask 2, the top score was 60.0. Although these results show encouraging progress, they also confirm that Arabic image captioning remains a challenging task, partic- ularly due to cultural grounding requirements, morphological richness, and dialectal variation. All datasets, baseline models, and evaluation tools are released publicly to support future research in Arabic multimodal NLP.",
        "authors": [
          "Ahlam Bashiti",
          "Alaa Aljabari",
          "Hadi Khaled Hamoud",
          "Md. Rafiul Biswas",
          "Bilal Mohammed Shalash",
          "Mustafa Jarrar",
          "Fadi Zaraket",
          "George Mikros",
          "Ehsaneddin Asgari",
          "Wajdi Zaghouani"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "376",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "389",
        "paper_id": 52,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.52.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.52.jpg",
        "title": "ImageEval 2025: The First Arabic Image Captioning Shared Task",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.52",
        "x": -4.016087055206299,
        "y": 0.6016203761100769,
        "year": "2025"
      },
      {
        "abstract": "Automatic speech recognition (ASR) plays a vital role in enabling natural human\u2013machine interaction across applications such as virtual assistants, industrial automation, customer sup- port, and real-time transcription. However, developing accurate ASR systems for low- resource languages like Arabic remains a signif- icant challenge due to limited labeled data and the linguistic complexity introduced by diverse dialects. In this work, we present a scalable training pipeline that combines weakly super- vised learning with supervised fine-tuning to develop a robust Arabic ASR model. In the first stage, we pretrain the model on 15,000 hours of weakly labeled speech covering both Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the subsequent stage, we perform continual supervised fine-tuning using a mixture of filtered weakly labeled data and a small, high-quality annotated dataset. Our approach achieves state-of-the-art results, ranking first in the multi-dialectal Arabic ASR challenge. These findings highlight the effec- tiveness of weak supervision paired with fine- tuning in overcoming data scarcity and deliver- ing high-quality ASR for low-resource, dialect- rich languages.",
        "authors": [
          "Mahmoud Salhab",
          "Shameed Sait",
          "Mohammad Abusheikh",
          "Hasan Abusheikh"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "734",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "739",
        "paper_id": 100,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.100.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.100.jpg",
        "title": "Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.100",
        "x": -2.9526121616363525,
        "y": 4.706427097320557,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our approach to Sub- task 2 of IslamicEval 2025, a shared task that involves retrieving relevant passages from Quranic verses and Sahih Bukhari hadiths to answer Modern Standard Arabic (MSA) ques- tions. We developed a multi-pipeline hybrid system that combines three complementary approaches: \ufb01ne-tuned embedding models us- ing triplet loss, keyword-based fuzzy match- ing, and large language model guided retrieval. Our system achieved MAP_@10 of 0.2296, MAP_Q@5 of 0.2623, and MAP_H@5 of 0.215 in the test set, demonstrating the effectiveness of combining multiple retrieval strategies for Arabic religious text question answering.",
        "authors": [
          "Eman Elrefai",
          "Toka Khaled",
          "Ahmed Soliman"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "528",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "533",
        "paper_id": 72,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.72.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.72.jpg",
        "title": "ThinkDrill at IslamicEval 2025 Shared Task: LLM Hybrid Approach for Qur\u2019an and Hadith Question Answering",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.72",
        "x": -6.996438980102539,
        "y": 1.1092591285705566,
        "year": "2025"
      },
      {
        "abstract": "This paper presents a Chain-of-Thought (CoT) prompting approach for Islamic inheritance reasoning in multiple-choice question answer- ing. We address the QIAS 2025 SubTask 1, which requires complex legal reasoning to de- termine correct inheritance shares according to Islamic jurisprudence. Our system employs two prompting strategies: direct answer ex- traction and step-by-step reasoning with regex- based answer extraction. We evaluate our ap- proach using Claude 3.7 Sonnet and GPT-4o on Islamic inheritance MCQ tasks. Results demonstrate significant performance improve- ments when incorporating the thinking step: Claude 3.7 improved from 0.67 to 0.81, and GPT-4o from 0.63 to 0.74. Error analysis re- veals that while models perform well in ba- sic reasoning, they struggle with complex cor- rection procedures (Tasheeh1) in inheritance calculations. Our findings confirm that struc- tured reasoning substantially enhances LLM performance on complex Arabic legal reason- ing tasks without requiring additional training or retrieval-augmented generation.",
        "authors": [
          "Serry Sibaee",
          "Mahmoud Reda",
          "Omer Nacar",
          "Yasser Alhabashi",
          "Adel Ammar",
          "Wadii Boulila"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "873",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "877",
        "paper_id": 120,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.120.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.120.jpg",
        "title": "ANLPers at QIAS: CoT for Islamic Inheritance",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.120",
        "x": -4.60050106048584,
        "y": 1.4365448951721191,
        "year": "2025"
      },
      {
        "abstract": "This paper describes the approach developed for the AraGenEval shared task, with a fo- cus on Arabic authorship identification and AI-generated text detection. Transformer- based models, including ALLaM-7B-Instruct- preview for Subtask 2 and AraModernBERT for Subtask 3, were fine-tuned using both the official and additional datasets. Prompt engi- neering and transfer learning techniques were adapted to address challenges specific to the Arabic language. Competitive performance was achieved on both subtasks, and all code and resources have been made publicly avail- able to facilitate reproducibility. Arabic NLP, Authorship Identification, AI- generated Text Detection, Transformer Models, Prompt Engineering, ALLaM, AraModernBERT",
        "authors": [
          "Thamer Maseer Alharbi"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "14",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "17",
        "paper_id": 2,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.2.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.2.jpg",
        "title": "MISSION at AraGenEval Shared Task: Enhanced Arabic Authority Classification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.2",
        "x": -2.9981539249420166,
        "y": 2.2189583778381348,
        "year": "2025"
      },
      {
        "abstract": "We present our systems for Track 2 (Gen- eral Arabic Health QA, MedArabiQ) of the AraHealthQA-2025 shared task, where our methodology secured 2nd place in both Sub- Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended question answer- ing) in Arabic clinical contexts. For Sub-Task 1, we leverage the Gemini 2.5 Flash model with few-shot prompting, dataset preprocess- ing, and an ensemble of three prompt con- figurations to improve classification accuracy on standard, biased, and fill-in-the-blank ques- tions. For Sub-Task 2, we employ a unified prompt with the same model, incorporating role-playing as an Arabic medical expert, few- shot examples, and post-processing to gener- ate concise responses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased variants.",
        "authors": [
          "Mohamed Younes",
          "Seif Ahmed",
          "Mohamed Basem"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "176",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "183",
        "paper_id": 25,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.25.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.25.jpg",
        "title": "!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.25",
        "x": -6.813526153564453,
        "y": 1.114601969718933,
        "year": "2025"
      },
      {
        "abstract": "We present our system for TAQEEM 2025 Task A on Arabic automatic essay scoring. Build- ing on a pretrained Arabic encoder, our work focuses on two key design axes: (i) replac- ing the standard linear head with a lightweight multi-layer perceptron (MLP) and (ii) optimiz- ing with distribution-aware objectives. We introduce a Weighted Mean-Squared Error loss, which assigns higher weights to less fre- quent scores to counteract the imbalanced, bell- shaped score distribution of the training data. On the official development folds, our sys- tem outperforms the baseline on Quadratic Weighted Kappa. Our findings underscore the importance of tailoring objective functions to specific data characteristics for achieving state- of-the-art results in AES.",
        "authors": [
          "Trong-Tai Dam Vu",
          "Th\u00ecn \u0110\u1eb7ng V\u0103n"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "983",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "988",
        "paper_id": 136,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.136.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.136.jpg",
        "title": "912 at TAQEEM 2025: A Distribution-aware Approach to Arabic Essay Scoring",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.136",
        "x": -3.119304656982422,
        "y": 0.7721952199935913,
        "year": "2025"
      },
      {
        "abstract": "This paper presents the MarsadLab submission to Track 1 of the AraHealthQA 2025 Shared Task, addressing two subtasks: (A) multi-label question categorization and (B) multi-label an- swer categorization in Arabic mental health discourse. Our approach employs a hybrid contextual\u2013lexical fusion architecture built on AraBERTv2, enriched with task-specific hand- crafted features such as lexical indicators, lin- guistic cues, and domain-informed keyword signals. On the official test set, the system achieved a weighted F1 score of 0.55 (Jaccard 0.41) for Task A and 0.79 (Jaccard 0.67) for Task B.",
        "authors": [
          "Mabrouka Bessghaier",
          "Shimaa Ibrahim",
          "Md. Rafiul Biswas",
          "Wajdi Zaghouani"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "233",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "238",
        "paper_id": 33,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.33.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.33.jpg",
        "title": "MarsadLab at AraHealthQA: Hybrid Contextual\u2013Lexical Fusion with AraBERT for Question and Answer Categorization",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.33",
        "x": -2.755004644393921,
        "y": 2.3996999263763428,
        "year": "2025"
      },
      {
        "abstract": "Mispronunciation detection at the phoneme level provides detailed feedback for Quranic reciters. Standard speech-to-text models cannot capture sub- tle differences in letter pronunciation; thus, de- veloping a speech-to-phoneme system is essential. Prior works have mainly explored encoder-only models. In this work, we adapt Whisper-large-v3 on the IqraEval dataset. Our experimental results show that the proposed system achieved an F1- score of 0.3224, an accuracy of 0.6894, and a high recall of 0.7624. These results highlight promising directions for further research and development in phoneme-level mispronunciation detection.",
        "authors": [
          "Nour Qandos",
          "Serry Sibaee",
          "Samar Ahmad",
          "Omer Nacar",
          "Adel Ammar",
          "Wadii Boulila",
          "Yasser Alhabashi"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "464",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "468",
        "paper_id": 64,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.64.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.64.jpg",
        "title": "ANPLers at IqraEval Shared task: Adapting Whisper-large-v3 as Speech-to-Phoneme for Qur\u2019anic Recitation Mispronunciation Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.64",
        "x": -3.124802589416504,
        "y": 3.9017717838287354,
        "year": "2025"
      },
      {
        "abstract": "We present a visual-language approach to Ara- bic readability assessment using the PIXEL Vi- sion Transformer, which processes rendered text as images to bypass tokenization chal- lenges. Our system participated in the BAREC 2025 Shared Task (Sentence-level Strict track). We evaluate orthographic variants (normaliza- tion, diacritization, transliteration) and mor- phological segmentation with different visual boundary markers. Results show that diacritiza- tion provides useful visual cues for disambigua- tion, morphological segmentation improves over word-level processing, and transliterated scripts outperform native Arabic script. Our approach demonstrates the potential of visual processing for readability assessment in com- plex languages and writing systems.",
        "authors": [
          "Ben Sapirstein"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "350",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "356",
        "paper_id": 48,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.48.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.48.jpg",
        "title": "Pixels at BAREC Shared Task 2025: Visual Arabic Readability Assessment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.48",
        "x": -3.005535125732422,
        "y": 3.971147060394287,
        "year": "2025"
      },
      {
        "abstract": "This paper describes our system for the BAREC 2025 Shared Task on Arabic Readability As- sessment. Our approach is centered on a hy- brid model that combines the deep contextual representations of a pre-trained transformer (AraBERT v02) with a rich set of engineered linguistic features. We extracted over 200 lex- ical, morphological, syntactic, and semantic features, which were refined to the 100 most informative ones through a multi-stage selec- tion process. Our final model demonstrates sig- nificant effectiveness, achieving a Quadratic Weighted Kappa (QWK) of 82.7% and an ex- act accuracy of 57.6% on the official blind test set. These results highlight the powerful syn- ergy between transformer-based embeddings and explicit linguistic signals for the nuanced task of assessing Arabic text readability.",
        "authors": [
          "Ahmed Alhassan",
          "Asim Mohamed",
          "Moayad Elamin"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "357",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "361",
        "paper_id": 49,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.49.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.49.jpg",
        "title": "Phantoms at BAREC Shared Task 2025: Enhancing Arabic Readability Prediction with Hybrid BERT and Linguistic Features",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.49",
        "x": -3.010685443878174,
        "y": 2.460638999938965,
        "year": "2025"
      },
      {
        "abstract": "This paper describes AraS2P, our speech-to- phonemes system submitted to the Iqra\u2019Eval 2025 Shared Task. We adapted Wav2Vec2- BERT via Two-Stage training strategy. In the first stage, task-adaptive continue pretraining was performed on large-scale Arabic speech- phonemes datasets, which were generated by converting the Arabic text using the MSA Phonetiser. In the second stage, the model was fine-tuned on the official shared task data, with additional augmentation from XTTS-v2- synthesized recitations featuring varied Ayat segments, speaker embeddings, and textual per- turbations to simulate possible human errors. The system ranked first on the official leader- board, demonstrating that phoneme-aware pre- training combined with targeted augmentation yields strong performance in phoneme-level mispronunciation detection.",
        "authors": [
          "Bassam Mattar",
          "Mohamed Fayed",
          "Ayman Khalafallah"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "469",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "474",
        "paper_id": 65,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.65.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.65.jpg",
        "title": "AraS2P: Arabic Speech-to-Phonemes System",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.65",
        "x": -2.222221612930298,
        "y": 1.9472349882125854,
        "year": "2025"
      },
      {
        "abstract": "We present a simple, training-light pipeline for multi-label categorization of Arabic mental-health questions in the AraHealthQA 2025 MentalQA Track 1 (question and an- swer classification). Our method, Ex- plain\u2013Retrieve\u2013Verify (ERV), couples a chain- of-thought LLM classifier with example-based retrieval and a verifier that arbitrates disagree- ments. The LLM first proposes candidate la- bels and rationales from a compact taxonomy prompt. A similarity agent then surfaces top- k nearest questions via multilingual sentence- transformer embeddings to induce case-based priors. A verification agent reconciles both signals to produce a final label set with a cal- ibrated confidence, followed by a lightweight post-processor for code parsing and confidence clamping. ERV requires no fine-tuning or exter- nal data and runs efficiently at inference time. In shared-task evaluation, our system achieved 0.61 weighted F1-score for question classifi- cation and 0.73 for answer classification. A hybrid approach combining ERV with MAR- BERT further improves answer classification to 0.80 weighted F1-score.",
        "authors": [
          "Ahmed Abdou"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "226",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "232",
        "paper_id": 32,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.32.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.32.jpg",
        "title": "mucAI at AraHealthQA 2025: Explain\u2013Retrieve\u2013Verify (ERV) Workflow for Multi-Label Arabic Health QA Classification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.32",
        "x": -6.561615467071533,
        "y": 1.451432466506958,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our system submitted to TAQEEM 2025, which designed to address two tasks: (A) holistic scoring and (B) trait-specific scoring. We propose a GPT-4o-based methodol- ogy that employs few-shot prompting to serve as a grader for both tasks. Specifically, for task A, we utilize prompt-based scoring criteria with exemplars to assess overall essay qual- ity. For task B, we design trait-specific prompt- ing schemes to capture fine-grained grading as- pects. Our system attains substantial agreement on Task A (QWK = 0.75) and a mean QWK of 0.65 across traits for task B, outperforming the shared task baseline on both tasks.",
        "authors": [
          "Nada Almarwani",
          "Alaa Alharbi",
          "Samah Aloufi"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "989",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "997",
        "paper_id": 137,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.137.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.137.jpg",
        "title": "Taibah at TAQEEM 2025: Leveraging GPT-4o for Arabic Essay Scoring",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.137",
        "x": -2.7946834564208984,
        "y": 0.8441181778907776,
        "year": "2025"
      },
      {
        "abstract": "Mental health question-answering (Men- talQA) is essential for delivering accessible and reliable mental health support. Natural language processing (NLP) techniques are increasingly integral to such systems, enabling automated categorization of questions and answers to improve information retrieval, response accuracy, and user guidance. In AraHealthQA 2025 (Track 1), we addressed two subtasks: multi-label question cate- gorization and answer categorization. We proposed an XLMR-Arabic pipeline en- hanced with a two-stage data augmentation strategy, combining large language model (LLM)-based paraphrasing with synthetic label merging. Additionally, we evaluated the effectiveness of \ufb01ne-tuned multilingual transformers, LLMs adapted with low-rank adaptation (LoRA), and LLMs under few-shot settings. Experimental results show that XLMR-Arabic achieved the best performance, reaching Jaccard scores of 53% and 77.44% on Subtasks 1 and 2, respectively, ranking our team second in both tracks.",
        "authors": [
          "Sajib Bhattacharjee",
          "Ratnajit Dhar",
          "Kawsar Ahmed",
          "Mohammed Moshiul Hoque"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "164",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "175",
        "paper_id": 24,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.24.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.24.jpg",
        "title": "Binary_Bunch at AraHealthQA Track 1: Arabic Mental Health Q&A Classification Using Data Augmentation and Transformer Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.24",
        "x": -4.641455173492432,
        "y": 0.2854682207107544,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our approach to the Qur\u2019an and Hadith QA task in the IslamicEval 2025 Shared Task. Reliable retrieval requires both accuracy and context-aware answers from Qur\u2019anic and Hadith text. To address this chal- lenge, We combine semantic search with LLM- based re-ranking. To enhance alignment, we augment the corpus with LLM-extracted Is- lamic facts and paraphrased queries. An LLM- based binary classifier further verifies whether retrieved passages answer the questions. Re- sults show improved accuracy and better align- ment with user intent.",
        "authors": [
          "Mohammad Basheer",
          "Watheq Mansour",
          "Abdulhamid Touma",
          "Ahmad Qadeib Alban"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "534",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "539",
        "paper_id": 73,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.73.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.73.jpg",
        "title": "Burhan at IslamicEval: Fact-Augmented and LLM-Driven Retrieval for Islamic QA",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.73",
        "x": -6.858002185821533,
        "y": 1.237896203994751,
        "year": "2025"
      },
      {
        "abstract": "This paper presents Lahjati (ECAPA-WavLM Fusion with Multi-Stage Optimization) system for the spoken Arabic Dialect Identification (ADI) subtask at Nadi 2025 (Talafha et al., 2025), The task aims to automatically identify the dialect of spoken Arabic utterances, a chal- lenging problem due to the rich linguistic diver- sity of Arabic and the scarcity of labeled speech resources. Our approach combines ECAPA- TDNN embeddings from SpeechBrain with WavLM-base representations. The proposed system achieved 94.08% accuracy on the val- idation set and ~51.0% on the test set. Chal- lenges included differentiating acoustically sim- ilar dialect pairs and mitigating the effects of varied recording conditions, which likely con- tributed to performance degradation on unseen data. These findings highlight both the poten- tial and limitations of fusing complementary speech representations for robust dialect identi- fication.",
        "authors": [
          "Sanad Albawwab",
          "Omar Qawasmeh"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "740",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "744",
        "paper_id": 101,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.101.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.101.jpg",
        "title": "Lahjati at NADI 2025 A ECAPA-WavLM Fusion with Multi-Stage Optimization",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.101",
        "x": -3.010679006576538,
        "y": 4.185847282409668,
        "year": "2025"
      },
      {
        "abstract": "This paper details the ImageEval 2025 Shared Task on Arabic image captioning. We designed a two-step, zero-shot framework that utilises the BLIP multimodal vision-language model to first generate English captions. These captions are then converted to Arabic via the M2M100 multilingual translation model. We tested the full pipeline on the official ImageEval 2025 benchmarking set, obtaining a cosine similarity of 0.383 and an LLM Judge score of 15.14. The corroborating numerical and qualitative find- ings confirm the viability of a translation-driven methodology for cross-lingual image caption- ing in Arabic, a language often classified as low-resource. Nonetheless, the experiments also uncovered weaknesses: subtle semantic layers and culturally specific references are in- adequately conveyed in the output and merit focused attention in subsequent iterations.",
        "authors": [
          "Abdulkadir Shehu Bichi"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "390",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "394",
        "paper_id": 53,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.53.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.53.jpg",
        "title": "Codezone Research Group at ImageEval Shared-Task 2: Arabic Image Captioning Using BLIP and M2M100: A Two-Stage Translation Approach for ImageEval 2025",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.53",
        "x": -4.562428951263428,
        "y": 4.592667579650879,
        "year": "2025"
      },
      {
        "abstract": "Navigating the complexities of Arabic read- ability prediction requires addressing the lan- guage\u2019s rich morphology and structural diver- sity. In the BAREC Shared Task 2025, we participated in all tracks using a stacked ensem- ble meta learning framework. Our approach combined seven fine-tuned transformer, whose outputs fed into a meta classifier trained on multiple features, including individual predic- tions, their average, and the average top pre- diction probabilities. On the blind test set, our ensemble achieved a Quadratic Weighted Kappa (QWK) of 86.4%, demonstrating the ef- fectiveness of integrating diverse transformer encoders for fine grained Arabic readability classification and the potential of meta learning in morphologically rich contexts.",
        "authors": [
          "Mostafa Saeed",
          "Rana Waly",
          "Abdelaziz Ashraf Hussein"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "320",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "330",
        "paper_id": 45,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.45.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.45.jpg",
        "title": "AMAR at BAREC Shared Task 2025: Arabic Meta-learner for Assessing Readability",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.45",
        "x": -2.062227249145508,
        "y": 0.33047863841056824,
        "year": "2025"
      },
      {
        "abstract": "Thispaperprovidesacomprehensiveoverview of the QIAS 2025 shared task, organized as part of the ArabicNLP 2025 conference and co\u00adlocated with EMNLP2025. The task was designedfortheevaluationoflargelanguage modelsinthecomplexdomainsofreligiousand legalreasoning. Itcomprisestwosubtasks: (1) IslamicInheritanceReasoning,requiringmod\u00ad elstocomputeinheritancesharesaccordingto Islamicjurisprudence,and (2) IslamicKnowl\u00ad edgeAssessment,whichcoversarangeoftradi\u00ad tionalIslamicdisciplines. Bothsubtaskswere structuredasmultiple\u00adchoicequestionanswer\u00ad ingchallenges,withquestionsstratifiedbyvary\u00ad ingdifficultylevels. Thesharedtaskattracted significant interest, with 44 teams participat\u00ad inginthedevelopmentphase,fromwhich18 teamsadvancedtothefinaltestphase. Ofthese, 6 teamssubmittedentriesforbothsubtasks, 8 forTask1only,andtwoforTask 2 only. Ulti\u00ad mately,16 teamssubmittedsystemdescription papers. Herein,wedetailthetask\u2019smotivation, datasetconstruction,evaluationprotocol,and presentasummaryoftheparticipatingsystems andtheirresults.",
        "authors": [
          "Abdessalam Bouchekif",
          "Samer Rashwani",
          "Emad Soliman Ali Mohamed",
          "Mutaz Alkhatib",
          "Heba Sbahi",
          "Shahd Gaben",
          "Wajdi Zaghouani",
          "Aiman Erbad",
          "Mohammed Ghaly"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "851",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "860",
        "paper_id": 117,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.117.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.117.jpg",
        "title": "QIAS 2025: Overview of the Shared Task on Islamic Inheritance Reasoning and Knowledge Assessment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.117",
        "x": -3.1547088623046875,
        "y": 0.017683036625385284,
        "year": "2025"
      },
      {
        "abstract": "This paper details our submission to the Ara- GenEval Shared Task on Arabic AI-generated text detection, where our team, BUSTED, se- cured 5th place. We investigated the effec- tiveness of three pre-trained transformer mod- els: AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each model on the provided dataset for a bi- nary classification task. Our findings revealed a surprising outcome: the multilingual XLM- RoBERTa model achieved the highest perfor- mance with an F1-score of 0.7701, outperform- ing the specialized Arabic models. This work underscores the complexities of AI-generated text detection and highlights the strong gener- alization capabilities of multilingual models.",
        "authors": [
          "Ali Zain",
          "Sareem Farooqui",
          "Muhammad Rafi"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "72",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "76",
        "paper_id": 12,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.12.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.12.jpg",
        "title": "BUSTED at ARATECT Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.12",
        "x": -3.6083154678344727,
        "y": 1.6070817708969116,
        "year": "2025"
      },
      {
        "abstract": "In this paper, we describe our submission to the IslamicEval 2025 shared task, covering hallucination detection/correction and closed- world retrieval in Quranic and Hadith. We \ufb01ne-tuned an LLM for detecting Quran and Ha- dith text spans, utilizing synthetic augmenta- tion, diacritic variation, and morphological nor- malization to improve detection robustness (F1 = 87.10%) and used another reasoning model with tools (F1 = 90.06%). For validation, the accuracy is 88.60%, and for correction the ac- curacy is 66.56% where we employed a lay- ered hierarchical index and search algorithm combining exact, normalized, fuzzy, and se- mantic matching with prompt-driven repair\u2014to ensure canonical alignment and diacritic \ufb01- delity. For the correction stage, we also uti- lized a reasoning model with access to tools with an accuracy of 61.04%. Regarding the ranked answer-bearing text retrieval task, we implemented a Retrieval-Augmented Genera- tion (RAG) system restricted to the corpora pro- vided by the shared task, with structured out- put, vector-store grounding, and prompts tuned for \u201canswer-enclosing\u201d citations that achieve MAP@10 of 0.6199 on the development set and 0.2807 on the test set. The results highlight the value of normalization, corpus-restricted search, and reasoning models with tools in mit- igating hallucinations and improving retrieval precision in low-resource religious settings and that much smaller \ufb01ne-tuned models can com- pete with frontier models (e.g. GPT-5 high)",
        "authors": [
          "Arij Al Adel",
          "Abu Bakr Soliman",
          "Mohamed Sakher Sawan",
          "Rahaf Al-Najjar",
          "Sameh Amin"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "503",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "508",
        "paper_id": 69,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.69.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.69.jpg",
        "title": "BurhanAI at IslamicEval 2025 Shared Task: Combating Hallucinations in LLMs for Islamic Content; Evaluation, Correction, and Retrieval-Based Solution",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.69",
        "x": -1.271941900253296,
        "y": 1.7415810823440552,
        "year": "2025"
      },
      {
        "abstract": "We present a mental health support system for Arabic that can classify both patient questions and doctor answers, and generate answers for new questions. The classification model orga- nizes the input text to understand better the in- tent of the user and the response style, while the generation model produces accurate and empathetic responses. In evaluations, our sys- tem ranked 3rd in answer classification and 4th in answer generation, with only a small mar- gin from the top-ranked systems. These re- sults highlight the effectiveness of multi-label classification and RAG for improving access to mental health information and support in Arabic.",
        "authors": [
          "Mohamed Zaytoon",
          "Ahmed Mahmoud Salem",
          "Ahmed Sakr",
          "Hossam Elkordi"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "198",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "203",
        "paper_id": 28,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.28.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.28.jpg",
        "title": "AraMinds at AraHealthQA 2025: A Retrieval-Augmented Generation System for Fine-Grained Classification and Answer Generation of Arabic Mental Health Q&A",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.28",
        "x": -2.664166212081909,
        "y": 0.37055814266204834,
        "year": "2025"
      },
      {
        "abstract": "The dynamic interplay of hope and hate speech on Arabic social media presents a critical chal- lenge for content moderation and digital dis- course analysis. This paper presents our sys- tems for the MAHED 2025 shared task on Multimodal Detection of Hope and Hate Emo- tions in Arabic Content, addressing the two text-based subtasks. Our approach centers on a systematic, empirical comparison of Arabic- native versus large-scale multilingual Trans- former encoders to determine the optimal pre- training strategy for this nuanced domain. Com- prehensive evaluations demonstrate the clear superiority of Arabic-native models, with our ARBERTv2-based system achieving the high- est performance. We secured 11th place in Sub- task 1 with a macro F1-score of 0.682 and 5th place in Subtask 2 with a macro F1-score of 0.514. Error analysis reveals persistent chal- lenges in interpreting implicit language and overcoming severe class imbalance, particu- larly in distinguishing targeted hate from gen- eral offensiveness. This work contributes a ro- bust benchmark for this comparison and un- derscores the importance of language-specific pre-training for nuanced affective computing in Arabic.",
        "authors": [
          "Md. Abdur Rahman",
          "Md Sabbir Dewan",
          "Md. Tofael Ahmed Bhuiyan",
          "Md. Ashiqur Rahman"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "658",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "664",
        "paper_id": 90,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.90.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.90.jpg",
        "title": "SmolLab_SEU at MAHED Shared Task: Do Arabic-Native Encoders Surpass Multilingual Models in Detecting the Nuances of Hope, Hate, and Emotion?",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.90",
        "x": -1.66680908203125,
        "y": 2.6910698413848877,
        "year": "2025"
      },
      {
        "abstract": "We present our systems for the NADI 2025 shared task on multidialectal Arabic speech processing, participating in both spoken di- alect identification (ADI) and automatic speech recognition (ASR) subtasks. Working under data constraints by using only the provided shared task resources for dialect adaptation, we explore effective model adaptation strate- gies for dialectal Arabic speech. For ADI, we fine-tune w2v-BERT 2.0 and employ voice con- version as data augmentation, improving accu- racy from 68.71% to 76.40% on a blind cross- domain test set. For ASR, we develop two complementary approaches: (1) a CTC-based model pre-trained on public Arabic speech data, and (2) Whisper-based models using two- stage fine-tuning. Our experiments show that while dialect-centric CTC models exhibit bet- ter zero-shot dialectal performance (58.89 vs 93.90 WER), Whisper achieves better perfor- mance after dialect-specific adaptation, which reduces WER from 93.89 to 39.78 WER. We also demonstrate that using character error rate (CER) as a validation criterion provides prac- tical benefits with minimal performance trade- offs. Despite using no external resources for dialect adaptation beyond the shared task data, our systems ranked second in ADI and third in ASR, demonstrating that careful adaptation strategies can overcome data constraints in di- alectal speech processing.",
        "authors": [
          "Badr M. Abdullah",
          "Yusser Al Ghussin",
          "Zena Al-Khalili",
          "\u00d6mer Tarik \u00d6zyilmaz",
          "Matias Valdenegro - Toro",
          "Simon Ostermann",
          "Dietrich Klakow"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "745",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "751",
        "paper_id": 102,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.102.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.102.jpg",
        "title": "Saarland-Groningen at NADI 2025 Shared Task: Effective Dialectal Arabic Speech Processing under Data Constraints",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.102",
        "x": -1.7342184782028198,
        "y": 0.6644185185432434,
        "year": "2025"
      },
      {
        "abstract": "Automatic Readability Assessment estimates how hard a text is for its target readers, using features such as vocabulary, spelling, morphol- ogy, etc. Based on this premise, we evaluate our experiments on Arabic language under the BAREC 2025 shared task protocol. This paper addresses the sentence-level readability assess- ment task with strict track, that allows only the use of BAREC train set to predict Arabic read- ability on a fine-grained 19-level scale. Our solution is based on a two-phase fine-tuning of AraBERT-v2 on a custom feature set of the BAREC corpus. In the blind test set, the system achieves a QWK of 85.6%.",
        "authors": [
          "Saoussan Trigui"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "362",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "366",
        "paper_id": 50,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.50.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.50.jpg",
        "title": "STBW at BAREC Shared Task 2025: AraBERT-v2 with MSE-SoftQWK Loss for Sentence-Level Arabic Readability",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.50",
        "x": -1.5257529020309448,
        "y": 2.892444610595703,
        "year": "2025"
      },
      {
        "abstract": "This paper presents my participation in the Sentence-level Readability Assessment, Strict track of the BAREC Shared Task 2025 (El- madani et al., 2025a). Building upon prior work that fine-tuned pre-trained transformer models (Elmadani et al., 2025b), this work explores the impact of incorporating a rich set of handcrafted features on readability pre- diction performance. A total of 51 features were extracted from the BAREC corpus (El- madani et al., 2025b), including morpholog- ical, lexical, and syntactic indicators, lever- aging established computational linguistics tools. These features were integrated into a hybrid architecture that combines transformer- based contextual embeddings with dense lay- ers for feature processing. To optimize perfor- mance, experiments included freezing strate- gies and gradual unfreezing, alongside architec- tural variations with additional classification layers. Among the tested models, the best performance was achieved with MARBERT, reaching a Quadratic Weighted Kappa (QWK) of 80.95% on the test set, and 83.1% on the blind test set.",
        "authors": [
          "Nour Rabih"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "331",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "342",
        "paper_id": 46,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.46.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.46.jpg",
        "title": "Noor at BAREC Shared Task 2025: A Hybrid Transformer-Feature Architecture for Sentence-level Readability Assessment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.46",
        "x": -3.278973340988159,
        "y": 0.0733342319726944,
        "year": "2025"
      },
      {
        "abstract": "Culture fundamentally shapes human percep- tion and reasoning, while religion\u2014often em- bedded within cultural contexts\u2014provides co- hesive moral frameworks and a sense of com- munity. The PalmX 2025 shared task intro- duced two subtasks aimed at evaluating the ca- pability of large language models (LLMs) to capture and represent culturally and Islamically grounded knowledge. In this paper, we present our participation in this shared task, leverag- ing parameter-efficient fine-tuning (PEFT) tech- niques in conjunction with targeted data aug- mentation strategies. We further conducted ex- tensive zero-shot evaluations across a range of Arabic-centric and multilingual models to establish strong baselines and guide model se- lection. Our submitted system achieved com- petitive performance on the blind test sets, ranking 3rd in Subtask 1 with an accuracy of 71.45% and 1st in Subtask 2 with an accuracy of 84.22%.",
        "authors": [
          "Jannatul Tajrin",
          "Bir Ballav Roy",
          "Firoj Alam"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "830",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "836",
        "paper_id": 114,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.114.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.114.jpg",
        "title": "AYA at PalmX 2025: Modeling Cultural and Islamic Knowledge in LLMs",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.114",
        "x": -3.2638533115386963,
        "y": 4.055211067199707,
        "year": "2025"
      },
      {
        "abstract": "With the rapid emergence of large language models (LLMs), AI-generated content has in- creased, presenting new opportunities and sig- nificant risks. Detecting such content is cru- cial, yet while research in high-resource lan- guages like English has advanced, work in low- resource languages, such as Arabic, remains limited. To help fill this gap, the AraGenEval 2025 workshop organized a shared task on AI- generated Text Detection in Arabic. We partic- ipated in Task 3, where we evaluated several transformer-based models, including AraBERT, RoBERTa, AraRoBERTa, mBERT, and mar- BERT, both with and without chunking of input sequences during training. The experimental re- sults show that applying chunking prior to train- ing improves the performance of transformers. Among the evaluated models by the system testset, AraBERT with chunking achieved the highest F1 score (0.67), outperforming the oth- ers. Based on these results, our team ranked 12th in Shared Task 3.",
        "authors": [
          "Sowrav Nath",
          "Shadman Saleh",
          "Kawsar Ahmed",
          "Mohammed Moshiul Hoque"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "65",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "71",
        "paper_id": 11,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.11.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.11.jpg",
        "title": "CUET-NLP_Team_SS306 at AraGenEval Shared Task: A Transformer-based Framework for Detecting AI-Generated Arabic Text",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.11",
        "x": -3.0330443382263184,
        "y": 4.574817657470703,
        "year": "2025"
      },
      {
        "abstract": "Detecting hate speech in social media content is essential to provide a safe space for people to connect. Memes have been used lately to sar- castically express one\u2019s opinion, and they can be used to hide harmful intentions and spread hateful speech. In this work, we build our system that detects hateful speech in memes by combining visual and textual features and merging them using different techniques to de- tect the inherent meaning and overcome the challenge of vast dialectal differences and the variety of topics discussed. To improve our sys- tem\u2019s robustness, we combine different tech- niques, such as multi-tasking, contrastive learn- ing, and vision language modeling in a final ensemble model that secured us the third place in the MAHED 2025 shared-task leaderboard with a macro-f1 score of 0.74, showing strong performance on the evaluation set.",
        "authors": [
          "Mohamed Zaytoon",
          "Ahmed Mahmoud Salem",
          "Ahmed Sakr",
          "Hossam Elkordi"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "626",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "631",
        "paper_id": 85,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.85.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.85.jpg",
        "title": "AraMinds at MAHED 2025: Leveraging Vision-Language Models and Contrastive Multi-task Learning for Multimodal Hate Speech Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.85",
        "x": -1.9044108390808105,
        "y": 1.0806480646133423,
        "year": "2025"
      },
      {
        "abstract": "We present the MarsadLab submission to TAQEEM 2025 Shared Task A on Automated Essay Scoring (AES) in Arabic. Our system extends AraBERT with a prompt-type embed- ding and lexicon-based features. The lexicon captures statistical associations between word usage and essay quality under each prompt type, providing prompt-aware, interpretable signals that complement semantic embeddings. Our system achieved an average QWK of 0.438, highlighting both the promise and the chal- lenges of incorporating prompt-sensitive lex- ical knowledge into AES. This work represents a first attempt at leveraging a task-aware lexi- con for Arabic AES, showing that lexical fea- tures provide educational value through inter- pretability but also require more sophisticated integration. Future improvements could com- bine these lexical indicators with discourse-, syntax-, and content-level features, as well as explore richer fusion strategies to better exploit their potential.",
        "authors": [
          "Mabrouka Bessghaier",
          "Md. Rafiul Biswas",
          "Amira Dhouib",
          "Wajdi Zaghouani"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "998",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "1002",
        "paper_id": 138,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.138.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.138.jpg",
        "title": "MarsadLab at TAQEEM 2025: Prompt-Aware Lexicon-Enhanced Transformer for Arabic Automated Essay Scoring",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.138",
        "x": -2.34425687789917,
        "y": 1.175783395767212,
        "year": "2025"
      },
      {
        "abstract": "This paper describes the MultiMinds team\u2019s participation in the MAHED 2025 shared task at ArabicNLP 2025, which targets the detection of hate speech, hope speech, and emotional expression in Arabic content. We addressed two subtasks. For the text-based subtask (Task 2), we experimented with multiple models, including Support Vector Machines with TF- IDF and AraBERT embeddings, XGBoost with fused AraBERT and XLM-RoBERTa embed- dings optimized via Optuna, and a fine-tuned AraBERT model and GPT-5 (gpt-oss-20b). The fine-tuned AraBERT achieved the best perfor- mance with an F1 score of 0.68. For the mul- timodal subtask (Task 3), we proposed an ar- chitecture combining DistilBERT for text rep- resentation with a lightweight ELU-Net en- hanced by a cross-attention mechanism, reach- ing 75% accuracy. Major challenges included dataset imbalance and noisy text, which we mit- igated through preprocessing, class-weighted optimization, and feature fusion. Our results demonstrate the benefits of combining mul- tiple embedding layers for text classification and leveraging lightweight multimodal archi- tectures for robust hate speech detection in Ara- bic.",
        "authors": [
          "Riddhiman Debnath",
          "Abdul Wadud Shakib",
          "Md. Saiful Islam"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "677",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "682",
        "paper_id": 93,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.93.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.93.jpg",
        "title": "MultiMinds at MAHED 2025: Multimodal and Multitask Approaches for Detecting Emotional, Hate, and Offensive Speech in Arabic Content",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.93",
        "x": -1.5571104288101196,
        "y": 2.7701520919799805,
        "year": "2025"
      },
      {
        "abstract": "Emotional contagion, the phenomenon where emotions spread between individuals, shows the importance of detecting both hope and hate speech in digital communications. This emo- tional transmission can amplify positive senti- ments that foster community resilience or prop- agate harmful content that divides societies. While hate speech detection in Arabic has been extensively studied, hope speech detection has received comparatively limited attention, cre- ating an imbalance in the understanding of emotional influence online. To address this gap, MAHED 2025 sub-task 1 introduced the task of detecting both hope and hate speech using a substantial dataset designed for devel- oping robust classification models. This pa- per presents an ensemble approach combining three Transformer-based encoder models with soft voting and weighted loss functions to ad- dress class imbalance issues. Those models, ArabicDeBERTa-DA, BERT-DA, and MAR- BERTV2, have been continually pre-trained on different domains of Arabic, showing the ben- efits of continual pre-training both on down- stream performance and computational effi- ciency. The proposed ensemble model achieved the highest performance in the competition with an F1 macro score of 72.3% using an en- semble voting of the best-performing variants.",
        "authors": [
          "Abdallah Saleh",
          "Mariam M Biltawi"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "651",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "657",
        "paper_id": 89,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.89.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.89.jpg",
        "title": "HTU at MAHED Shared Task: Ensemble-Based Classification of Arabic Hate and Hope Speech Using Pre-trained Dialectal Arabic Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.89",
        "x": -4.454414367675781,
        "y": 0.5828759074211121,
        "year": "2025"
      },
      {
        "abstract": "This paper presents the Metapseud system de- signed for the Iqra\u2019Eval shared task, which ad- dresses the automatic assessment of Qur\u2019anic recitation pronunciation, as part of ARABIC- NLP 2025. This system applies multi-stage fine-tuning of Wav2Vec2.0 with curriculum- inspired training, followed by domain adap- tation to Qur\u2019anic phoneme annotations. The decoding is improved using beam search with a CTC-based decoder. The results show that staged adaptation achieved a phoneme error rate (PER) of 21% in the development set, and beam search improves the accuracy in the open test set from 76.9% to 82.1%. The findings of this work emphasize the significance of cur- riculum learning, domain adaptation, and de- coding strategies in recognizing mispronunci- ation in Qur\u2019anic recitation.",
        "authors": [
          "Ayman Mansour"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "475",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "479",
        "paper_id": 66,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.66.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.66.jpg",
        "title": "Metapseud at Iqra\u2019Eval: Domain Adaptation with Multi-Stage Fine-Tuning for Phoneme-Level Qur\u2019anic Mispronunciation Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.66",
        "x": -1.866964340209961,
        "y": 0.8400599360466003,
        "year": "2025"
      },
      {
        "abstract": "Arabic question-answering (Q/A) chatbots face signi\ufb01cant challenges due to the scarcity of large, high-quality datasets and the com- plexities of the Arabic language, including its rich morphology, multiple dialects, and di- verse writing forms. To address these chal- lenges, we implement an enhanced retrieval- augmented generation (RAG) pipeline for Ara- bic medical chatbots, leveraging a dataset of approximately one million Q/A pairs collected from various Arabic healthcare resources. Ex- perimental results demonstrate that our ap- proach signi\ufb01cantly outperforms previous Ara- bic medical QA systems, improving the qual- ity and relevance of generated answers, with the BERTScore increasing from 0.82 to 0.86. This work represents a step forward in develop- ing scalable and accurate Arabic medical chat- bots.",
        "authors": [
          "Hossam Amer",
          "Rawan Tarek Taha",
          "Gannat Elsayed",
          "Ensaf Hussein Mohamed"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "222",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "225",
        "paper_id": 31,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.31.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.31.jpg",
        "title": "Egyhealth at General Arabic Health QA (MedArabiQ): An Enhanced RAG Framework with Large-Scale Arabic Q&A Medical Data",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.31",
        "x": -2.9408278465270996,
        "y": 4.544438362121582,
        "year": "2025"
      },
      {
        "abstract": "Automated Essay Scoring (AES) has emerged as a significant research problem in natural language processing, offering valuable tools to support educators in assessing student writing. Motivated by the growing need for reliable Arabic AES systems, we organized the first shared Task for Arabic Quality Evaluation of Essays in Multi-dimensions ( TAQEEM) held at the ArabicNLP 2025 conference. TAQEEM 2025 includes two subtasks: Task A on holis- tic scoring and Task B on trait-specific scor- ing. It introduces a new (and first of its kind) dataset of 1,265 Arabic essays, annotated with holistic and trait-specific scores, including rel- evance, organization, vocabulary, style, devel- opment, mechanics, and grammar. The main goal of TAQEEM is to address the scarcity of standardized benchmarks and high-quality resources in Arabic AES. TAQEEM 2025 at- tracted 11 registered teams for Task A and 10 for Task B, with a total of 5 teams, across both tasks, submitting system runs for evalu- ation. This paper presents an overview of the task, outlines the approaches employed, and discusses the results of the participating teams.",
        "authors": [
          "May Bashendy",
          "Salam Albatarni",
          "Sohaila Eltanbouly",
          "Walid Massoud",
          "Houda Bouamor",
          "Tamer Elsayed"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "966",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "976",
        "paper_id": 134,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.134.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.134.jpg",
        "title": "TAQEEM 2025: Overview of The First Shared Task for Arabic Quality Evaluation of Essays in Multi-dimensions",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.134",
        "x": -1.565528392791748,
        "y": 0.755346417427063,
        "year": "2025"
      },
      {
        "abstract": "Arabic-speaking communities face persistent challenges in mental health support due to lin- guistic complexity, cultural nuances, and lim- ited specialized resources. This study intro- duces AraHealthQA 2025, a multi-task frame- work for Arabic mental health question an- swering, tackling three subtasks: (i) ques- tion classification, (ii) answer strategy clas- sification, and (iii) generative question an- swering using a Retrieval-Augmented Gener- ation (RAG) pipeline. For classification,fine- tuned AraBERTv2, MARBERTv2, and Ara- bic RoBERTa on multi-label mental health data. For generation, developing a culturally- aware RAG system that integrates semantic chunking, query enhancement, and hybrid re- trieval. Dense retrieval via akhooli/Arabic- SBERT-100K, sparse retrieval via rank_bm25, and generation using Sakalti/Saka-14B fine- tuned with culturally aligned mental health terminology (e.g., respecting religious sensi- tivities in advice). The approach achieves weighted F1-scores of 0.742 (question classifi- cation) and 0.718 (answer classification), and a BERTScore F1 of 0.821 representing up to 15% improvement over retrieval-only baselines. These findings demonstrate the potential of cul- turally sensitive, Arabic-focused NLP systems to advance accessible mental health support.",
        "authors": [
          "AbdelAziz Amr",
          "Mamdouh Koritam",
          "Mohamed Youssef",
          "Marwa Aldeeb",
          "Ensaf H. Mohamed"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "192",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "197",
        "paper_id": 27,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.27.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.27.jpg",
        "title": "Arabic Mental Health Question Answering: A Multi-Task Approach with Advanced Retrieval-Augmented Generation",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.27",
        "x": -3.624755382537842,
        "y": 2.8061788082122803,
        "year": "2025"
      },
      {
        "abstract": "WedescribeourparticipationintheQIAS2025 Shared Task on Islamic Studies Question An- swering, comprising two subtasks: (1) Islamic Inheritance Reasoning and (2) General Islamic Knowledge Assessment. Both were solved us- ing the Claude 4 Opus LLM via API with tailored prompting. For Subtask 1, we im- plemented a lightweight Retrieval-Augmented Generation (RAG) pipeline, which retrieves the top Google Search result (often from Is- lamWeb), preprocesses it, and appends it to a structured few-shot Arabic prompt, thereby boosting reasoning accuracy. For Subtask 2, where web-retrieval was not feasible due to closed-book sources, we applied topic-diverse few-shot prompting to leverage the model\u2019s in- ternal knowledge. Our systems achieved 4th/15 (0.895)inSubtask1and3rd/10(0.9259)inSub- task 2, demonstrating the e\ufb00ectiveness of tar- geted retrieval in open-web contexts and struc- tured prompting in closed-domain Arabic QA.",
        "authors": [
          "Mohamed Motasim Hamed",
          "Nada Ghneim",
          "Riad Sonbol"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "883",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "891",
        "paper_id": 122,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.122.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.122.jpg",
        "title": "HIAST at QIAS 2025: Retrieval-Augmented LLMs with Top-Hit Web Evidence for Arabic Islamic Reasoning QA",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.122",
        "x": -4.97794771194458,
        "y": 2.835900068283081,
        "year": "2025"
      },
      {
        "abstract": "This paper presents HUMAIN\u2019s submission to the IslamicEval 2025 Shared Task 1, address- ing hallucination detection and correction in Quranic and Hadith LLM-generated content. Our three-stage pipeline covers: (1) Span De- tection via sequence-to-sequence annotation us- ing TANL-style markup, (2) Validation with retrieval-based similarity and substring match- ing against reference corpora, and (3) Correc- tion through exact matching, LCS alignment, and semantic re-ranking. On the official test set, our system achieved 87.2% F-1 for span detection, 86.1% accuracy for validation, and 68.2% accuracy for correction. While system- atic detection is highly achievable, meaningful correction remains limited by semantic com- plexity where small textual differences can significantly impact religious understanding. This work presents a multi-stage LLM-based pipeline for Islamic content verification.",
        "authors": [
          "Arwa Omayrah",
          "Sakhar Alkhereyf",
          "Ahmed Abdelali",
          "Abdulmohsen Al-Thubaity",
          "Jeril Kuriakose",
          "Ibrahim AbdulMajeed"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "509",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "514",
        "paper_id": 70,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.70.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.70.jpg",
        "title": "HUMAIN at IslamicEval 2025 Shared Task 1: A Three-Stage LLM-Based Pipeline for Detecting and Correcting Hallucinations in Quran and Hadith",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.70",
        "x": -3.035327672958374,
        "y": 0.0878678485751152,
        "year": "2025"
      },
      {
        "abstract": "Recent advancements in large language mod- els (LLMs) have opened new possibilities for processing complex natural language tasks, in- cluding those involving highly regarded reli- gious content. However, working with divine sources such as the Holy Quran and Hadith presents unique challenges. These Classical Arabic texts have, for centuries, been metic- ulously preserved and recited word-for-word, allowing no tolerance for errors \u2014 even a sin- gle incorrect diacritic can entirely alter the meaning. Such sensitivity demands excep- tional precision, as hallucinations or inaccura- cies from LLMs could lead to significant misin- terpretations among general users. To address this challenge, we present an Arabic-focused, LLM-powered framework designed to identify and verify the integrity of religious text gen- erated by widely used LLMs. Evaluation on benchmark subtasks demonstrates strong per- formance, achieving a Macro-Avg F1 score of 86.11% on Subtask 1A and an Accuracy of 89.82% on Subtask 1B.",
        "authors": [
          "Mohammed ElKoumy",
          "Khalid Allam",
          "Ahmad Tamer",
          "Mohamed Alqablawi"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "515",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "527",
        "paper_id": 71,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.71.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.71.jpg",
        "title": "TCE at IslamicEval 2025: Retrieval-Augmented LLMs for Quranic and Hadith Content Identification and Verification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.71",
        "x": -1.9723210334777832,
        "y": 0.27781403064727783,
        "year": "2025"
      },
      {
        "abstract": "We present an overview of the AraGenEval shared task, organized as part of the Arabic- NLP 2025 conference. This task introduced the first benchmark suite for Arabic authorship analysis, featuring three subtasks: Authorship Style Transfer, Authorship Identification, and AI-Generated Text Detection. We curated high- quality datasets, including over 47,000 para- graphs from 21 authors and a balanced corpus of human- and AI-generated texts. The task at- tracted significant global participation, with 72 registered teams from 16 countries. The results highlight the effectiveness of transformer-based models, with top systems leveraging prompt en- gineering for style transfer, model ensembling for authorship identification, and a mix of mul- tilingual and Arabic-specific models for AI text detection. This paper details the task design, datasets, participant systems, and key findings, establishing a foundation for future research in Arabic stylistics and trustworthy NLP.",
        "authors": [
          "Shadi Abudalfa",
          "Saad Ezzini",
          "Ahmed Abdelali",
          "Hamza Alami",
          "Abdessamad Benlahbib",
          "Salmane Chafik",
          "Mo El-Haj",
          "Abdelkader El Mahdaouy",
          "Mustafa Jarrar",
          "Salima Lamsiyah",
          "Hamzah Luqman"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "1",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "13",
        "paper_id": 1,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.1.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.1.jpg",
        "title": "The AraGenEval Shared Task on Arabic Authorship Style Transfer and AI Generated Text Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.1",
        "x": -2.7514984607696533,
        "y": 4.4843292236328125,
        "year": "2025"
      },
      {
        "abstract": "Mental health detection in online discourse is a growing area of NLP research, particularly for low-resource languages such as Arabic, where stigma and limited access to professional care make anonymous, technology-driven solutions valuable. In the context of the AraHealth shared task, we were provided with three subtasks: multi-label classification for questions, multi- label classification for answers, and a QA sys- tem leveraging models developed in the previ- ous two tasks. Our approach employed data augmentation to address class imbalance, as certain categories in the dataset were signifi- cantly underrepresented. Since our method re- lied on prompting models to classify questions and answers as well as to generate answers for the QA system, we utilized Gradient-free Edit- Based Instruction Search (GrIPS) to optimize prompt selection. Our system achieved strong results across all three subtasks, ranking 1st in answer classification and 3rd in both question classification and QA system answer genera- tion.",
        "authors": [
          "AbdulRahman A. Morsy",
          "Saad Mankarious",
          "Ayah Zirikly"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "184",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "191",
        "paper_id": 26,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.26.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.26.jpg",
        "title": "Sindbad at AraHealthQA Track 1: Leveraging Large Language Models for Mental Health Q&A",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.26",
        "x": -2.5325732231140137,
        "y": 0.3822360932826996,
        "year": "2025"
      },
      {
        "abstract": "W e address Arabic medical question an- swering (QA) in the AraHealthQA shared task, which evaluates systems on two in- put formats: (i) fill-in-the-blank termi- nology items (gaps) and (ii) open-ended patient\u2013doctor dialogues (gabs). W e propose MedGapGab, a modular large language model (LLM) framework that assigns each question type to a spe- cialized model\u2014 Gemini 2.5 Flash for terminology-focused gaps and DeepSeek V3 for reasoning-intensive gabs. In ad- dition, we use TF-IDF\u2013driven few-shot prompting to retrieve relevant examples from the development set and embed them into the prompts for better contextual- ization. MedGapGab achieves 87.26% BER TScore, ranking 1 st on the o\ufb00icial leaderboard. These results demonstrate that combining TF-IDF-guided example retrieval with type-aware model routing yields strong performance in Arabic med- ical QA and can inform future work on resource-scarce medical domains.",
        "authors": [
          "Baraa Hikal"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "213",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "221",
        "paper_id": 30,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.30.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.30.jpg",
        "title": "MedGapGab at AraHealthQA: Modular LLM Assignment for Gaps and Gabs in Arabic Medical Question Answering",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.30",
        "x": -1.5613141059875488,
        "y": 0.5255848169326782,
        "year": "2025"
      },
      {
        "abstract": "Hope and hate speech detection in natural lan- guage processing addresses the challenge of identifying social media content within the fast- paced environment of online platforms. Hope- ful speech that promotes supportive and in- clusive language plays a crucial role in coun- teracting online toxicity, whereas hate speech poses threats and challenges to society. This paper focuses on text-based Arabic hate and hope speech detection, demonstrating the sys- tem submitted by the REGLAT team to the MAHED shared task held in conjunction with ArabicNLP 2025. The proposed system em- ploys an ensemble-based model that combines a TF-IDF + Logistic Regression classifier with a fine-tuned AraBERTv2 model as baselines. A majority voting approach is then applied to aggregate the predictions. The proposed model reported an F1 score of 0.58. These promis- ing results are notable given the simplicity of the system\u2019s architecture, and they highlight the potential of our approach for improving the performance of this task.",
        "authors": [
          "Nsrin Ashraf",
          "Mariam Labib",
          "Tarek Elshishtawy",
          "Hamada Nayel"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "645",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "650",
        "paper_id": 88,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.88.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.88.jpg",
        "title": "REGLAT at MAHED Shared Task: A Hybrid Ensemble-Based System for Arabic Hate Speech Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.88",
        "x": -1.5724848508834839,
        "y": 2.7171900272369385,
        "year": "2025"
      },
      {
        "abstract": "Hallucination in Large Language Models (LLMs) remains a significant challenge and continues to draw substantial research atten- tion. The problem becomes especially criti- cal when hallucinations arise in sensitive do- mains, such as religious discourse. To address this gap, we introduce IslamicEval 2025\u2014the first shared task specifically focused on evalu- ating and detecting hallucinations in Islamic content. The task consists of two subtasks: (1) Hallucination Detection and Correction of quoted verses (Ayahs) from the Holy Quran and quoted Hadiths; and (2) Qur\u2019an and Hadith Question Answering, which assesses retrieval models and LLMs by requiring answers to be retrieved from grounded, authoritative sources. Thirteen teams participated in the final phase of the shared task, employing a range of pipelines and frameworks. Their diverse approaches un- derscore both the complexity of the task and the importance of effectively managing halluci- nations in Islamic discourse.",
        "authors": [
          "Hamdy Mubarak",
          "Rana Malhas",
          "Watheq Mansour",
          "Abubakr Mohamed",
          "Mahmoud Fawzi",
          "Majd Hawasly",
          "Tamer Elsayed",
          "Kareem Mohamed Darwish",
          "Walid Magdy"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "480",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "493",
        "paper_id": 67,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.67.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.67.jpg",
        "title": "IslamicEval 2025: The First Shared Task of Capturing LLMs Hallucination in Islamic Content",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.67",
        "x": -5.518510341644287,
        "y": 1.3338183164596558,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our system for SubTask 1: Islamic Inheritance Reasoning in the QIAS 2025 Shared Task, which evaluates large lan- guage models (LLMs) on (ilm al-maw\u0101r\u012bth) (Islamic science of inheritance) using a bench- mark of Arabic multiple-choice questions (MCQs) derived from expert-reviewed fat- was. We explore static and dynamic few- shot prompting, retrieval-augmented genera- tion (RAG) with a large fatwa corpus, and a progressive n-gram overlap retrieval method. The n-gram method is applied both to select the top five most similar MCQs for dynamic prompting and to retrieve the most relevant fatwa answer as additional context during in- ference. We evaluate proprietary and open- source LLMs individually and in ensemble form. Results show that dynamic prompt- ing and RAG consistently improve accuracy across our best performing model, Gemini, achieving 62.26% accuracy on the test set.",
        "authors": [
          "Shatha Altammami"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "867",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "872",
        "paper_id": 119,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.119.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.119.jpg",
        "title": "SHA at the QIAS Shared Task: LLMs for Arabic Islamic Inheritance Reasoning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.119",
        "x": -2.752981662750244,
        "y": 4.659287929534912,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our system for Subtask-2: Emotion, Offensive Language, and Hate De- tection in the MAHED 2025 Shared Task at ArabicNLP 2025. We address the challenge of multi-label classi\ufb01cation in Arabic social media text using a two-stage, prompt-based framework with large language models. In the \ufb01rst stage, our system classi\ufb01es emotions into 12 distinct categories; in the second stage, it detects offensive messages and, when relevant, further identi\ufb01es the presence of hate speech. Both stages leverage the Meta-Llama-3.1-8B model, \ufb01ne-tuned to capture the diverse lin- guistic and dialectal characteristics of Arabic. Our approach achieved a macro F1-score of 0.518 on the of\ufb01cial test set, placing 4th in Subtask 2. The results demonstrate the effec- tiveness of prompt-based modeling for com- plex Arabic text classi\ufb01cation and contribute a practical, LLM-based solution for emotion and hate speech detection in low-resource sce- narios.",
        "authors": [
          "Ratnajit Dhar",
          "Arpita Mallik"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "620",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "625",
        "paper_id": 84,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.84.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.84.jpg",
        "title": "CUET-823 at MAHED 2025 Shared Task: Large Language Model-Based Framework for Emotion, Offensive, and Hate Detection in Arabic",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.84",
        "x": -3.1535961627960205,
        "y": 4.931142807006836,
        "year": "2025"
      },
      {
        "abstract": "Authorship Identification for Arabic texts is challenging due to the language\u2019s dialectal di- versity and the wide stylistic variation across genres, cultures, and historical periods. It has critical applications in copyright enforcement, forensic linguistics, and literary analysis. Rec- ognizing its importance, we addressed this chal- lenge using the AraGenEval 2025shared task dataset, which contains works by writers from diverse backgrounds and time periods. We con- ducted extensive experiments with multiple ar- chitectures and proposed an ensemble model that combines the strengths of four fine-tuned transformer-based models. We applied data augmentation to enrich the dataset and class weighting to handle class imbalance during training. Our system achieved a Macro-F1 score of 90%, representing a 15% improve- ment over our baseline, and ranked 1st in the competition.",
        "authors": [
          "Muhammad Helmy",
          "Batool Najeh Balah",
          "Ahmed Mohamed Sallam",
          "Ammar Sherif"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "59",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "64",
        "paper_id": 10,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.10.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.10.jpg",
        "title": "Sebaweh at AraGenEval Shared Task: BERENSE - BERt based ENSEmbler for Arabic Authorship Identification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.10",
        "x": -1.5484340190887451,
        "y": 2.7695181369781494,
        "year": "2025"
      },
      {
        "abstract": "Large Language Models (LLMs) have demon- strated impressive multilingual capabilities; however, their reasoning often reflects English- centric perspectives, which can limit accuracy in culture-specific contexts. Arabic, with its diverse dialects, rich historical heritage, and complex socio-cultural norms, presents a par- ticularly challenging setting for such evalua- tion. To address this gap, we participated in the PalmX 2025 shared task, which bench- marks cultural reasoning in Arabic through multiple-choice questions covering traditions, social norms, history, geography, arts, and di- alectal expressions. By applying parameter- efficient adaptation and culturally informed prompt formatting, we aligned model outputs with both linguistic correctness and cultural rel- evance. Our approach achieved an accuracy of 71.65%, securing second placeoverall and closely matching the top system. These results demonstrate that targeted adaptation can sig- nificantly enhance cultural reasoning in LLMs, paving the way for more culturally aware Arti- ficial Intelligence.",
        "authors": [
          "Pulkit Chatwal",
          "Santosh Kumar Mishra"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "837",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "842",
        "paper_id": 115,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.115.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.115.jpg",
        "title": "Cultura-Arabica: Probing and Enhancing Arabic Cultural Awareness in Large Language Models via LoRA",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.115",
        "x": -2.3935859203338623,
        "y": 0.64243084192276,
        "year": "2025"
      },
      {
        "abstract": "PalNLP addressed Arabic readability level pre- diction as a fine-grained ordinal classification problem by strictly using the Balanced Arabic Readability Evaluation Corpus (BAREC). The approach treats the 19-class ordinal classifica- tion problem as a regression task with post- hoc threshold optimization, leveraging a BERT- based model and an ensemble strategy. The system achieved a Quadratic Weighted Kappa (QWK) score of 81.1 in the blind test dataset, indicating an almost perfect agreement between the system\u2019s classifications and the true labels, and placing 18 th out of 24 teams. The find- ings show that the model effectively learned broad readability patterns, with a competitive \u00b11 accuracy, but faced challenges in accurately predicting readability levels of most sentences.",
        "authors": [
          "Mutaz Ayesh"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "343",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "349",
        "paper_id": 47,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.47.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.47.jpg",
        "title": "PalNLP at BAREC Shared Task 2025: Predicting Arabic Readability Using Ordinal Regression and K-Fold Ensemble Learning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.47",
        "x": -2.077158212661743,
        "y": 0.656496524810791,
        "year": "2025"
      },
      {
        "abstract": "Sentence-level readability assessment, which measures how easily individual sentences can be understood, has seen signi\ufb01cant advances in English. However, Arabic readability assess- ment remains underexplored, primarily due to the language\u2019s morphological complexity and the scarcity of \ufb01ne-grained annotated datasets. To address this gap, we leveraged the BAREC corpus, which provides 69K sentences anno- tated across 19 readability levels, enabling us to develop and compare \ufb01ve di\ufb00erent modeling strategies ranging from lightweight classi\ufb01ers to \ufb01ne-tuned Arabic language models. Our ex- periments revealed that task-speci\ufb01c pretrain- ing with CamelBERT yielded substantial per- formance gains, while curriculum learning of- fered bene\ufb01ts in speci\ufb01c scenarios. Ultimately, direct \ufb01ne-tuning achieved state-of-the-art per- formance (QWK = 82.4). Through detailed error analysis, we identi\ufb01ed that models strug- gledmostwithdistinguishingbetweenthelower readability level 2 and higher readability levels (15-19), highlighting the inherent challenges in \ufb01ne-grainedArabicreadabilitymodelingacross the full spectrum of pro\ufb01ciency levels.",
        "authors": [
          "Anya Amel Nait Djoudi",
          "Patrice Bellot",
          "Adrian-Gabriel Chifu"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "367",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "375",
        "paper_id": 51,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.51.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.51.jpg",
        "title": "LIS at BAREC Shared Task 2025: Multi-Scale Curriculum Learning for Arabic Sentence-Level Readability Assessment Using Pre-trained Language Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.51",
        "x": -4.428169250488281,
        "y": 0.24083295464515686,
        "year": "2025"
      },
      {
        "abstract": "We participated in NADI 2025 shared tasks on Arabic Dialect Identification (ADI) and Automatic Speech Recognition (ASR) across eight Arabic dialects. For ADI, we employ an enhanced ECAPA-TDNN with V oxLin- gua107 initialization, featuring self-attention classification head, progressive unfreezing, ad- vanced augmentation, and test-time augmenta- tion. This approach ranked third with 61.6% accuracy and 0.3068 macro cost. For ASR, we implement a zero-shot cascaded system us- ing Whisper Large-v3 and MARBERT with extreme parameter efficiency (0.0004% train- able), ranking seventh with 104.895 WER and 84.693 CER. Our results validate complemen- tary paradigms: direct audio processing for competitive dialect classification versus founda- tion model robustness for cross-dialectal tran- scription.",
        "authors": [
          "Md. Rafiul Biswas",
          "Kais Attia",
          "Shimaa Ibrahim",
          "Mabrouka Bessghaier",
          "Wajdi Zaghouani"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "752",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "756",
        "paper_id": 103,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.103.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.103.jpg",
        "title": "MarsadLab at NADI Shared Task: Arabic Dialect Identification and Speech Recognition using ECAPA-TDNN and Whisper",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.103",
        "x": -1.6203316450119019,
        "y": 0.5600028038024902,
        "year": "2025"
      },
      {
        "abstract": "Large language models (LLMs) have been widely used recently. Adapting these models to multiple languages would enhance the accuracy and precision of the other languages. Apply- ing LLMs with Arabic language could improve the prediction of Arabic language. This work applies LLMs with MCQs of Arabic in both the cultural and Islamic domain. The dataset used is PalmX, which is an MCQ benchmark dataset. In this work, traditional and AI gen- eration data augmentations are used. For the cultural domain, we applied data augmenta- tion techniques, including paraphrasing using Fanar-1-9B-Instruct model and answer shuf- fling. For the Islamic domain, we used the original dataset without augmentation to main- tain content integrity. We then fine-tuned the Qwen2.5-3B-Instruct model on both datasets and evaluate its performance, achieving 65.90% accuracy on the cultural set and 70.83% on the Islamic set. Experiment and evaluation are dis- cussed and the best accuracy achieved in this work is explained in both domains.",
        "authors": [
          "Walid Al-Dhabyani",
          "Hamzah A. Alsayadi"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "790",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "796",
        "paper_id": 108,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.108.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.108.jpg",
        "title": "Hamyaria at PalmX2025: Leveraging Large Language Models to Improve Arabic Multiple-Choice Questions in Cultural and Islamic Domains",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.108",
        "x": -2.0450279712677,
        "y": 0.40413349866867065,
        "year": "2025"
      },
      {
        "abstract": "Image captioning is the task of automatically generating natural language descriptions for vi- sual content, with applications in search, social media, and beyond. While English captioning has advanced significantly, Arabic captioning remains underdeveloped due to a scarcity of high-quality, culturally relevant datasets. This work, conducted under the ImageEval 2025 Shared Task, addresses this gap by introduc- ing a novel, manually annotated, open-source dataset for Arabic image captioning. Our cu- rated resource consists of 500 unique black- and-white historical photographs document- ing pivotal events in modern Palestinian and Lebanese history. The dataset spans from the British Colonial era in Palestine through the events of 1948, and includes documentation of the 1982 Israeli invasion of Beirut. This contribution provides a foundational resource to advance research in Arabic NLP and multi- modal systems, offering a vital benchmark for models processing complex historical, cultural, and traumatic imagery.",
        "authors": [
          "Sarah Yassine"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "438",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "442",
        "paper_id": 60,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.60.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.60.jpg",
        "title": "AZLU at ImagEval Shared Task: Bridging Linguistics and Cultural Gaps in Arabic Image Captioning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.60",
        "x": -5.980228424072266,
        "y": 3.7646780014038086,
        "year": "2025"
      },
      {
        "abstract": "Readability assessment is essential for effec- tive communication of scientific and medical content in Arabic. We present a system for the BAREC 2025 Shared Task Arabic Read- ability Assessment. The system fine-tunes AraBERTv2 with a CORAL ordinal head, ap- plies AraBERT-specific preprocessing, and se- lects checkpoints using Quadratic-Weighted Kappa (QWK) with early stopping. Our model achieves a QWK of 85.5 on the Sentence Blind Test, demonstrating its effectiveness for auto- matic Arabic readability prediction.",
        "authors": [
          "Ahmad M. Nazzal"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "266",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "268",
        "paper_id": 37,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.37.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.37.jpg",
        "title": "ZAI at BAREC Shared Task 2025: AraBERT CORAL for Fine Grained Arabic Readability",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.37",
        "x": -4.807343482971191,
        "y": 0.01804334670305252,
        "year": "2025"
      },
      {
        "abstract": "In this paper, we present a system for solv- ing Islamic inheritance problems using large language models (LLMs), focusing on accu- rate reasoning in Arabic based on fara\u2019id rules. Our approach is built on the Qwen3-4B model, quantized, and trained using the Unsloth frame- work for efficiency. We explore multiple train- ing strategies: (1) retrieval-augmented gener- ation (RAG) using fatwas from Islamweb, (2) supervised fine-tuning (SFT) on annotated in- heritance datasets, (3) instruction tuning of a base Qwen model followed by GRPO training for multiple choice question solving, and (4) a two-stage pipeline involving SFT on a classi- cal Islamic inheritance book followed by MCQ fine-tuning. Among these, the fourth approach achieved 97.2% accuracy, outperforming all other submissions and ranking our team first in the competition.",
        "authors": [
          "Eman Elrefai",
          "Mohamed Lotfy Elrefai",
          "Aml Hassan Esmail"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "953",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "959",
        "paper_id": 132,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.132.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.132.jpg",
        "title": "Gumball at QIAS 2025: Arabic LLM Automated Reasoning in Islamic Inheritance",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.132",
        "x": -2.2640421390533447,
        "y": 0.6220368146896362,
        "year": "2025"
      },
      {
        "abstract": "This paper presents a lightweight system for the AraGenEval shared task, addressing AI-generated text detection and authorship identification in Arabic. Using pretrained xlm-roberta-large embeddings with mean pooling and [CLS] token strategies, combined with classical classifiers (RidgeClassifierCV and LinearSVC), our approach achieved F1- scores of 0.7400 and 0.8130 on the ARATECT and authorship datasets, respectively. Mean pooling outperformed [CLS] by 3%, demon- strating efficiency and robustness for limited Arabic data while capturing stylistic nuances critical for both tasks.",
        "authors": [
          "Mena Hany"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "37",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "41",
        "paper_id": 6,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.6.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.6.jpg",
        "title": "NLP_wizard at AraGenEval shared task: Embedding-Based Classification for AI Detection and Authorship Attribution",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.6",
        "x": -1.8181633949279785,
        "y": 2.9837026596069336,
        "year": "2025"
      },
      {
        "abstract": "This paper details the system developed by team Sakinah-AI for the MentalQA 2025 shared task, focusing on Arabic mental health question classification. We compare few-shot learning with Large Language Models against fine-tuning of BERT-based models (CAMeL- BERT and AraBERTv2). Few-shot learn- ing with Palmyra-Med-70B achieved the high- est weighted F1-score of 0.605, followed by hyperparameter-optimized CAMeL-BERT at 0.597. Notably, 5-fold ensemble methods proved detrimental to performance. Our re- sults demonstrate that for low-resource special- ized domains, both few-shot learning and opti- mized fine-tuning of appropriate base models outperform ensemble strategies. To ensure re- producibility all experimental code and final fine-tuned models are made publicly available.",
        "authors": [
          "Fatimah Mohamed Emad Elden",
          "Mumina Ab. Abukar"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "140",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "148",
        "paper_id": 21,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.21.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.21.jpg",
        "title": "Sakinah-AI at MentalQA: A Comparative Study of Few-Shot, Optimized, and Ensemble Methods for Arabic Mental Health Question Classification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.21",
        "x": -1.6312669515609741,
        "y": 1.0426244735717773,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our submission to the QIAS 2025 shared task on Islamic knowledge un- derstanding and reasoning. We developed a hybrid retrieval-augmented generation (RAG) system that combines sparse and dense re- trieval methods with cross-encoder reranking to improve large language model (LLM) per- formance. Our three-stage pipeline incorpo- rates BM25 for initial retrieval, a dense em- bedding retrieval model for semantic matching, and cross-encoder reranking for precise content retrieval. We evaluate our approach on both subtasks using two LLMs, Fanar and Mistral, demonstrating that the proposed RAG pipeline enhances performance across both, with accu- racy improvements up to 25%, depending on the task and model configuration. Our best configuration is achieved with Fanar, yielding accuracy scores of 45% in Subtask 1 and 80% in Subtask 2.",
        "authors": [
          "Muhammad Abu Ahmad",
          "Mohamad Ballout",
          "Raia Abu Ahmad",
          "Elia Bruni"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "899",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "904",
        "paper_id": 124,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.124.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.124.jpg",
        "title": "Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.124",
        "x": -1.4101086854934692,
        "y": 2.8914411067962646,
        "year": "2025"
      },
      {
        "abstract": "We present the findings of the sixth Nuanced Arabic Dialect Identification ( NADI 2025) Shared Task, which focused on Arabic speech dialect processing across three subtasks: spo- ken dialect identification (Subtask 1), speech recognition (Subtask 2), and diacritic restora- tion for spoken dialects (Subtask 3). A total of 44 teams registered, and during the testing phase, 100 valid submissions were received from eight unique teams. The distribution was as follows: 34 submissions for Subtask 1 \u201cfive teams\u201d, 47 submissions for Subtask 2 \u201csix teams\u201d, and 19 submissions for Subtask 3 \u201ctwo teams\u201d. The best-performing systems achieved 79.8% accuracy on Subtask 1, 35.68/12.20 WER/CER (overall average) on Subtask 2, and 55/13 WER/CER on Subtask 3. These results highlight the ongoing challenges of Arabic di- alect speech processing, particularly in dialect identification, recognition, and diacritic restora- tion. We also summarize the methods adopted by participating teams and briefly outline direc- tions for future editions of NADI.1",
        "authors": [
          "Bashar Talafha",
          "Hawau Olamide Toyin",
          "Peter Sullivan",
          "AbdelRahim A. Elmadany",
          "Abdurrahman Juma",
          "Amirbek Djanibekov",
          "Chiyu Zhang",
          "Hamad Alshehhi",
          "Hanan Aldarmaki",
          "Mustafa Jarrar",
          "Nizar Habash",
          "Muhammad Abdul-Mageed"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "720",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "733",
        "paper_id": 99,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.99.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.99.jpg",
        "title": "NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.99",
        "x": -3.4700958728790283,
        "y": 1.1160106658935547,
        "year": "2025"
      },
      {
        "abstract": "The rise of social media and online communi- cation platforms has led to the spread of Ara- bic textual posts and memes as a key form of digital expression. While these contents can be humorous and informative, they are also increasingly being used to spread offensive lan- guage and hate speech. Consequently, there is a growing demand for precise analysis of content in Arabic text and meme. This paper explores the potential of large language models to effectively identify hope, hate speech, of- fensive language, and emotional expressions within such content. We evaluate the perfor- mance of base LLMs, fine-tuned LLMs, and pre-trained embedding models . The evaluation is conducted using a dataset of Arabic textual speech and memes proposed in the ArabicNLP MAHED 2025 challenge. The results under- score the capacity of LLMs such as GPT-4o- mini, fine-tuned with Arabic textual speech, and Gemini Flash 2.5, fine-tuned with Arabic memes, to deliver the superior performance. They achieve up to 72.1%, 57.8%, and 79.6% macro F1 scores for task 1, 2, and 3, respec- tively and secure first place overall in the chal- lenge1 (Zaghouani et al., 2025). The proposed solutions offer a more nuanced understanding of both text and memes for accurate and effi- cient Arabic content moderation systems.",
        "authors": [
          "Nouar Aldahoul",
          "Yasir Zaki"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "575",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "584",
        "paper_id": 76,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.76.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.76.jpg",
        "title": "NYUAD at MAHED Shared Task: Detecting Hope, Hate, and Emotion in Arabic Textual Speech and Multi-modal Memes Using Large Language Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.76",
        "x": -2.9062516689300537,
        "y": 3.9896469116210938,
        "year": "2025"
      },
      {
        "abstract": "We present VLCAP, an Arabic image caption- ing framework that integrates CLIP-based vi- sual label retrieval with multimodal text gen- eration. Rather than relying solely on end-to- end captioning, VLCAP grounds generation in interpretable Arabic visual concepts extracted with three multilingual encoders, mCLIP, Ara- CLIP, and Jina V4, each evaluated separately for label retrieval. A hybrid vocabulary is built from training captions and enriched with about 21K general domain labels translated from the Visual Genome dataset, covering ob- jects, attributes, and scenes. The top- k re- trieved labels are transformed into fluent Ara- bic prompts and passed along with the origi- nal image to vision\u2013language models. In the second stage, we tested Qwen-VL and Gem- ini Pro Vision for caption generation, resulting in six encoder\u2013decoder configurations. The results show that mCLIP + Gemini Pro Vi- sion achieved the best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL obtained the highest LLM-judge score (36.33%). This interpretable pipeline en- ables culturally coherent and contextually ac- curate Arabic captions.",
        "authors": [
          "Passant Elchafei",
          "Amany Fashwan"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "408",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "413",
        "paper_id": 56,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.56.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.56.jpg",
        "title": "VLCAP at ImageEval 2025 Shared Task: Multimodal Arabic Captioning with Interpretable Visual Concept Integration",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.56",
        "x": -2.8536863327026367,
        "y": 4.7183403968811035,
        "year": "2025"
      },
      {
        "abstract": "This paper presents our submission to the PalmX 2025 Shared Task on Arabic cultural and religious knowledge comprehension. We focus on training large language models capa- ble of representing domain-specific cultural and religious knowledge in Arabic. Our approach leverages parameter-efficient fine-tuning of the instruction-tuned Qwen2.5-7B model using Low-Rank Adaptation (LoRA). To address the challenges of limited training data, we apply quantization-aware fine-tuning with 4- bit precision, enabling efficient adaptation un- der constrained resources. The model is fur- ther aligned with the multiple-choice evalu- ation format to enhance task-specific reason- ing. Without relying on external data augmen- tation, our system achieves competitive perfor- mance across both the Arabic General Culture and Islamic Culturesubtasks, demonstrating the effectiveness of targeted fine-tuning for en- riching cultural and religious knowledge rep- resentation in LLMs. On the blind test sets, our systems ranked 7th and 4th in the cul- tural and Islamic subtasks, respectively. To ensure reproducibility, we make our full code- base and experimental configurations available at https://github.com/rafiulbiswas/PalmX.",
        "authors": [
          "Md. Rafiul Biswas",
          "Shimaa Ibrahim",
          "Kais Attia",
          "Firoj Alam",
          "Wajdi Zaghouani"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "818",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "824",
        "paper_id": 112,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.112.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.112.jpg",
        "title": "MarsadLab at PalmX Shared Task: An LLM Benchmark for Arabic Culture and Islamic Civilization",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.112",
        "x": -3.0857906341552734,
        "y": 4.862277984619141,
        "year": "2025"
      },
      {
        "abstract": "",
        "authors": [
          "Huthayfa Malhis",
          "Mohammad Tami",
          "Huthaifa I. Ashqar"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "99",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "106",
        "paper_id": 17,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.17.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.17.jpg",
        "title": "Jenin at AraGenEval Shared Task: Parameter-Efficient Fine-Tuning and Layer-Wise Analysis of Arabic LLMs for Authorship Style Transfer and Classification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.17",
        "x": -2.2318050861358643,
        "y": 0.865698516368866,
        "year": "2025"
      },
      {
        "abstract": "Arabic hate speech detection presents unique challenges due to the language\u2019s morpholog- ical complexity, dialectal diversity, and the subtle nature of emotional expressions in so- cial media. In this paper, we present our submission to the MAHED shared task for Arabic hate speech classification, which aims to classify Arabic text into three categories: hope, hate, and not_applicable. This task is crucial for building safer online communi- ties and has applications in content modera- tion, social media analysis, and digital well- being initiatives. We systematically evalu- ate six transformer-based encoders, comparing Arabic-specific models (MARBERT, AraBERT, ALCALM) against multilingual alternatives (XLM-RoBERTa, LaBSE, BGE). Our approach demonstrates that specialized Arabic models specially encoders trained on more than one dialect like marber significantly outperform their multilingual counterparts, with MAR- BERT achieving the best overall performance. Using our proposed methodology, we achieved competitive results on the MAHED shared task with a macro-F1 score of 0.707 on the test split, securing a strong position in the final competi- tion rankings.",
        "authors": [
          "Ahmed Khalil Elzainy",
          "Mohamed Amin",
          "Ahmed Samir",
          "Hazem Abdelsalam"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "615",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "619",
        "paper_id": 83,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.83.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.83.jpg",
        "title": "AAA at MAHED Text-based Hate and Hope Speech Classification: A Systematic Encoder Evaluation for Arabic Hope and Hate Speech Classification",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.83",
        "x": -4.334371089935303,
        "y": -0.04650990292429924,
        "year": "2025"
      },
      {
        "abstract": "Islamic inheritance law (\u2019Ilm al-Maw\u00afar\u00af\u0131th) re- quires precise identification of heirs and cal- culation of shares, which poses a challenge for AI. In this paper, we present a lightweight framework for solving multiple-choice inher- itance questions using a specialised Arabic text encoder and Attentive Relevance Scoring (ARS). The system ranks answer options ac- cording to semantic relevance, and enables fast, on-device inference without generative reason- ing. We evaluate Arabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based LLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an accuracy of up to 87.6%, they require more resources and are context- dependent. Our MARBERT-based approach achieves 69.87% accuracy, presenting a com- pelling case for efficiency, on-device deploya- bility, and privacy. While this is lower than the 87.6% achieved by the best-performing LLM, our work quantifies a critical trade-off between the peak performance of large models and the practical advantages of smaller, specialized sys- tems in high-stakes domains.",
        "authors": [
          "Salah Eddine Bekhouche",
          "Abdellah Zakaria Sellam",
          "Telli Hichem",
          "Cosimo Distante",
          "Abdenour Hadid"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "929",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "934",
        "paper_id": 128,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.128.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.128.jpg",
        "title": "CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.128",
        "x": -4.790569305419922,
        "y": 0.4131481349468231,
        "year": "2025"
      },
      {
        "abstract": "The escalating presence of propaganda and hate speech on social media platforms underscores the need for robust automated detection systems to preserve the integrity of public discourse. Our team, participated in the MAHED 2025 Shared Task at the ArabicNLP 2025 conference, co-located with EMNLP 2025, focusing on Subtask 1 (Text-based Hate and Hope Speech Classi\ufb01- cation) and Subtask 2 (Emotion, Offensive, and Directed Hate Detection) in Arabic content. In Subtask 1, we experimented with models including XLM-RoBERTa-Large, Davlan/xlm-roberta-base-\ufb01netuned-arabic, asafaya/bert-base-arabic, aubmindlab/bert- base-arabertv2, Google Gemma-7B, and Qwen2.5-14B-Instruct, achieving the highest macro-f1 of 0.674 with Gemma-7B and ranking 12th on the leaderboard. In Subtask 2, using models such as aubmindlab/bert-base- arabertv2, Google Gemma-7B, Qwen2.5- 14B-Instruct, asafaya/bert-base-arabic, and domain-speci\ufb01c hate-speech models, our best macro-f1 was 0.48 with both Gemma-7B and aubmindlab/bert-base-arabertv2, placing us 6th in the leaderboard.",
        "authors": [
          "Md Sagor Chowdhury",
          "Adiba Fairooz Chowdhury"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "692",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "699",
        "paper_id": 95,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.95.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.95.jpg",
        "title": "Quasar at MAHED Shared Task : Decoding Emotions and Offense in Arabic Text using LLM and Transformer-Based Approaches",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.95",
        "x": -6.810389041900635,
        "y": 1.2259427309036255,
        "year": "2025"
      },
      {
        "abstract": "Social media platforms have become major spaces for sharing opinions, humor, and infor- mation through memes that blend images with text. While many memes are harmless, some promote hate speech against individuals or com- munities based on cultural, religious, gender, or national identity. Detecting such content in Arabic is particularly challenging due to lin- guistic complexity, cultural context, and limited annotated data. In this study, we present an ef- fective approach for detecting hateful content in Arabic memes using theQCRI Prop2Hate- Memedataset, which contains image\u2013text pairs labeled for hatefulness. We experimented with several multimodal configurations, and the best performance was achieved using a combina- tion of InceptionNet for visual features and multilingual BERT for text. These represen- tations were fused after applying normaliza- tion and augmentation to enhance robustness. OurInceptionNetwithmBERTconfiguration achieved a macro F1-score of 63 percent and secured the sixth position on the official Cod- aBench leaderboard. These findings highlight the strength of our multimodal model and sup- port its potential for detecting harmful Arabic content in low-resource settings.",
        "authors": [
          "Joy Das",
          "Alamgir Hossain",
          "Mohammed Moshiul Hoque"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "683",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "691",
        "paper_id": 94,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.94.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.94.jpg",
        "title": "joy_2004114 at MAHED Shared Task : Filtering Hate Speech from Memes using A Multimodal Fusion-based Approach",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.94",
        "x": -2.925983428955078,
        "y": 4.263843536376953,
        "year": "2025"
      },
      {
        "abstract": "",
        "authors": [
          "Osama Farouk Zaki"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "935",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "939",
        "paper_id": 129,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.129.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.129.jpg",
        "title": "CIS-RG at QIAS 2025 Shared Task: Approaches for Enhancing Performance of LLM on Islamic Legal Reasoning and its Mathematical Calculations",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.129",
        "x": -2.623387575149536,
        "y": 2.3074769973754883,
        "year": "2025"
      },
      {
        "abstract": "We present our system for the MAHED 2025 Shared Task on Arabic Hate Meme Detection (subtask 3), a binary classification task to deter- mine whether a multimodal meme containing Arabic text and an image conveys a hateful message. Our approach uses multimodal fu- sion combining a visual encoder and an Ara- bic text encoder. We explored four fusion strategies\u2014transformer fusion, early fusion, cross-attention, and bilinear fusion\u2014and found transformer fusion offered the best single- model trade-off, while an ensemble of all four achieved the highest score. To address the severe class imbalance (90.05% not-hate vs. 9.95% hate), we applied class-weighted loss, focal loss, strong regularization, and light aug- mentation. Our best submission reached a macro-F1 score of 0.75 on the gold test set.",
        "authors": [
          "Yassir El Attar"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "608",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "614",
        "paper_id": 82,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.82.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.82.jpg",
        "title": "YassirEA at MAHED 2025: Fusion-Based Multimodal Models for Arabic Hate Meme Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.82",
        "x": -5.854769229888916,
        "y": 0.8813059329986572,
        "year": "2025"
      },
      {
        "abstract": "We present a two-stage framework for enhanc- ing Arabic cultural understanding in small lan- guage models, specifically designed for PalmX 2025(Alwajih et al., 2025) Shared Task 1: Gen- eral Culture Evaluation. Our approach com- bines continuous pretraining on a culturally- enriched Arabic corpus spanning 10 Arab coun- tries and different cultural domains, followed by supervised fine-tuning on cultural question- answering data. Using Parameter-Efficient Fine- T uning (PEFT) (Zhang et al., 2025) with LoRA on the Qwen3-4B base model, we achieve 74% accuracy on the development set and 64% on the blind test set, ranking our team ninth in the competition. Our system demonstrates the ef- fectiveness of targeted cultural pretraining for improving Arabic language models\u2019 cultural competency while maintaining computational efficiency.",
        "authors": [
          "Eman Elrefai",
          "Esraa Khaled",
          "Alhassan Ehab"
        ],
        "cluster": 4,
        "cluster_label": "Cluster 4: arabic, task, models",
        "cluster_label_llm": "Dialects, Safety, Readability, RAG-QA",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "825",
        "keywords": "arabic, task, models, shared, 2025, shared task, model, accuracy, set, using",
        "last_page": "829",
        "paper_id": 113,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.113.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.113.jpg",
        "title": "Star at PalmX 2025: Arabic Cultural Understanding via Targeted Pretraining and Lightweight Fine-tuning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.113",
        "x": -1.4037169218063354,
        "y": 2.8991570472717285,
        "year": "2025"
      },
      {
        "abstract": "Generating culturally accurate captions for im- ages in Arabic remains a challenging task due to the language\u2019s rich morphology, complex syntax, and diverse cultural contexts.Cultural preservation involves capturing the significance and emotional resonance of images related to Palestinian heritage, ensuring accurate repre- sentation for future generations. We present a translation-assisted, instruction-tuned mul- timodal pipeline for Arabic image caption- ing, developed for the ImageEval 2025 Shared Task Subtask 2: on the evaluation of im- age captioning models.Our approach leverages the Qwen2.5-VL-7B-Instruct model with 4- bit quantization, fine-tuned using Parameter- Efficient Fine-Tuning (PEFT) with LoRA. We implemented a pipeline involving translation of Arabic captions to English, followed by back-translation to generate fluent Arabic out- puts. We evaluated several vision-language models, including Qwen2.5 VL (7B), Llama 3.2 (11B), and Pixtral (12B). The Qwen2.5 VL (7B) model achieved a BLEU-1 score of 22.6, a Cosine Similarity of 57.48, and an LLM Judge Score of 31.43, securing third place in the com- petition. These results underscore the potential of instruction-tuned multimodal models to pro- duce culturally sensitive Arabic captions.",
        "authors": [
          "Muhammad Abu Horaira",
          "Farhan Amin",
          "Sakibul Hasan",
          "Md. Tanvir Ahammed Shawon",
          "Muhammad Ibrahim Khan"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "414",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "418",
        "paper_id": 57,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.57.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.57.jpg",
        "title": "PhantomTroupe at ImageEval 2025 Shared Task: Multimodal Arabic Image Captioning through Translation-Based Fine-Tuning of LLM Models",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.57",
        "x": -6.925508499145508,
        "y": 1.155779242515564,
        "year": "2025"
      },
      {
        "abstract": "This paper describes Elyadata & LIA\u2019s joint submission to the NADI multi-dialectal Arabic Speech Processing 2025. We participated in the Spoken Arabic Dialect Identification (ADI) and multi-dialectal Arabic ASR subtasks. Our submission ranked first for the ADI subtask and second for the multi-dialectal Arabic ASR subtask among all participants. Our ADI sys- tem is a fine-tuned Whisper-large-v3 encoder with data augmentation. This system obtained the highest ADI accuracy score of 79.83% on the official test set. For multi-dialectal Arabic ASR, we fine-tuned SeamlessM4T-v2 Large (Egyptian variant) separately for each of the eight considered dialects. Overall, we obtained an average WER and CER of 38.54% and 14.53%, respectively, on the test set. Our re- sults demonstrate the effectiveness of large pre- trained speech models with targeted fine-tuning for Arabic speech processing.",
        "authors": [
          "Haroun Elleuch",
          "Youssef Saidi",
          "Salima Mdhaffar",
          "Yannick Est\u00e8ve",
          "Fethi Bougares"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "762",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "766",
        "paper_id": 105,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.105.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.105.jpg",
        "title": "ELYADATA & LIA at NADI 2025: ASR and ADI Subtasks",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.105",
        "x": -3.2789273262023926,
        "y": 2.2188899517059326,
        "year": "2025"
      },
      {
        "abstract": "This paper describes our system for Task 3 of the Arabic NLP 2025 competition: detecting hateful content in Arabic memes. The task re- quires a robust understanding of both visual and textual information and their interplay. We developed and compared three distinct multi- modal fusion architectures: a Cross-Attention model, a progressive CNN-based fusion model, and a two-stage model using custom-trained embeddings with a gated fusion classifier. All models leverage pre-trained CLIP and MAR- BERT encoders for image and text represen- tation, respectively. We detail our approach to handling the significant class imbalance in the dataset through data re-splitting and the application of a weighted Focal Loss. Our post- competition analysis, training on all available data, shows that the CNN-based fusion model achieved the highest macro F1-score of 0.779, demonstrating the effectiveness of its hierarchi- cal feature extraction for this task.",
        "authors": [
          "Itbaan Safwan"
        ],
        "cluster": 3,
        "cluster_label": "Cluster 3: arabic, task, models",
        "cluster_label_llm": "Arabic Dialects, NER & Misinformation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "712",
        "keywords": "arabic, task, models, model, language, subtask, shared, shared task, fine, text",
        "last_page": "719",
        "paper_id": 98,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.98.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.98.jpg",
        "title": "Thinking Nodes at MAHED: A Comparative Study of Multimodal Architectures for Arabic Hateful Meme Detection",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.98",
        "x": -2.8758416175842285,
        "y": 2.2712786197662354,
        "year": "2025"
      },
      {
        "abstract": "This paper addresses the challenge of apply- ing Large Language Models (LLMs) to Is- lamic jurisprudence, a domain that requires both textual retrieval and precise rule-based reasoning. We focus on the QIAS 2025 shared task, which evaluates LLMs on two subtasks: Islamic inheritance reasoning and general Is- lamic knowledge assessment. Prior works in Arabic NLP and religious QA largely empha- size retrieval and classification, but they do not evaluate multi-step procedural reasoning. To fill this gap, we propose a hybrid multi-agent framework, termed Retrieval-Augmented Rea- soning (RAR). For inheritance problems, our Virtual Inheritance Expert parses natural lan- guage cases into structured JSON, retrieves rel- evant fatwas, and applies rule-based synthesis. For general knowledge, our Proponent\u2013Critic Debate simulates dialectical reasoning, with a head scholar model providing final judgment. Using an ensemble of Gemini, Fanar, and Mis- tral, our system achieved 2nd place in Subtask 1 and 1st place in Subtask 2. These results demonstrate that decomposing complex reason- ing into specialized pipelines supports robust- ness and accuracy in high-stakes domains.",
        "authors": [
          "Nguyen Xuan Phuc",
          "Th\u00ecn \u0110\u1eb7ng V\u0103n"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "905",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "913",
        "paper_id": 125,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.125.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.125.jpg",
        "title": "PuxAI at QIAS 2025: Multi-Agent Retrieval-Augmented Generation for Islamic Inheritance and Knowledge Reasoning",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.125",
        "x": -2.6098222732543945,
        "y": 3.745779037475586,
        "year": "2025"
      },
      {
        "abstract": "This paper details the system developed by team MedLingua for the MedArabiQ2025 Shared Task, specifically participating in Track 2, Sub-Task 1: Multiple Choice Question An- swering. Our approach centered on evaluating the zero-shot and few-shot capabilities of vari- ous Large Language Models (LLMs) on Arabic medical questions, as fine-tuning was not per- mitted. We systematically tested a range of models, from general-purpose state-of-the-art LLMs like Google\u2019s Gemini 2.5 Pro to spe- cialized medical models such as BiMediX2 and MedGemma. Our findings reveal that ad- vanced, general-domain models significantly outperform specialized medical LLMs that are not optimized for Arabic. Our best performing system, using Gemini 2.5 Pro, achieved an ac- curacy of 78% in the development set and 74% on the blind test set, securing the 3rd place on the official competition leaderboard.",
        "authors": [
          "Fatimah Mohamed Emad Elden",
          "Mumina Ab. Abukar"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "126",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "139",
        "paper_id": 20,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.20.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.20.jpg",
        "title": "MedLingua at MedArabiQ2025: Zero- and Few-Shot Prompting of Large Language Models for Arabic Medical QA",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.20",
        "x": -7.01531982421875,
        "y": 1.0374114513397217,
        "year": "2025"
      },
      {
        "abstract": "We present a graph-based approach enriched with lexicons to predict document-level read- ability in Arabic, developed as part of the Con- strained Track of the BAREC Shared Task 2025. Our system models each document as a sentence-level graph, where nodes rep- resent sentences and lemmas, and edges cap- ture linguistic relationships such as lexical co-occurrence and class membership. Sen- tence nodes are enriched with features from the SAMER lexicon as well as contextual em- beddings from the Arabic transformer model. The graph neural network (GNN) and trans- former sentence encoder are trained as two independent branches, and their predictions are combined via late fusion at inference. For document-level prediction, sentence-level out- puts are aggregated using max pooling to re- flect the most difficult sentence. Experimental results show that this hybrid method outper- forms standalone GNN or transformer branches across multiple readability metrics. Overall, the findings highlight that fusion offers advantages at the document level, but the GNN-only ap- proach remains stronger for precise prediction of sentence-level readability.",
        "authors": [
          "Passant Elchafei",
          "Mayar Osama",
          "Mohamad Rageh",
          "Mervat Abu - Elkheir"
        ],
        "cluster": 0,
        "cluster_label": "Cluster 0: arabic, task, models",
        "cluster_label_llm": "Arabic NLP: Disinfo, QA, MT",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "261",
        "keywords": "arabic, task, models, model, text, shared, shared task, language, detection, using",
        "last_page": "265",
        "paper_id": 36,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.36.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.36.jpg",
        "title": "GNNinjas at BAREC Shared Task 2025: Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.36",
        "x": -6.506566524505615,
        "y": 0.5690486431121826,
        "year": "2025"
      },
      {
        "abstract": "We present the findings of the first shared task on Qur\u2019anic pronunciation assessment, which focuses on addressing the unique chal- lenges of evaluating precise pronunciation of Qur\u2019anic recitation. To fill an existing research gap, the Iqra\u2019Eval 2025 shared task intro- duces the first open benchmark for Mispro- nunciation Detection and Diagnosis (MDD) in Qur\u2019anic recitation, using Modern Standard Arabic (MSA) reading of Qur\u2019anic texts as its case study. The task provides a comprehensive evaluation framework with increasingly com- plex subtasks: error localization and detailed error diagnosis. Leveraging the recently devel- oped QuranMB benchmark dataset along with auxiliary training resources, this shared task aims to stimulate research in an area of both linguistic and cultural significance while ad- dressing computational challenges in pronun- ciation assessment.",
        "authors": [
          "Yassine El Kheir",
          "Amit Meghanani",
          "Hawau Olamide Toyin",
          "Nada Almarwani",
          "Omnia Ibrahim",
          "Yousseif Ahmed Elshahawy",
          "Mostafa Shahin",
          "Ahmed Ali"
        ],
        "cluster": 2,
        "cluster_label": "Cluster 2: arabic, task, models",
        "cluster_label_llm": "Arabic Multitask Multimodal LLM Benchmarks",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "443",
        "keywords": "arabic, task, models, dataset, based, model, using, performance, paper, shared task",
        "last_page": "452",
        "paper_id": 61,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.61.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.61.jpg",
        "title": "Iqra\u2019Eval: A Shared Task on Qur\u2019anic Pronunciation Assessment",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.61",
        "x": -3.7642407417297363,
        "y": 1.8620725870132446,
        "year": "2025"
      },
      {
        "abstract": "Cultural understanding is essential for large language models (LLMs), particularly in the Arabic context where many models struggle to capture nuanced cultural elements. To ad- dress this gap, we propose a novel approach for Arabic cultural multiple-choice question answering that integrates retrieval-based train- ing data augmentation with parameter-efficient fine-tuning. Our system employs Gemini 1 to retrieve contextual evidence for each question, selects candidate pairs, and adapts NileChat- 3B by fine-tuning only three projection layers, reducing trainable parameters by 68.2% while preserving general language proficiency. On the PalmX 2025 Subtask 1 benchmark 2, our system attains 67.60% accuracy on the blind test set, ranking 6th overall and outperforming the NileChat-3B baseline by 3% on the devel- opment set. The model weights are publicly available at MohamedGomaa30/Ibn-Al-Nafs.",
        "authors": [
          "Mohamed Gomaa",
          "Noureldin Elmadany"
        ],
        "cluster": 1,
        "cluster_label": "Cluster 1: arabic, language, models",
        "cluster_label_llm": "Arabic LLMs Dialects Bias Evaluation",
        "conference": "Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks",
        "first_page": "797",
        "keywords": "arabic, language, models, llms, task, data, fine, model, based, results",
        "last_page": "801",
        "paper_id": 109,
        "pdf_url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.109.pdf",
        "publication_date": "2025/11",
        "thumbnail": "https://aclanthology.org/thumb/2025.arabicnlp-sharedtasks.109.jpg",
        "title": "ISL-NLP at PalmX 2025: Retrieval-Augmented Fine-Tuning for Arabic Cultural Question Answering",
        "type": "shared_task",
        "url": "https://aclanthology.org/2025.arabicnlp-sharedtasks.109",
        "x": -3.1694557666778564,
        "y": 4.136401653289795,
        "year": "2025"
      }
    ]
  },
  "encoding": {
    "color": {
      "field": "cluster_label_llm",
      "legend": {
        "columns": 1,
        "labelFontSize": 13,
        "labelLimit": 0,
        "symbolLimit": 0,
        "title": "Research Topics",
        "titleFontSize": 14
      },
      "type": "nominal"
    },
    "href": {
      "field": "url",
      "type": "nominal"
    },
    "opacity": {
      "condition": {
        "param": "param_67",
        "value": 1
      },
      "value": 0.15
    },
    "size": {
      "condition": {
        "param": "param_67",
        "value": 100
      },
      "value": 40
    },
    "tooltip": [
      {
        "field": "title",
        "title": "Title",
        "type": "nominal"
      },
      {
        "field": "authors",
        "title": "Authors",
        "type": "nominal"
      },
      {
        "field": "year",
        "title": "Year",
        "type": "nominal"
      },
      {
        "field": "type",
        "title": "Type",
        "type": "nominal"
      },
      {
        "field": "cluster_label_llm",
        "title": "Cluster",
        "type": "nominal"
      }
    ],
    "x": {
      "axis": {
        "domain": false,
        "labels": false,
        "ticks": false
      },
      "field": "x",
      "scale": {
        "zero": false
      },
      "type": "quantitative"
    },
    "y": {
      "axis": {
        "domain": false,
        "labels": false,
        "ticks": false
      },
      "field": "y",
      "scale": {
        "zero": false
      },
      "type": "quantitative"
    }
  },
  "height": 500,
  "mark": {
    "opacity": 0.3,
    "size": 40,
    "stroke": "#666",
    "strokeWidth": 1,
    "type": "circle"
  },
  "params": [
    {
      "bind": "legend",
      "name": "param_67",
      "select": {
        "fields": [
          "cluster_label_llm"
        ],
        "toggle": "true",
        "type": "point"
      }
    },
    {
      "bind": {
        "input": "select",
        "labels": [
          "All",
          "2023",
          "2024",
          "2025"
        ],
        "name": "Year",
        "options": [
          null,
          "2023",
          "2024",
          "2025"
        ]
      },
      "name": "param_65",
      "select": {
        "fields": [
          "year"
        ],
        "type": "point"
      }
    },
    {
      "bind": {
        "input": "select",
        "labels": [
          "All",
          "main",
          "shared_task"
        ],
        "name": "Type",
        "options": [
          null,
          "main",
          "shared_task"
        ]
      },
      "name": "param_66",
      "select": {
        "fields": [
          "type"
        ],
        "type": "point"
      }
    },
    {
      "bind": "scales",
      "name": "param_68",
      "select": {
        "encodings": [
          "x",
          "y"
        ],
        "type": "interval"
      }
    }
  ],
  "title": {
    "color": "#000000",
    "font": "system-ui, -apple-system, sans-serif",
    "fontSize": 18,
    "text": "Arabic NLP Papers: 328 Research Papers (2023-2025)"
  },
  "transform": [
    {
      "filter": {
        "empty": true,
        "param": "param_65"
      }
    },
    {
      "filter": {
        "empty": true,
        "param": "param_66"
      }
    }
  ],
  "usermeta": {
    "embedOptions": {
      "loader": {
        "target": "_blank"
      }
    }
  },
  "width": 800
};

vegaEmbed('#vis', spec, {
    actions: false,
    config: {
        legend: {
            labelColor: "black",
            titleColor: "black"
        },
        view: {
            continuousHeight: 420   // shrink chart height a bit so everything fits
        }
    }
}).catch(console.error);
</script>
</body>
</html>