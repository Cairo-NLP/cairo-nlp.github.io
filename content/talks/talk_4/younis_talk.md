---
title: "Cracking Long Sequences “How Mamba Outperforms Transformers”"
date: 2025-11-18
draft: false
speaker: "Abdullah Younis"
speaker_image: "/images/speakers/younis.jpeg"  # Add this line
affiliation: ""
event_date: "2025-11-18"
time: "7:30 PM Cairo Time"
location: "Online (Google Meet)"
status: "upcoming"
tags: ["Mamba", "state-space models", "sequence modeling"]
video_url: "https://youtu.be/l3nRiVVIhAU"
---

## Abstract

Demystifies how Mamba models efficiently handle extremely long sequences while outperforming Transformers in speed, scalability, and memory usage. We break down the core ideas behind selective state-space models, why they excel where attention struggles, and what this shift means for the future of sequence modeling.

## Speaker Bio
Short Bio:  Younis is an (NLP Research Engineer ll) and a Computer Science Master’s student specializing in multimodal AI systems and advanced speech-modeling, with research spanning conversational analysis, interaction modeling, and real-time systems. His work also encompasses modern generative modeling principles, including learning frameworks for structured sequence generation and complex signal modeling. He has designed and deployed production-grade AI solutions for leading enterprises, while contributing to research, model refinement, and system innovation within R&D environments. 
