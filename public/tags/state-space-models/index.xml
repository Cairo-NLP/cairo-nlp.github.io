<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>State-Space Models on Cairo NLP</title>
    <link>http://localhost:1313/tags/state-space-models/</link>
    <description>Recent content in State-Space Models on Cairo NLP</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>cairo.nlp.group@gmail.com (cAIro|NLP Group)</managingEditor>
    <webMaster>cairo.nlp.group@gmail.com (cAIro|NLP Group)</webMaster>
    <lastBuildDate>Tue, 18 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/state-space-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cracking Long Sequences “How Mamba Outperforms Transformers”</title>
      <link>http://localhost:1313/talks/talk_4/younis_talk/</link>
      <pubDate>Tue, 18 Nov 2025 00:00:00 +0000</pubDate><author>cairo.nlp.group@gmail.com (cAIro|NLP Group)</author>
      <guid>http://localhost:1313/talks/talk_4/younis_talk/</guid>
      <description>Abstract Demystifies how Mamba models efficiently handle extremely long sequences while outperforming Transformers in speed, scalability, and memory usage. We break down the core ideas behind selective state-space models, why they excel where attention struggles, and what this shift means for the future of sequence modeling.&#xA;Speaker Bio Short Bio: Younis is an (NLP Research Engineer ll) and a Computer Science Master’s student specializing in multimodal AI systems and advanced speech-modeling, with research spanning conversational analysis, interaction modeling, and real-time systems.</description>
    </item>
  </channel>
</rss>
